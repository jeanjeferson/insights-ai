"""
üß™ TESTE PARA UNIFIED BUSINESS INTELLIGENCE
===========================================

Suite de testes para validar a funcionalidade core do Unified Business Intelligence.
Baseada no template funcionando do KPI Calculator Tool V3.0.
"""

import pytest
import pandas as pd
import numpy as np
import time
import os
import json
from datetime import datetime, timedelta
from pathlib import Path
import tracemalloc

# Importar ferramenta a ser testada
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.insights.tools.business_intelligence_tool import BusinessIntelligenceTool


class TestBusinessIntelligenceTool:
    """
    Suite de testes para Unified Business Intelligence
    
    Focada em valida√ß√£o funcional dos 10 tipos de an√°lise de BI.
    """
    
    # Tipos de an√°lise executivos identificados na ferramenta
    EXECUTIVE_ANALYSES = [
        'executive_summary',              # Resumo C-level
        'executive_dashboard',            # Dashboard visual executivo
        'financial_analysis',             # KPIs + visualiza√ß√µes + benchmarks
        'customer_intelligence',          # RFM + segmenta√ß√£o + reten√ß√£o
        'product_performance'             # ABC + rankings + categoria
    ]
    
    ALL_ANALYSES = [
        # Executivos
        'executive_summary', 'executive_dashboard',
        # Financeiros  
        'financial_analysis', 'profitability_analysis',
        # Clientes e produtos
        'customer_intelligence', 'product_performance',
        # Especializados
        'demographic_analysis', 'geographic_analysis', 
        'sales_team_analysis', 'comprehensive_report'
    ]
    
    @pytest.fixture(autouse=True)
    def setup(self, real_vendas_data):
        """Setup autom√°tico para cada teste."""
        self.bi_tool = BusinessIntelligenceTool()
        self.real_data_path = real_vendas_data
        self.test_logs = []
        self.start_time = time.time()
        
        print(f"üöÄ Iniciando teste Unified Business Intelligence com dados: {self.real_data_path}")
    
    def setup_standalone(self, data_path):
        """Setup para execu√ß√£o standalone."""
        self.bi_tool = BusinessIntelligenceTool()
        self.real_data_path = data_path
        self.test_logs = []
        self.start_time = time.time()
        
        print(f"üöÄ Iniciando teste Unified Business Intelligence com dados: {self.real_data_path}")
    
    def log_test(self, level: str, message: str, **kwargs):
        """Logging simplificado para testes."""
        elapsed = time.time() - self.start_time
        log_entry = {
            'elapsed': round(elapsed, 2),
            'level': level,
            'message': message,
            **kwargs
        }
        self.test_logs.append(log_entry)
        print(f"[{elapsed:6.2f}s] [{level}] {message}")
        if kwargs:
            print(f"    {kwargs}")
    
    def test_executive_summary_basic(self):
        """
        Teste b√°sico do resumo executivo.
        """
        self.log_test("INFO", "Iniciando teste de resumo executivo")
        
        # Medir performance b√°sica
        start_time = time.time()
        tracemalloc.start()
        
        result = self.bi_tool._run(
            analysis_type="executive_summary",
            data_csv=self.real_data_path,
            time_period="last_12_months",
            detail_level="summary",
            include_forecasts=True
        )
        
        execution_time = time.time() - start_time
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        # Valida√ß√µes b√°sicas
        assert isinstance(result, str), "Resultado deve ser string"
        assert len(result) > 200, "Resultado muito curto para um resumo executivo"
        # Verificar erros mais espec√≠ficos
        error_indicators = ["‚ùå erro", "error:", "exception:", "traceback"]
        has_error = any(indicator in result.lower() for indicator in error_indicators)
        assert not has_error, f"Erro detectado: {result[:200]}..."
        
        # Valida√ß√µes de conte√∫do executivo (aceita termos em portugu√™s e ingl√™s)
        executive_terms = [
            "executivo", "executive", "sum√°rio", "summary", 
            "kpi", "receita", "revenue", "performance", "neg√≥cio", "business"
        ]
        found_terms = [term for term in executive_terms if term in result.lower()]
        assert len(found_terms) >= 3, f"Poucos termos executivos encontrados: {found_terms}"
        
        self.log_test("SUCCESS", "Resumo executivo validado", 
                     execution_time=f"{execution_time:.2f}s",
                     memory_peak=f"{peak/1024/1024:.1f}MB",
                     terms_found=len(found_terms),
                     result_length=len(result))
        
        return result
    
    def test_executive_dashboard_basic(self):
        """
        Teste b√°sico do dashboard executivo.
        """
        self.log_test("INFO", "Iniciando teste de dashboard executivo")
        
        start_time = time.time()
        
        result = self.bi_tool._run(
            analysis_type="executive_dashboard",
            data_csv=self.real_data_path,
            output_format="interactive",
            include_forecasts=True,
            export_file=False  # N√£o exportar arquivo para acelerar teste
        )
        
        execution_time = time.time() - start_time
        
        # Valida√ß√µes b√°sicas
        assert isinstance(result, str), "Resultado deve ser string"
        assert len(result) > 100, "Resultado muito curto"
        # Verificar erros mais espec√≠ficos (evitar false positives no HTML)
        error_indicators = ["‚ùå erro", "error:", "exception:", "traceback"]
        has_error = any(indicator in result.lower() for indicator in error_indicators)
        assert not has_error, f"Erro detectado: {result[:200]}..."
        
        # Valida√ß√µes de conte√∫do de dashboard
        dashboard_terms = [
            "dashboard", "gr√°fico", "chart", "visual", "plotly",
            "interativo", "interactive", "figure", "html"
        ]
        found_terms = [term for term in dashboard_terms if term in result.lower()]
        assert len(found_terms) >= 2, f"Poucos termos de dashboard encontrados: {found_terms}"
        
        self.log_test("SUCCESS", "Dashboard executivo validado", 
                     execution_time=f"{execution_time:.2f}s",
                     terms_found=len(found_terms),
                     result_length=len(result))
        
        return result
    
    def test_financial_analysis_basic(self):
        """
        Teste b√°sico da an√°lise financeira.
        """
        self.log_test("INFO", "Iniciando teste de an√°lise financeira")
        
        start_time = time.time()
        
        result = self.bi_tool._run(
            analysis_type="financial_analysis",
            data_csv=self.real_data_path,
            time_period="last_12_months",
            include_forecasts=True
        )
        
        execution_time = time.time() - start_time
        
        # Valida√ß√µes b√°sicas
        assert isinstance(result, str), "Resultado deve ser string"
        assert len(result) > 200, "Resultado muito curto para an√°lise financeira"
        # Verificar erros mais espec√≠ficos
        error_indicators = ["‚ùå erro", "error:", "exception:", "traceback"]
        has_error = any(indicator in result.lower() for indicator in error_indicators)
        assert not has_error, f"Erro detectado: {result[:200]}..."
        
        # Valida√ß√µes de conte√∫do financeiro
        financial_terms = [
            "receita", "revenue", "kpi", "financeiro", "financial",
            "lucro", "profit", "margem", "margin", "crescimento", "growth"
        ]
        found_terms = [term for term in financial_terms if term in result.lower()]
        assert len(found_terms) >= 3, f"Poucos termos financeiros encontrados: {found_terms}"
        
        # Verificar valores monet√°rios
        assert "R$" in result or "real" in result.lower(), "Deve incluir valores monet√°rios"
        
        self.log_test("SUCCESS", "An√°lise financeira validada", 
                     execution_time=f"{execution_time:.2f}s",
                     terms_found=len(found_terms),
                     result_length=len(result))
        
        return result
    
    def test_executive_analyses_batch(self):
        """
        Teste das 5 an√°lises executivas mais importantes em lote.
        """
        self.log_test("INFO", f"Testando {len(self.EXECUTIVE_ANALYSES)} an√°lises executivas")
        
        results = {}
        
        for analysis_type in self.EXECUTIVE_ANALYSES:
            start_time = time.time()
            
            try:
                result = self.bi_tool._run(
                    analysis_type=analysis_type,
                    data_csv=self.real_data_path,
                    time_period="last_12_months",
                    detail_level="summary"
                )
                
                execution_time = time.time() - start_time
                success = "erro" not in result.lower() and len(result) > 50
                
                results[analysis_type] = {
                    'success': success,
                    'execution_time': round(execution_time, 2),
                    'output_length': len(result)
                }
                
                self.log_test("SUCCESS" if success else "ERROR", 
                             f"An√°lise {analysis_type}",
                             **results[analysis_type])
                
            except Exception as e:
                results[analysis_type] = {
                    'success': False,
                    'error': str(e),
                    'execution_time': time.time() - start_time
                }
                self.log_test("ERROR", f"Erro em {analysis_type}: {str(e)}")
        
        # Valida√ß√µes
        successful = [analysis for analysis, res in results.items() if res['success']]
        success_rate = len(successful) / len(self.EXECUTIVE_ANALYSES)
        
        assert success_rate >= 0.6, f"Taxa de sucesso baixa: {success_rate:.1%}"
        
        self.log_test("SUCCESS", "Teste de an√°lises executivas conclu√≠do",
                     success_rate=f"{success_rate:.1%}",
                     successful_analyses=successful)
        
        return results
    
    def test_all_analyses_comprehensive(self):
        """
        Teste abrangente de todas as an√°lises de BI dispon√≠veis.
        """
        self.log_test("INFO", f"Testando todas as {len(self.ALL_ANALYSES)} an√°lises de BI")
        
        results = {}
        failed = []
        
        for analysis_type in self.ALL_ANALYSES:
            try:
                start_time = time.time()
                
                result = self.bi_tool._run(
                    analysis_type=analysis_type,
                    data_csv=self.real_data_path,
                    time_period="last_12_months"
                )
                
                execution_time = time.time() - start_time
                
                if "erro" in result.lower():
                    failed.append(f"{analysis_type}: erro no resultado")
                elif len(result) < 50:
                    failed.append(f"{analysis_type}: resultado muito curto")
                else:
                    results[analysis_type] = {
                        'success': True,
                        'execution_time': round(execution_time, 2),
                        'output_length': len(result)
                    }
                    
                    print(f"‚úÖ {analysis_type}: {execution_time:.2f}s, {len(result)} chars")
                
            except Exception as e:
                failed.append(f"{analysis_type}: {str(e)}")
                print(f"‚ùå {analysis_type}: ERRO - {str(e)}")
        
        # Calcular taxa de sucesso
        total_analyses = len(self.ALL_ANALYSES)
        successful_count = len(results)
        success_rate = successful_count / total_analyses
        
        # Aceitar at√© 40% de falha (60% de sucesso)
        assert success_rate >= 0.6, f"Taxa de sucesso baixa: {success_rate:.1%}, Falhas: {failed[:5]}"
        
        self.log_test("SUCCESS", "Teste abrangente conclu√≠do",
                     success_rate=f"{success_rate:.1%}",
                     successful_count=successful_count,
                     total_analyses=total_analyses,
                     failed_count=len(failed))
        
        print(f"üìä Taxa de sucesso: {success_rate:.1%} ({successful_count}/{total_analyses})")
        
        return results
    
    def test_output_formats(self):
        """
        Teste dos diferentes formatos de sa√≠da.
        """
        self.log_test("INFO", "Testando formatos de sa√≠da")
        
        formats = ["text", "interactive", "html"]
        results = {}
        
        for output_format in formats:
            try:
                start_time = time.time()
                
                result = self.bi_tool._run(
                    analysis_type="executive_summary",
                    data_csv=self.real_data_path,
                    output_format=output_format,
                    export_file=False  # N√£o exportar arquivo para acelerar
                )
                
                execution_time = time.time() - start_time
                
                results[output_format] = {
                    'success': isinstance(result, str) and len(result) > 50,
                    'execution_time': round(execution_time, 2),
                    'output_length': len(result) if isinstance(result, str) else 0
                }
                
                self.log_test("SUCCESS", f"Formato {output_format}",
                             **results[output_format])
                
            except Exception as e:
                results[output_format] = {
                    'success': False,
                    'error': str(e)
                }
                self.log_test("ERROR", f"Erro no formato {output_format}: {str(e)}")
        
        # Pelo menos um formato deve funcionar
        successful = [fmt for fmt, res in results.items() if res['success']]
        assert len(successful) >= 1, f"Nenhum formato funcionou: {results}"
        
        self.log_test("SUCCESS", "Formatos de sa√≠da testados",
                     successful_formats=successful)
        
        return results
    
    def test_error_handling_basic(self):
        """
        Teste b√°sico de tratamento de erros.
        """
        self.log_test("INFO", "Testando tratamento de erros")
        
        # Teste 1: Arquivo inexistente
        result1 = self.bi_tool._run(
            analysis_type="executive_summary",
            data_csv="arquivo_inexistente.csv"
        )
        
        assert isinstance(result1, str), "Resultado deve ser string mesmo com erro"
        assert len(result1) > 10, "Mensagem de erro muito curta"
        
        # Teste 2: Tipo de an√°lise inv√°lido
        result2 = self.bi_tool._run(
            analysis_type="analise_inexistente",
            data_csv=self.real_data_path
        )
        
        assert isinstance(result2, str), "Resultado deve ser string mesmo com erro"
        assert "n√£o suportada" in result2.lower() or "op√ß√µes" in result2.lower(), "Deve indicar an√°lise n√£o suportada"
        
        self.log_test("SUCCESS", "Tratamento de erros validado")
        
        return True
    
    def test_performance_basic(self):
        """
        Teste b√°sico de performance.
        """
        self.log_test("INFO", "Testando performance b√°sica")
        
        start_time = time.time()
        tracemalloc.start()
        
        # Executar an√°lise r√°pida
        result = self.bi_tool._run(
            analysis_type="executive_summary",
            data_csv=self.real_data_path,
            detail_level="summary",
            include_forecasts=False  # Acelerar
        )
        
        execution_time = time.time() - start_time
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        # Valida√ß√µes de performance
        assert execution_time < 45, f"Execu√ß√£o muito lenta: {execution_time:.2f}s"
        assert peak < 600 * 1024 * 1024, f"Uso de mem√≥ria muito alto: {peak/1024/1024:.1f}MB"
        
        self.log_test("SUCCESS", "Performance validada",
                     execution_time=f"{execution_time:.2f}s",
                     memory_peak=f"{peak/1024/1024:.1f}MB",
                     result_length=len(result))
        
        return True
    
    def teardown_method(self, method):
        """Limpeza ap√≥s cada teste."""
        elapsed = time.time() - self.start_time
        print(f"üèÅ Teste {method.__name__} conclu√≠do em {elapsed:.2f}s")
        
        # Log summary se dispon√≠vel
        if self.test_logs:
            success_logs = [log for log in self.test_logs if log['level'] == 'SUCCESS']
            error_logs = [log for log in self.test_logs if log['level'] == 'ERROR']
            print(f"üìä Resumo: {len(success_logs)} sucessos, {len(error_logs)} erros")


# Execu√ß√£o standalone para desenvolvimento
if __name__ == "__main__":
    test_instance = TestBusinessIntelligenceTool()
    
    # Verificar se existe arquivo de dados real
    data_path = "data/vendas.csv"
    if not os.path.exists(data_path):
        print(f"‚ö†Ô∏è Arquivo {data_path} n√£o encontrado. Usando dados de amostra.")
        data_path = "src/tests/data_tests/vendas_sample.csv"
    
    if os.path.exists(data_path):
        test_instance.setup_standalone(data_path)
        
        print("üß™ Executando testes do Unified Business Intelligence...")
        
        # Executar testes principais
        try:
            test_instance.test_executive_summary_basic()
            test_instance.test_financial_analysis_basic()
            test_instance.test_executive_analyses_batch()
            test_instance.test_output_formats()
            test_instance.test_error_handling_basic()
            
            print("‚úÖ Todos os testes principais passaram!")
            
        except Exception as e:
            print(f"‚ùå Erro nos testes: {str(e)}")
    else:
        print(f"‚ùå Nenhum arquivo de dados encontrado para teste.") 