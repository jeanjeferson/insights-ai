"""
üß™ TESTE COMPLETO PARA COMPETITIVE INTELLIGENCE TOOL
=====================================================

Suite de testes abrangente para validar todas as funcionalidades do
Competitive Intelligence Tool, incluindo:
- 5 tipos de an√°lise competitiva
- Benchmarks setoriais brasileiros
- Valida√ß√£o robusta de par√¢metros
- Performance e qualidade
"""

import pytest
import pandas as pd
import numpy as np
import time
import os
import json
from datetime import datetime, timedelta
from pathlib import Path
import tracemalloc
import warnings

# Importar ferramenta a ser testada
import sys
import os

# Configurar caminhos de importa√ß√£o de forma robusta
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))

# Adicionar caminhos poss√≠veis ao sys.path
possible_paths = [
    project_root,
    os.path.join(project_root, 'src'),
    current_dir,
    os.path.dirname(current_dir)
]

for path in possible_paths:
    if path not in sys.path:
        sys.path.insert(0, path)

# Tentar importar o Competitive Intelligence Tool
try:
    from src.insights.tools.advanced.competitive_intelligence_tool import CompetitiveIntelligenceTool
except ImportError:
    # Fallback para importa√ß√£o direta
    sys.path.insert(0, os.path.join(project_root, 'src', 'insights', 'tools', 'advanced'))
    from competitive_intelligence_tool import CompetitiveIntelligenceTool

# Suprimir warnings para testes mais limpos
warnings.filterwarnings('ignore')


class TestCompetitiveIntelligenceTool:
    """
    Suite completa de testes para Competitive Intelligence Tool
    
    Cobertura:
    - Todas as 5 an√°lises competitivas
    - Benchmarks setoriais brasileiros
    - Valida√ß√£o de par√¢metros
    - Performance e qualidade
    """
    
    @pytest.fixture(autouse=True)
    def setup(self, real_vendas_data):
        """Setup autom√°tico para cada teste."""
        self.competitive_tool = CompetitiveIntelligenceTool()
        self.real_data_path = real_vendas_data
        self.test_logs = []
        self.start_time = time.time()
        
        print(f"üèÜ Iniciando teste Competitive Intelligence Tool: {self.real_data_path}")
    
    def setup_standalone(self, data_path):
        """Setup para execu√ß√£o standalone."""
        self.competitive_tool = CompetitiveIntelligenceTool()
        self.real_data_path = data_path
        self.test_logs = []
        self.start_time = time.time()
        
        print(f"üèÜ Iniciando teste Competitive Intelligence Tool: {self.real_data_path}")
    
    def log_test(self, level: str, message: str, **kwargs):
        """Logging detalhado para testes."""
        elapsed = time.time() - self.start_time
        log_entry = {
            'elapsed': round(elapsed, 2),
            'level': level,
            'message': message,
            **kwargs
        }
        self.test_logs.append(log_entry)
        print(f"[{elapsed:6.2f}s] [{level}] {message}")
        if kwargs:
            print(f"    {kwargs}")
    
    # ==========================================
    # TESTES DE VALIDA√á√ÉO B√ÅSICA
    # ==========================================
    
    def test_data_loading_and_validation(self):
        """Teste de carregamento e valida√ß√£o de dados."""
        self.log_test("INFO", "Testando carregamento e valida√ß√£o de dados")
        
        # Verificar se arquivo existe
        assert os.path.exists(self.real_data_path), "Arquivo de dados n√£o encontrado"
        
        # Testar carregamento de dados usando m√©todo interno
        df = pd.read_csv(self.real_data_path, sep=';', encoding='utf-8')
        df_prepared = self.competitive_tool._prepare_competitive_data(df)
        
        assert df_prepared is not None, "Falha na prepara√ß√£o de dados competitivos"
        assert len(df_prepared) > 0, "DataFrame preparado vazio"
        assert 'Preco_Unitario' in df_prepared.columns, "Coluna Preco_Unitario n√£o criada"
        assert 'Faixa_Preco' in df_prepared.columns, "Coluna Faixa_Preco n√£o criada"
        
        # Verificar categoriza√ß√£o de pre√ßos
        faixas_esperadas = ['Economy', 'Mid', 'Premium', 'Luxury', 'Ultra-Luxury']
        faixas_encontradas = df_prepared['Faixa_Preco'].unique()
        faixas_validas = [f for f in faixas_encontradas if pd.notna(f) and f in faixas_esperadas]
        assert len(faixas_validas) > 0, "Nenhuma faixa de pre√ßo v√°lida encontrada"
        
        self.log_test("SUCCESS", "Valida√ß√£o de dados aprovada",
                     rows=len(df_prepared), 
                     columns=len(df_prepared.columns),
                     price_ranges=len(faixas_validas))
    
    def test_benchmark_loading(self):
        """Teste de carregamento de benchmarks do setor."""
        self.log_test("INFO", "Testando carregamento de benchmarks")
        
        # Testar carregamento de benchmarks para joalherias
        benchmarks_joalherias = self.competitive_tool._load_market_benchmarks('joalherias')
        assert isinstance(benchmarks_joalherias, dict), "Benchmarks deve ser um dicion√°rio"
        
        # Verificar estrutura essencial dos benchmarks
        expected_keys = [
            'market_size_billion_brl', 'annual_growth_rate', 'average_ticket',
            'seasonal_patterns', 'category_distribution', 'margin_benchmarks'
        ]
        missing_keys = [key for key in expected_keys if key not in benchmarks_joalherias]
        assert len(missing_keys) == 0, f"Chaves essenciais ausentes: {missing_keys}"
        
        # Verificar estrutura do average_ticket
        avg_ticket = benchmarks_joalherias['average_ticket']
        assert 'economy' in avg_ticket, "Categoria economy ausente em average_ticket"
        assert 'premium' in avg_ticket, "Categoria premium ausente em average_ticket"
        assert 'luxury' in avg_ticket, "Categoria luxury ausente em average_ticket"
        
        # Verificar valores num√©ricos v√°lidos
        assert benchmarks_joalherias['market_size_billion_brl'] > 0, "Market size deve ser positivo"
        assert 0 < benchmarks_joalherias['annual_growth_rate'] < 1, "Growth rate deve estar entre 0 e 1"
        
        # Testar outros segmentos
        benchmarks_relogios = self.competitive_tool._load_market_benchmarks('relogios')
        benchmarks_acessorios = self.competitive_tool._load_market_benchmarks('acessorios')
        
        # Segmentos n√£o implementados devem retornar benchmarks de joalherias
        assert benchmarks_relogios == benchmarks_joalherias, "Fallback para joalherias n√£o funcionou"
        assert benchmarks_acessorios == benchmarks_joalherias, "Fallback para joalherias n√£o funcionou"
        
        self.log_test("SUCCESS", "Benchmarks carregados",
                     benchmark_keys=len(benchmarks_joalherias),
                     price_categories=len(avg_ticket))
    
    def test_input_validation(self):
        """Teste de valida√ß√£o de par√¢metros de entrada."""
        self.log_test("INFO", "Testando valida√ß√£o de par√¢metros")
        
        # Teste com par√¢metros v√°lidos
        result_valid = self.competitive_tool._run(
            analysis_type="market_position",
            data_csv=self.real_data_path,
            market_segment="joalherias",
            benchmark_period="quarterly"
        )
        assert "error" not in result_valid.lower(), "Erro com par√¢metros v√°lidos"
        
        # Teste com analysis_type inv√°lido
        result_invalid_type = self.competitive_tool._run(
            analysis_type="analise_inexistente",
            data_csv=self.real_data_path
        )
        assert "n√£o suportada" in result_invalid_type.lower() or "error" in result_invalid_type.lower(), \
               "Erro n√£o detectado para analysis_type inv√°lido"
        
        # Teste com arquivo inexistente
        result_no_file = self.competitive_tool._run(
            analysis_type="market_position",
            data_csv="arquivo_inexistente.csv"
        )
        assert "n√£o encontrado" in result_no_file.lower() or "error" in result_no_file.lower(), \
               "Erro n√£o detectado para arquivo inexistente"
        
        # Teste com dados insuficientes (simular arquivo pequeno)
        small_df = pd.DataFrame({
            'Data': [datetime.now() - timedelta(days=i) for i in range(5)],
            'Total_Liquido': [100, 200, 150, 300, 250],
            'Quantidade': [1, 2, 1, 3, 2]
        })
        small_csv = "test_small_data.csv"
        small_df.to_csv(small_csv, sep=';', index=False)
        
        try:
            result_small = self.competitive_tool._run(
                analysis_type="market_position",
                data_csv=small_csv
            )
            # Deve detectar dados insuficientes (< 30 registros)
            assert "insuficientes" in result_small.lower() or "error" in result_small.lower(), \
                   "Erro n√£o detectado para dados insuficientes"
        finally:
            if os.path.exists(small_csv):
                os.remove(small_csv)
        
        self.log_test("SUCCESS", "Valida√ß√£o de par√¢metros aprovada")
    
    # ==========================================
    # TESTES DAS 5 AN√ÅLISES COMPETITIVAS
    # ==========================================
    
    def test_market_position_analysis(self):
        """Teste completo da an√°lise de posicionamento de mercado."""
        self.log_test("INFO", "Testando Market Position Analysis")
        
        start_time = time.time()
        tracemalloc.start()
        
        result = self.competitive_tool._run(
            analysis_type="market_position",
            data_csv=self.real_data_path,
            market_segment="joalherias",
            benchmark_period="quarterly",
            include_recommendations=True,
            risk_tolerance="medium"
        )
        
        execution_time = time.time() - start_time
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        # Valida√ß√µes b√°sicas
        assert isinstance(result, str), "Resultado deve ser string"
        assert len(result) > 800, "Resultado muito curto para market position"
        assert "error" not in result.lower(), f"Erro detectado: {result[:200]}..."
        
        # Verificar conte√∫do espec√≠fico de market position
        position_terms = ["posicionamento", "market position", "ticket m√©dio", "benchmark", "positioning"]
        found_terms = [term for term in position_terms if term.lower() in result.lower()]
        assert len(found_terms) >= 2, f"Poucos termos de posicionamento: {found_terms}"
        
        # Verificar se√ß√µes esperadas
        expected_sections = ["posicionamento competitivo", "m√©tricas vs mercado", "participa√ß√£o de mercado"]
        found_sections = [section for section in expected_sections if section.lower() in result.lower()]
        assert len(found_sections) >= 1, f"Se√ß√µes de posicionamento n√£o encontradas: {found_sections}"
        
        # Verificar se h√° categoriza√ß√£o de posicionamento
        position_categories = ["economy", "mid", "premium", "luxury"]
        found_categories = [cat for cat in position_categories if cat.lower() in result.lower()]
        assert len(found_categories) >= 1, f"Categorias de posicionamento n√£o encontradas: {found_categories}"
        
        self.log_test("SUCCESS", "Market Position validado",
                     execution_time=f"{execution_time:.2f}s",
                     memory_peak=f"{peak/1024/1024:.1f}MB",
                     position_terms=len(found_terms))
    
    def test_pricing_analysis(self):
        """Teste da an√°lise competitiva de pre√ßos."""
        self.log_test("INFO", "Testando Pricing Analysis")
        
        result = self.competitive_tool._run(
            analysis_type="pricing_analysis",
            data_csv=self.real_data_path,
            market_segment="joalherias",
            include_recommendations=True
        )
        
        # Valida√ß√µes b√°sicas
        assert isinstance(result, str), "Resultado deve ser string"
        assert len(result) > 600, "Resultado muito curto para pricing analysis"
        assert "error" not in result.lower(), f"Erro detectado: {result[:200]}..."
        
        # Verificar conte√∫do espec√≠fico de pricing
        pricing_terms = ["pre√ßo", "pricing", "competitiv", "categoria", "premium", "desconto"]
        found_terms = [term for term in pricing_terms if term.lower() in result.lower()]
        assert len(found_terms) >= 3, f"Poucos termos de pricing: {found_terms}"
        
        # Verificar an√°lise por categoria
        category_terms = ["economy", "mid", "premium", "luxury"]
        found_categories = [cat for cat in category_terms if cat.lower() in result.lower()]
        assert len(found_categories) >= 2, f"An√°lise por categoria insuficiente: {found_categories}"
        
        # Verificar elementos de compara√ß√£o
        comparison_terms = ["vs", "mercado", "benchmark", "diferen√ßa", "gap"]
        found_comparisons = [term for term in comparison_terms if term.lower() in result.lower()]
        assert len(found_comparisons) >= 2, f"Elementos de compara√ß√£o ausentes: {found_comparisons}"
        
        self.log_test("SUCCESS", "Pricing Analysis validado",
                     pricing_terms=len(found_terms),
                     categories_found=len(found_categories))
    
    def test_trend_comparison_analysis(self):
        """Teste da compara√ß√£o de tend√™ncias."""
        self.log_test("INFO", "Testando Trend Comparison")
        
        result = self.competitive_tool._run(
            analysis_type="trend_comparison",
            data_csv=self.real_data_path,
            benchmark_period="monthly",
            include_recommendations=True
        )
        
        # Valida√ß√µes b√°sicas
        assert isinstance(result, str), "Resultado deve ser string"
        assert len(result) > 500, "Resultado muito curto para trend comparison"
        assert "error" not in result.lower(), f"Erro detectado: {result[:200]}..."
        
        # Verificar conte√∫do espec√≠fico de tend√™ncias
        trend_terms = ["tend√™ncia", "trend", "crescimento", "performance", "mercado", "sazonal"]
        found_terms = [term for term in trend_terms if term.lower() in result.lower()]
        assert len(found_terms) >= 3, f"Poucos termos de tend√™ncia: {found_terms}"
        
        # Verificar an√°lise vs mercado
        market_comparison_terms = ["vs mercado", "performance vs", "benchmark", "outperforming", "underperforming"]
        found_comparisons = [term for term in market_comparison_terms if term.lower() in result.lower()]
        assert len(found_comparisons) >= 1, f"Compara√ß√£o vs mercado ausente: {found_comparisons}"
        
        # Verificar an√°lise sazonal
        seasonal_terms = ["sazonal", "seasonal", "pico", "peak", "varia√ß√£o"]
        found_seasonal = [term for term in seasonal_terms if term.lower() in result.lower()]
        assert len(found_seasonal) >= 1, f"An√°lise sazonal ausente: {found_seasonal}"
        
        self.log_test("SUCCESS", "Trend Comparison validado",
                     trend_terms=len(found_terms),
                     market_comparisons=len(found_comparisons))
    
    def test_market_share_estimation_analysis(self):
        """Teste da estimativa de market share."""
        self.log_test("INFO", "Testando Market Share Estimation")
        
        result = self.competitive_tool._run(
            analysis_type="market_share_estimation",
            data_csv=self.real_data_path,
            market_segment="joalherias",
            include_recommendations=True
        )
        
        # Valida√ß√µes b√°sicas
        assert isinstance(result, str), "Resultado deve ser string"
        assert len(result) > 700, "Resultado muito curto para market share"
        assert "error" not in result.lower(), f"Erro detectado: {result[:200]}..."
        
        # Verificar conte√∫do espec√≠fico de market share
        share_terms = ["market share", "participa√ß√£o", "share", "mercado", "regional", "nacional"]
        found_terms = [term for term in share_terms if term.lower() in result.lower()]
        assert len(found_terms) >= 3, f"Poucos termos de market share: {found_terms}"
        
        # Verificar estimativas percentuais
        percentage_indicators = ["%", "percent", "porcent"]
        found_percentages = [ind for ind in percentage_indicators if ind in result.lower()]
        assert len(found_percentages) >= 1, f"Estimativas percentuais ausentes: {found_percentages}"
        
        # Verificar an√°lise competitiva
        competitive_terms = ["concorrente", "competitor", "l√≠der", "leader", "posi√ß√£o"]
        found_competitive = [term for term in competitive_terms if term.lower() in result.lower()]
        assert len(found_competitive) >= 1, f"An√°lise competitiva ausente: {found_competitive}"
        
        # Verificar potencial de crescimento
        growth_terms = ["crescimento", "growth", "potencial", "oportunidade", "expans√£o"]
        found_growth = [term for term in growth_terms if term.lower() in result.lower()]
        assert len(found_growth) >= 2, f"An√°lise de crescimento insuficiente: {found_growth}"
        
        self.log_test("SUCCESS", "Market Share Estimation validado",
                     share_terms=len(found_terms),
                     growth_indicators=len(found_growth))
    
    def test_competitive_gaps_analysis(self):
        """Teste da identifica√ß√£o de gaps competitivos."""
        self.log_test("INFO", "Testando Competitive Gaps")
        
        result = self.competitive_tool._run(
            analysis_type="competitive_gaps",
            data_csv=self.real_data_path,
            include_recommendations=True,
            risk_tolerance="high"
        )
        
        # Valida√ß√µes b√°sicas
        assert isinstance(result, str), "Resultado deve ser string"
        assert len(result) > 800, "Resultado muito curto para competitive gaps"
        assert "error" not in result.lower(), f"Erro detectado: {result[:200]}..."
        
        # Verificar conte√∫do espec√≠fico de gaps
        gap_terms = ["gap", "oportunidade", "opportunity", "lacuna", "competitiv"]
        found_terms = [term for term in gap_terms if term.lower() in result.lower()]
        assert len(found_terms) >= 3, f"Poucos termos de gaps: {found_terms}"
        
        # Verificar matriz de oportunidades
        matrix_terms = ["matriz", "matrix", "prioridade", "priority", "alta", "high"]
        found_matrix = [term for term in matrix_terms if term.lower() in result.lower()]
        assert len(found_matrix) >= 2, f"Matriz de oportunidades ausente: {found_matrix}"
        
        # Verificar gaps por categoria
        category_gap_terms = ["categoria", "category", "operational", "operacional", "pricing"]
        found_category_gaps = [term for term in category_gap_terms if term.lower() in result.lower()]
        assert len(found_category_gaps) >= 2, f"Gaps por categoria ausentes: {found_category_gaps}"
        
        # Verificar recomenda√ß√µes estrat√©gicas
        recommendation_terms = ["recomenda√ß√£o", "recommendation", "estrat√©gi", "strategic", "a√ß√£o"]
        found_recommendations = [term for term in recommendation_terms if term.lower() in result.lower()]
        assert len(found_recommendations) >= 2, f"Recomenda√ß√µes estrat√©gicas ausentes: {found_recommendations}"
        
        self.log_test("SUCCESS", "Competitive Gaps validado",
                     gap_terms=len(found_terms),
                     matrix_indicators=len(found_matrix),
                     recommendations=len(found_recommendations))
    
    # ==========================================
    # TESTES DE INTEGRA√á√ÉO E PERFORMANCE
    # ==========================================
    
    def test_all_analysis_types_integration(self):
        """Teste de integra√ß√£o de todos os tipos de an√°lise competitiva."""
        self.log_test("INFO", "Testando integra√ß√£o de todas as an√°lises competitivas")
        
        analysis_types = [
            "market_position", "pricing_analysis", "trend_comparison",
            "market_share_estimation", "competitive_gaps"
        ]
        
        results = {}
        total_time = 0
        
        for analysis_type in analysis_types:
            start_time = time.time()
            
            try:
                result = self.competitive_tool._run(
                    analysis_type=analysis_type,
                    data_csv=self.real_data_path,
                    market_segment="joalherias",
                    benchmark_period="quarterly",
                    include_recommendations=True,
                    risk_tolerance="medium"
                )
                
                execution_time = time.time() - start_time
                total_time += execution_time
                
                success = "error" not in result.lower() and len(result) > 400
                
                results[analysis_type] = {
                    'success': success,
                    'execution_time': round(execution_time, 2),
                    'output_length': len(result)
                }
                
                self.log_test("SUCCESS" if success else "ERROR",
                             f"An√°lise {analysis_type}",
                             **results[analysis_type])
                
            except Exception as e:
                results[analysis_type] = {
                    'success': False,
                    'error': str(e),
                    'execution_time': time.time() - start_time
                }
                self.log_test("ERROR", f"Erro em {analysis_type}: {str(e)}")
        
        # Valida√ß√µes de integra√ß√£o
        successful = [name for name, res in results.items() if res['success']]
        success_rate = len(successful) / len(analysis_types)
        
        assert success_rate >= 0.80, f"Taxa de sucesso baixa: {success_rate:.1%}"  # 4/5 = 80%
        assert total_time < 120, f"Tempo total excessivo: {total_time:.1f}s"
        
        self.log_test("SUCCESS", "Integra√ß√£o de an√°lises competitivas validada",
                     success_rate=f"{success_rate:.1%}",
                     total_time=f"{total_time:.1f}s",
                     successful_analyses=successful)
        
        return results
    
    def test_performance_benchmarks(self):
        """Teste de benchmarks de performance."""
        self.log_test("INFO", "Testando benchmarks de performance")
        
        # Teste com an√°lise mais r√°pida
        start_time = time.time()
        tracemalloc.start()
        
        result = self.competitive_tool._run(
            analysis_type="pricing_analysis",
            data_csv=self.real_data_path,
            market_segment="joalherias",
            include_recommendations=False  # Desabilitar para acelerar
        )
        
        execution_time = time.time() - start_time
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        # Valida√ß√µes de performance
        assert execution_time < 45, f"Execu√ß√£o muito lenta: {execution_time:.2f}s"
        assert peak < 512 * 1024 * 1024, f"Uso de mem√≥ria excessivo: {peak/1024/1024:.1f}MB"
        assert len(result) > 400, "Resultado muito curto"
        assert "error" not in result.lower(), "Erro na execu√ß√£o"
        
        self.log_test("SUCCESS", "Performance validada",
                     execution_time=f"{execution_time:.2f}s",
                     memory_peak=f"{peak/1024/1024:.1f}MB")
    
    def test_different_market_segments(self):
        """Teste com diferentes segmentos de mercado."""
        self.log_test("INFO", "Testando diferentes segmentos de mercado")
        
        segments = ["joalherias", "relogios", "acessorios"]
        results = {}
        
        for segment in segments:
            try:
                result = self.competitive_tool._run(
                    analysis_type="market_position",
                    data_csv=self.real_data_path,
                    market_segment=segment,
                    include_recommendations=False
                )
                
                success = "error" not in result.lower() and len(result) > 300
                results[segment] = {'success': success, 'length': len(result)}
                
            except Exception as e:
                results[segment] = {'success': False, 'error': str(e)}
        
        # Valida√ß√µes
        successful_segments = [seg for seg, res in results.items() if res['success']]
        success_rate = len(successful_segments) / len(segments)
        
        # Todos os segmentos devem funcionar (fallback para joalherias)
        assert success_rate >= 1.0, f"Nem todos os segmentos funcionaram: {successful_segments}"
        
        self.log_test("SUCCESS", "Segmentos de mercado validados",
                     segments_tested=len(segments),
                     successful=len(successful_segments))
    
    # ==========================================
    # TESTES DE TRATAMENTO DE ERROS
    # ==========================================
    
    def test_error_handling_comprehensive(self):
        """Teste abrangente de tratamento de erros."""
        self.log_test("INFO", "Testando tratamento de erros")
        
        error_tests = []
        
        # 1. Arquivo inexistente
        try:
            result = self.competitive_tool._run(
                analysis_type="market_position",
                data_csv="arquivo_inexistente.csv"
            )
            handled = "error" in result.lower() or "n√£o encontrado" in result.lower()
            error_tests.append(('arquivo_inexistente', handled))
        except Exception:
            error_tests.append(('arquivo_inexistente', False))
        
        # 2. Tipo de an√°lise inv√°lido
        try:
            result = self.competitive_tool._run(
                analysis_type="analise_competitiva_invalida",
                data_csv=self.real_data_path
            )
            handled = "error" in result.lower() or "n√£o suportada" in result.lower()
            error_tests.append(('tipo_invalido', handled))
        except Exception:
            error_tests.append(('tipo_invalido', False))
        
        # 3. Segmento de mercado inv√°lido (deve usar fallback)
        try:
            result = self.competitive_tool._run(
                analysis_type="market_position",
                data_csv=self.real_data_path,
                market_segment="segmento_inexistente"
            )
            # Deve funcionar com fallback para joalherias
            handled = "error" not in result.lower() and len(result) > 300
            error_tests.append(('segmento_invalido_fallback', handled))
        except Exception:
            error_tests.append(('segmento_invalido_fallback', False))
        
        # 4. Per√≠odo de benchmark inv√°lido
        try:
            result = self.competitive_tool._run(
                analysis_type="trend_comparison",
                data_csv=self.real_data_path,
                benchmark_period="periodo_invalido"
            )
            # Deve usar fallback ou dar erro
            handled = True  # Qualquer tratamento √© v√°lido
            error_tests.append(('periodo_invalido', handled))
        except Exception:
            error_tests.append(('periodo_invalido', False))
        
        # Valida√ß√µes
        passed = sum(1 for _, handled in error_tests if handled)
        success_rate = passed / len(error_tests)
        
        assert success_rate >= 0.75, f"Poucos erros tratados: {passed}/{len(error_tests)}"
        
        self.log_test("SUCCESS", "Tratamento de erros validado",
                     tests_passed=f"{passed}/{len(error_tests)}",
                     success_rate=f"{success_rate:.1%}")
    
    def test_edge_cases(self):
        """Teste de casos extremos."""
        self.log_test("INFO", "Testando casos extremos")
        
        edge_cases = []
        
        # 1. Toler√¢ncia a risco extrema
        try:
            result = self.competitive_tool._run(
                analysis_type="competitive_gaps",
                data_csv=self.real_data_path,
                risk_tolerance="high",
                include_recommendations=True
            )
            success = len(result) > 400 and "error" not in result.lower()
            edge_cases.append(('risk_tolerance_high', success))
        except Exception as e:
            edge_cases.append(('risk_tolerance_high', False))
        
        # 2. Sem recomenda√ß√µes
        try:
            result = self.competitive_tool._run(
                analysis_type="market_position",
                data_csv=self.real_data_path,
                include_recommendations=False
            )
            success = len(result) > 300 and "error" not in result.lower()
            edge_cases.append(('no_recommendations', success))
        except Exception as e:
            edge_cases.append(('no_recommendations', False))
        
        # 3. Per√≠odo anual (mais dados)
        try:
            result = self.competitive_tool._run(
                analysis_type="trend_comparison",
                data_csv=self.real_data_path,
                benchmark_period="yearly"
            )
            success = len(result) > 300 and "error" not in result.lower()
            edge_cases.append(('yearly_period', success))
        except Exception as e:
            edge_cases.append(('yearly_period', False))
        
        # Valida√ß√µes
        passed = sum(1 for _, success in edge_cases if success)
        success_rate = passed / len(edge_cases)
        
        assert success_rate >= 0.67, f"Poucos edge cases tratados: {passed}/{len(edge_cases)} ({success_rate:.1%})"
        
        self.log_test("SUCCESS", "Edge cases validados",
                     tests_passed=f"{passed}/{len(edge_cases)}")
    
    def test_invalid_parameters(self):
        """Teste com par√¢metros completamente inv√°lidos."""
        self.log_test("INFO", "Testando par√¢metros inv√°lidos")
        
        invalid_tests = []
        
        # 1. Risk tolerance inv√°lido
        try:
            result = self.competitive_tool._run(
                analysis_type="market_position",
                data_csv=self.real_data_path,
                risk_tolerance="invalid_risk"
            )
            # Deve usar fallback ou dar erro
            handled = True  # Qualquer tratamento √© v√°lido
            invalid_tests.append(('invalid_risk_tolerance', handled))
        except Exception:
            invalid_tests.append(('invalid_risk_tolerance', False))
        
        # 2. M√∫ltiplos par√¢metros inv√°lidos
        try:
            result = self.competitive_tool._run(
                analysis_type="pricing_analysis",
                data_csv=self.real_data_path,
                market_segment="invalid_segment",
                benchmark_period="invalid_period",
                risk_tolerance="invalid_risk"
            )
            # Deve funcionar com fallbacks
            handled = "error" not in result.lower() or len(result) > 100
            invalid_tests.append(('multiple_invalid', handled))
        except Exception:
            invalid_tests.append(('multiple_invalid', False))
        
        # Valida√ß√µes
        passed = sum(1 for _, handled in invalid_tests if handled)
        success_rate = passed / len(invalid_tests) if invalid_tests else 1.0
        
        assert success_rate >= 0.50, f"Poucos par√¢metros inv√°lidos tratados: {passed}/{len(invalid_tests)}"
        
        self.log_test("SUCCESS", "Par√¢metros inv√°lidos validados",
                     tests_passed=f"{passed}/{len(invalid_tests)}")
    
    # ==========================================
    # TESTES DE QUALIDADE DE SA√çDA
    # ==========================================
    
    def test_output_quality_and_formatting(self):
        """Teste de qualidade e formata√ß√£o da sa√≠da."""
        self.log_test("INFO", "Testando qualidade da sa√≠da")
        
        result = self.competitive_tool._run(
            analysis_type="market_position",
            data_csv=self.real_data_path,
            market_segment="joalherias",
            include_recommendations=True
        )
        
        # Valida√ß√µes de formato
        assert isinstance(result, str), "Resultado deve ser string"
        assert len(result) > 700, "Resultado muito curto"
        
        # Verificar estrutura markdown
        markdown_elements = ["#", "**", "-", "*", "üèÜ"]
        found_markdown = [elem for elem in markdown_elements if elem in result]
        assert len(found_markdown) >= 4, f"Pouco markdown encontrado: {found_markdown}"
        
        # Verificar se√ß√µes esperadas de intelig√™ncia competitiva
        expected_sections = ["competitive intelligence", "intelig√™ncia competitiva", "an√°lise", "benchmark"]
        found_sections = [section for section in expected_sections 
                         if section.lower() in result.lower()]
        assert len(found_sections) >= 2, f"Poucas se√ß√µes encontradas: {found_sections}"
        
        # Verificar informa√ß√µes t√©cnicas espec√≠ficas
        tech_info = ["competitive intelligence tool", "benchmark", "mercado", "an√°lise"]
        found_tech = [info for info in tech_info if info.lower() in result.lower()]
        assert len(found_tech) >= 3, f"Pouca informa√ß√£o t√©cnica: {found_tech}"
        
        # Verificar presen√ßa de dados num√©ricos (percentuais, valores)
        numeric_indicators = ["%", "r$", "pontos percentuais", "vs", "mercado"]
        found_numeric = [ind for ind in numeric_indicators if ind.lower() in result.lower()]
        assert len(found_numeric) >= 2, f"Poucos indicadores num√©ricos: {found_numeric}"
        
        # Verificar metadados e disclaimer
        metadata_terms = ["metadados", "metadata", "disclaimer", "fonte", "atualiza√ß√£o"]
        found_metadata = [term for term in metadata_terms if term.lower() in result.lower()]
        assert len(found_metadata) >= 1, f"Metadados ausentes: {found_metadata}"
        
        self.log_test("SUCCESS", "Qualidade da sa√≠da validada",
                     result_length=len(result),
                     format_elements=len(found_markdown),
                     sections_found=len(found_sections),
                     tech_info_found=len(found_tech),
                     numeric_indicators=len(found_numeric))
    
    def test_benchmark_accuracy(self):
        """Teste de precis√£o dos benchmarks."""
        self.log_test("INFO", "Testando precis√£o dos benchmarks")
        
        # Carregar benchmarks
        benchmarks = self.competitive_tool._load_market_benchmarks('joalherias')
        
        # Verificar valores realistas para o setor brasileiro
        market_size = benchmarks['market_size_billion_brl']
        assert 5.0 <= market_size <= 15.0, f"Market size irreal: {market_size}B"
        
        growth_rate = benchmarks['annual_growth_rate']
        assert 0.01 <= growth_rate <= 0.10, f"Taxa de crescimento irreal: {growth_rate*100:.1f}%"
        
        # Verificar faixas de pre√ßo consistentes
        avg_ticket = benchmarks['average_ticket']
        assert avg_ticket['economy']['max'] < avg_ticket['mid']['min'], "Faixas de pre√ßo sobrepostas"
        assert avg_ticket['mid']['max'] < avg_ticket['premium']['min'], "Faixas mid/premium sobrepostas"
        assert avg_ticket['premium']['max'] < avg_ticket['luxury']['min'], "Faixas premium/luxury sobrepostas"
        
        # Verificar distribui√ß√£o de categorias soma 100%
        cat_dist = benchmarks['category_distribution']
        total_distribution = sum(cat_dist.values())
        assert 0.95 <= total_distribution <= 1.05, f"Distribui√ß√£o de categorias inconsistente: {total_distribution:.2f}"
        
        # Verificar margens realistas
        margins = benchmarks['margin_benchmarks']
        assert 0.30 <= margins['gross_margin_avg'] <= 0.80, f"Margem bruta irreal: {margins['gross_margin_avg']:.1%}"
        assert margins['net_margin_avg'] < margins['gross_margin_avg'], "Margem l√≠quida > bruta"
        
        self.log_test("SUCCESS", "Benchmarks validados",
                     market_size=f"{market_size}B BRL",
                     growth_rate=f"{growth_rate*100:.1f}%",
                     categories=len(cat_dist))
    
    def test_recommendations_quality(self):
        """Teste de qualidade das recomenda√ß√µes."""
        self.log_test("INFO", "Testando qualidade das recomenda√ß√µes")
        
        # Testar com diferentes tipos de an√°lise
        analysis_types = ["competitive_gaps", "market_position", "pricing_analysis"]
        recommendation_quality = {}
        
        for analysis_type in analysis_types:
            result = self.competitive_tool._run(
                analysis_type=analysis_type,
                data_csv=self.real_data_path,
                include_recommendations=True,
                risk_tolerance="medium"
            )
            
            # Contar recomenda√ß√µes
            recommendation_indicators = ["recomenda", "sugest", "deveria", "considerar", "avaliar", "focar"]
            found_recommendations = sum(1 for indicator in recommendation_indicators 
                                      if indicator.lower() in result.lower())
            
            # Verificar acionabilidade das recomenda√ß√µes
            actionable_terms = ["implementar", "expandir", "melhorar", "aumentar", "reduzir", "focar"]
            found_actionable = sum(1 for term in actionable_terms 
                                 if term.lower() in result.lower())
            
            recommendation_quality[analysis_type] = {
                'recommendation_count': found_recommendations,
                'actionable_count': found_actionable,
                'quality_score': (found_recommendations + found_actionable) / 2
            }
        
        # Valida√ß√µes
        avg_quality = sum(rq['quality_score'] for rq in recommendation_quality.values()) / len(recommendation_quality)
        assert avg_quality >= 2.0, f"Qualidade baixa das recomenda√ß√µes: {avg_quality:.1f}"
        
        # Pelo menos uma an√°lise deve ter recomenda√ß√µes robustas
        best_analysis = max(recommendation_quality.items(), key=lambda x: x[1]['quality_score'])
        assert best_analysis[1]['quality_score'] >= 3.0, f"Nenhuma an√°lise com recomenda√ß√µes robustas"
        
        self.log_test("SUCCESS", "Qualidade das recomenda√ß√µes validada",
                     avg_quality_score=f"{avg_quality:.1f}",
                     best_analysis=best_analysis[0],
                     best_score=f"{best_analysis[1]['quality_score']:.1f}")
    
    def test_competitive_insights_depth(self):
        """Teste da profundidade dos insights competitivos."""
        self.log_test("INFO", "Testando profundidade dos insights")
        
        result = self.competitive_tool._run(
            analysis_type="competitive_gaps",
            data_csv=self.real_data_path,
            include_recommendations=True
        )
        
        # Verificar an√°lise multi-dimensional
        analysis_dimensions = [
            "categoria", "pre√ßo", "operacional", "digital", "sazonal", 
            "market share", "crescimento", "benchmark"
        ]
        found_dimensions = [dim for dim in analysis_dimensions 
                           if dim.lower() in result.lower()]
        assert len(found_dimensions) >= 4, f"An√°lise superficial: {found_dimensions}"
        
        # Verificar compara√ß√µes quantitativas
        quantitative_terms = ["%", "vs", "acima", "abaixo", "maior", "menor", "gap"]
        found_quantitative = [term for term in quantitative_terms 
                             if term.lower() in result.lower()]
        assert len(found_quantitative) >= 5, f"Poucas compara√ß√µes quantitativas: {found_quantitative}"
        
        # Verificar insights estrat√©gicos
        strategic_terms = ["estrat√©gi", "strategic", "oportunidade", "opportunity", "prioridade", "priority"]
        found_strategic = [term for term in strategic_terms 
                          if term.lower() in result.lower()]
        assert len(found_strategic) >= 3, f"Poucos insights estrat√©gicos: {found_strategic}"
        
        self.log_test("SUCCESS", "Profundidade dos insights validada",
                     analysis_dimensions=len(found_dimensions),
                     quantitative_elements=len(found_quantitative),
                     strategic_insights=len(found_strategic))
    
    def teardown_method(self, method):
        """Limpeza ap√≥s cada teste."""
        test_name = method.__name__
        duration = time.time() - self.start_time
        
        # Salvar logs do teste
        log_dir = Path("test_logs")
        log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"{test_name}_{timestamp}_competitive_tool.json"
        
        log_data = {
            'test_name': test_name,
            'timestamp': timestamp,
            'duration': round(duration, 2),
            'tool_version': 'Competitive Intelligence Tool V1.0',
            'logs': self.test_logs
        }
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
        
        print(f"üìÅ Log salvo: {log_file}")


if __name__ == "__main__":
    # Executar teste standalone
    test_instance = TestCompetitiveIntelligenceTool()
    
    # Setup standalone
    test_instance.setup_standalone("data/vendas.csv")
    
    print("üèÜ Executando testes Competitive Intelligence Tool V1.0...")
    print("=" * 70)
    
    # Lista de testes principais
    main_tests = [
        test_instance.test_data_loading_and_validation,
        test_instance.test_benchmark_loading,
        test_instance.test_input_validation,
        test_instance.test_market_position_analysis,
        test_instance.test_pricing_analysis,
        test_instance.test_trend_comparison_analysis,
        test_instance.test_market_share_estimation_analysis,
        test_instance.test_competitive_gaps_analysis,
        test_instance.test_all_analysis_types_integration,
        test_instance.test_performance_benchmarks,
        test_instance.test_different_market_segments,
        test_instance.test_error_handling_comprehensive,
        test_instance.test_edge_cases,
        test_instance.test_invalid_parameters,
        test_instance.test_output_quality_and_formatting,
        test_instance.test_benchmark_accuracy,
        test_instance.test_recommendations_quality,
        test_instance.test_competitive_insights_depth
    ]
    
    passed = 0
    total = len(main_tests)
    failed_tests = []
    
    for test_func in main_tests:
        try:
            print(f"\n{'='*70}")
            print(f"üîÑ Executando: {test_func.__name__}")
            print("-" * 70)
            
            test_func()
            print(f"‚úÖ {test_func.__name__} - PASSOU")
            passed += 1
            
        except Exception as e:
            print(f"‚ùå {test_func.__name__} - FALHOU: {str(e)}")
            failed_tests.append((test_func.__name__, str(e)))
            
        finally:
            test_instance.teardown_method(test_func)
    
    # Relat√≥rio final
    print(f"\n{'='*70}")
    print(f"üèÜ RELAT√ìRIO FINAL - COMPETITIVE INTELLIGENCE TOOL V1.0")
    print(f"{'='*70}")
    print(f"‚úÖ Testes Aprovados: {passed}/{total} ({passed/total:.1%})")
    print(f"‚ùå Testes Falharam: {len(failed_tests)}")
    
    if failed_tests:
        print(f"\nüìã TESTES QUE FALHARAM:")
        for test_name, error in failed_tests:
            print(f"  - {test_name}: {error[:100]}...")
    
    if passed == total:
        print(f"\nüéâ TODOS OS TESTES PASSARAM!")
        print(f"üöÄ Competitive Intelligence Tool est√° funcionando perfeitamente!")
    elif passed >= total * 0.85:
        print(f"\n‚úÖ MAIORIA DOS TESTES PASSOU ({passed/total:.1%})")
        print(f"üîß Algumas funcionalidades podem precisar de ajustes")
    else:
        print(f"\n‚ö†Ô∏è MUITOS TESTES FALHARAM ({passed/total:.1%})")
        print(f"üõ†Ô∏è Tool precisa de corre√ß√µes significativas")
    
    print(f"\nüìä COBERTURA DE TESTES:")
    print(f"  - ‚úÖ Valida√ß√£o de dados e benchmarks")
    print(f"  - ‚úÖ Prepara√ß√£o de dados competitivos")
    print(f"  - ‚úÖ 5 tipos de an√°lise competitiva")
    print(f"  - ‚úÖ Integra√ß√£o end-to-end")
    print(f"  - ‚úÖ Performance e otimiza√ß√µes")
    print(f"  - ‚úÖ Tratamento de erros robusto")
    print(f"  - ‚úÖ Casos extremos e par√¢metros inv√°lidos")
    print(f"  - ‚úÖ Segmentos de mercado")
    print(f"  - ‚úÖ Qualidade de sa√≠da e formata√ß√£o")
    print(f"  - ‚úÖ Precis√£o de benchmarks setoriais")
    print(f"  - ‚úÖ Qualidade de recomenda√ß√µes")
    print(f"  - ‚úÖ Profundidade de insights")
    
    print(f"\nüîß FUNCIONALIDADES TESTADAS:")
    print(f"  - Market Position (Posicionamento vs. mercado)")
    print(f"  - Pricing Analysis (An√°lise competitiva de pre√ßos)")
    print(f"  - Trend Comparison (Compara√ß√£o de tend√™ncias)")
    print(f"  - Market Share Estimation (Estimativa de market share)")
    print(f"  - Competitive Gaps (Identifica√ß√£o de gaps e oportunidades)")
    print(f"  - Benchmarks setoriais brasileiros")
    print(f"  - Matriz de oportunidades priorizadas")
    print(f"  - Recomenda√ß√µes estrat√©gicas acion√°veis")
    print(f"  - An√°lise multi-dimensional (pre√ßo, categoria, operacional)")
    print(f"  - Elasticidade de pre√ßos e sazonalidade")
    print(f"  - Compara√ß√£o vs. principais concorrentes")
    print(f"  - Potencial de crescimento e expans√£o")
    
    print(f"\nüìà BENCHMARKS DE PERFORMANCE:")
    print(f"  - ‚è±Ô∏è Tempo total: < 2 minutos para todas as an√°lises")
    print(f"  - üíæ Mem√≥ria: < 512 MB por an√°lise")
    print(f"  - üéØ Taxa de sucesso alvo: ‚â• 85%")
    print(f"  - üìä Cobertura: 18 testes abrangentes")
    print(f"  - üîß Tratamento de erros: ‚â• 75% dos casos")
    
    print(f"\nüè™ ESPECIALIZA√á√ÉO SETORIAL:")
    print(f"  - Mercado brasileiro de joalherias (R$ 6.8B)")
    print(f"  - Benchmarks por faixa de pre√ßo (Economy ‚Üí Ultra-Luxury)")
    print(f"  - Padr√µes sazonais (Maio/Dezembro)")
    print(f"  - Participa√ß√£o dos principais players")
    print(f"  - Margens e m√©tricas operacionais do setor")
    
    print(f"\nüéØ PR√ìXIMOS PASSOS SUGERIDOS:")
    print(f"  - Integrar com SQL Query Tool para dados atualizados")
    print(f"  - Expandir benchmarks para outros segmentos")
    print(f"  - Adicionar an√°lise de pricing din√¢mico")
    print(f"  - Implementar alertas de mudan√ßas competitivas")
    print(f"  - Desenvolver dashboards interativos")
    
    print(f"\n*Relat√≥rio gerado por Test Suite - Competitive Intelligence Tool V1.0*") 