#!/usr/bin/env python
"""
üöÄ INSIGHTS-AI FLOW - Vers√£o Otimizada com CrewAI Flows

Caracter√≠sticas:
- Execu√ß√£o paralela e ass√≠ncrona
- Controle de fluxo inteligente  
- Recovery autom√°tico de falhas
- Monitoramento em tempo real
- Estado persistido e rastre√°vel
"""

import asyncio
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Union
from pathlib import Path
import json
import psutil

from crewai.flow.flow import Flow, listen, start, router, and_, or_
from crewai.flow.persistence import persist
from pydantic import BaseModel, Field
from crewai import Agent, Crew, Task

# Importar componentes existentes
from insights.crew import Insights
from insights.tools.sql_query_tool import SQLServerQueryTool
from insights.tools.file_generation_tool import FileGenerationTool

# =============== CONFIGURA√á√ÉO DE LOGGING PARA FLOW ===============

def setup_flow_logging():
    """Configurar logging especializado para Flow"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    logs_dir = Path("logs/flow_executions")
    logs_dir.mkdir(parents=True, exist_ok=True)
    
    log_file = logs_dir / f"flow_execution_{timestamp}.log"
    
    flow_logger = logging.getLogger('insights_flow')
    flow_logger.setLevel(logging.INFO)
    
    # Remover handlers existentes
    for handler in flow_logger.handlers[:]:
        flow_logger.removeHandler(handler)
    
    # Handler para arquivo
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # Handler para console
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # Formata√ß√£o
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)8s | FLOW | %(funcName)s:%(lineno)d | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    flow_logger.addHandler(file_handler)
    flow_logger.addHandler(console_handler)
    flow_logger.propagate = False
    
    flow_logger.info("üöÄ INSIGHTS-AI FLOW - LOGGING INICIADO")
    flow_logger.info(f"üìÅ Arquivo de log: {log_file}")
    
    return flow_logger, str(log_file)

# Configurar logger global do Flow
flow_logger, flow_log_file = setup_flow_logging()

# =============== ESTADO ESTRUTURADO DO FLOW ===============

class DataQuality(BaseModel):
    """Modelo para qualidade de dados"""
    total_registros: int = 0
    completude_percent: float = 0.0
    consistencia_percent: float = 0.0
    anomalias_detectadas: int = 0
    gaps_temporais: int = 0
    score_confiabilidade: float = 0.0
    
    class Config:
        """Configura√ß√£o para serializa√ß√£o JSON"""
        json_encoders = {
            # Qualquer encoder especial se necess√°rio
        }

class AnalysisResult(BaseModel):
    """Modelo base para resultados de an√°lises"""
    status: str = "pendente"  # pendente, executando, concluido, erro
    inicio_execucao: Optional[str] = None  # ISO format timestamp
    fim_execucao: Optional[str] = None     # ISO format timestamp
    tempo_execucao: float = 0.0
    resultado: Optional[Dict] = None
    erro_mensagem: Optional[str] = None
    confidence_score: float = 0.0

class InsightsFlowState(BaseModel):
    """Estado estruturado completo do Flow de Insights"""
    
    # =============== INPUTS DO USU√ÅRIO ===============
    data_inicio: str = ""
    data_fim: str = ""
    modo_execucao: str = "completo"  # completo, rapido, recovery
    
    # =============== CONTROLE DE EXECU√á√ÉO ===============
    flow_id: str = Field(default_factory=lambda: f"flow_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    inicio_flow: Optional[str] = None  # ISO format timestamp
    fase_atual: str = "inicializando"
    progresso_percent: float = 0.0
    
    # =============== DADOS E QUALIDADE ===============
    dados_extraidos: bool = False
    qualidade_dados: Dict[str, Any] = Field(default_factory=dict)
    dataset_path: Optional[str] = None
    dados_validados: bool = False
    
    # =============== RESULTADOS DAS AN√ÅLISES ===============
    # An√°lises b√°sicas
    engenharia_dados: Dict[str, Any] = Field(default_factory=dict)
    analise_tendencias: Dict[str, Any] = Field(default_factory=dict)
    analise_sazonalidade: Dict[str, Any] = Field(default_factory=dict)
    analise_segmentos: Dict[str, Any] = Field(default_factory=dict)
    analise_inventario: Dict[str, Any] = Field(default_factory=dict)
    analise_financeira: Dict[str, Any] = Field(default_factory=dict)
    analise_clientes_rfv: Dict[str, Any] = Field(default_factory=dict)
    analise_estoque: Dict[str, Any] = Field(default_factory=dict)
    analise_vendedores: Dict[str, Any] = Field(default_factory=dict)
    
    # An√°lises dependentes
    analise_projecoes: Dict[str, Any] = Field(default_factory=dict)
    
    # An√°lises avan√ßadas
    analise_clientes_avancada: Dict[str, Any] = Field(default_factory=dict)
    analise_produtos_avancada: Dict[str, Any] = Field(default_factory=dict)
    analise_financeira_avancada: Dict[str, Any] = Field(default_factory=dict)
    analise_estoque_avancada: Dict[str, Any] = Field(default_factory=dict)
    analise_vendedores_performance: Dict[str, Any] = Field(default_factory=dict)
    
    # Relat√≥rios finais
    dashboard_html_dinamico: Dict[str, Any] = Field(default_factory=dict)
    relatorio_executivo_completo: Dict[str, Any] = Field(default_factory=dict)
    sintese_estrategica: Dict[str, Any] = Field(default_factory=dict)
    
    # =============== ESTADO GERAL ===============
    analises_concluidas: List[str] = []
    analises_em_execucao: List[str] = []
    erros_detectados: List[str] = []
    warnings: List[str] = []
    
    # =============== FLAGS DE CONTROLE ===============
    pode_executar_analises_basicas: bool = False
    pode_executar_projecoes: bool = False
    pode_gerar_relatorio_final: bool = False
    execucao_completa: bool = False
    
    # =============== M√âTRICAS DE PERFORMANCE ===============
    tempo_total_execucao: float = 0.0
    tempo_por_analise: Dict[str, float] = {}
    memoria_utilizada_mb: float = 0.0
    
    # =============== OUTPUTS GERADOS ===============
    arquivos_gerados: List[str] = []
    relatorio_final_path: Optional[str] = None
    dashboard_path: Optional[str] = None
    
    # =============== ETAPA 3: SISTEMA AVAN√áADO ===============
    etapa3_habilitada: bool = True
    ultima_atividade: float = Field(default_factory=time.time)  # Para monitoramento

# =============== CLASSE PRINCIPAL DO FLOW ===============

@persist()  # Persist√™ncia autom√°tica de estado
class InsightsFlow(Flow[InsightsFlowState]):
    """
    üöÄ FLOW PRINCIPAL DO INSIGHTS-AI
    
    Caracter√≠sticas:
    - Execu√ß√£o otimizada com paraleliza√ß√£o
    - Recovery autom√°tico de falhas
    - Monitoramento em tempo real
    - Estado persistido e rastre√°vel
    - Compatibilidade total com crews existentes
    """
    
    def __init__(self, persistence=None, **kwargs):
        super().__init__(persistence=persistence, **kwargs)
        self.crews_cache = {}  # Cache de crews para reutiliza√ß√£o
        self.start_time = time.time()
        self.etapa3_controller = None  # Controlador da Etapa 3 (n√£o persistido)
        flow_logger.info("üîß Inicializando InsightsFlow...")
    
    # =============== M√âTODOS DE INICIALIZA√á√ÉO ===============
    
    @start()
    def inicializar_flow(self):
        """
        üöÄ PONTO DE ENTRADA DO FLOW
        Valida inputs, configura ambiente e prepara execu√ß√£o
        """
        flow_logger.info("üöÄ INICIANDO INSIGHTS-AI FLOW")
        flow_logger.info("=" * 60)
        
        self.state.inicio_flow = datetime.now().isoformat()
        self.state.fase_atual = "validacao_inputs"
        
        # Validar inputs obrigat√≥rios
        if not self.state.data_inicio or not self.state.data_fim:
            erro = "‚ùå Datas de in√≠cio e fim s√£o obrigat√≥rias"
            flow_logger.error(erro)
            self.state.erros_detectados.append(erro)
            return "erro_inputs"
        
        # Validar formato das datas
        try:
            datetime.strptime(self.state.data_inicio, '%Y-%m-%d')
            datetime.strptime(self.state.data_fim, '%Y-%m-%d')
            flow_logger.info(f"‚úÖ Datas validadas: {self.state.data_inicio} a {self.state.data_fim}")
        except ValueError as e:
            erro = f"‚ùå Formato de data inv√°lido: {e}"
            flow_logger.error(erro)
            self.state.erros_detectados.append(erro)
            return "erro_formato_data"
        
        # Log do setup inicial
        flow_logger.info(f"üìÖ Per√≠odo de an√°lise: {self.state.data_inicio} at√© {self.state.data_fim}")
        flow_logger.info(f"üîß Modo de execu√ß√£o: {self.state.modo_execucao}")
        flow_logger.info(f"üÜî Flow ID: {self.state.flow_id}")
        
        # =============== INICIALIZAR ETAPA 3 ===============
        if self.state.etapa3_habilitada:
            try:
                # Inicializar controlador da Etapa 3 (n√£o persistido)
                self.etapa3_controller = FlowEtapa3Controller(self.state)
                self.etapa3_controller.iniciar_sistemas_etapa3()
                flow_logger.info("üöÄ Etapa 3 - Sistemas avan√ßados inicializados")
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao inicializar Etapa 3: {e}")
                self.state.etapa3_habilitada = False
        
        self.state.progresso_percent = 10.0
        self.state.fase_atual = "preparacao_extracao"
        
        flow_logger.info("‚úÖ Inicializa√ß√£o conclu√≠da - Preparando extra√ß√£o de dados")
        return "inputs_validados"
    
    @listen(inicializar_flow)
    def extrair_e_processar_dados(self):
        """
        üîß EXTRA√á√ÉO E PROCESSAMENTO DE DADOS
        Usa o engenheiro de dados existente com melhorias de monitoramento
        """
        flow_logger.info("üîß INICIANDO EXTRA√á√ÉO DE DADOS")
        flow_logger.info("-" * 40)
        
        self.state.fase_atual = "extracao_dados"
        self.state.engenharia_dados = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("engenharia_dados")
        
        try:
            # Criar crew do engenheiro de dados se n√£o existir no cache
            if "engenheiro_dados" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Engenheiro de Dados...")
                insights_crew = Insights()
                self.crews_cache["engenheiro_dados"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Engenheiro de Dados em cache")
            
            # Preparar inputs para a extra√ß√£o
            inputs_extracao = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim
            }
            
            flow_logger.info(f"üìä Executando extra√ß√£o com inputs: {inputs_extracao}")
            
            # Executar apenas a task do engenheiro de dados
            crew_instance = self.crews_cache["engenheiro_dados"]
            
            # Criar um crew m√≠nimo apenas com engenheiro de dados
            engenheiro_crew = Crew(
                agents=[crew_instance.engenheiro_dados()],
                tasks=[crew_instance.engenheiro_dados_task()],
                verbose=True
            )
            
            # Executar extra√ß√£o
            inicio_extracao = time.time()
            resultado_extracao = engenheiro_crew.kickoff(inputs=inputs_extracao)
            tempo_extracao = time.time() - inicio_extracao
            
            # Processar resultado da extra√ß√£o
            self.state.engenharia_dados["fim_execucao"] = datetime.now().isoformat()
            self.state.engenharia_dados["tempo_execucao"] = tempo_extracao
            self.state.engenharia_dados["resultado"] = {"output": str(resultado_extracao)}
            self.state.engenharia_dados["status"] = "concluido"
            self.state.engenharia_dados["confidence_score"] = 95.0  # Score alto para extra√ß√£o bem-sucedida
            
            # Atualizar estado geral
            self.state.dados_extraidos = True
            self.state.analises_concluidas.append("engenharia_dados")
            self.state.analises_em_execucao.remove("engenharia_dados")
            self.state.tempo_por_analise["engenharia_dados"] = tempo_extracao
            
            # Simular avalia√ß√£o de qualidade dos dados
            self.avaliar_qualidade_dados()
            
            self.state.progresso_percent = 25.0
            self.state.fase_atual = "dados_prontos"
            
            flow_logger.info(f"‚úÖ EXTRA√á√ÉO CONCLU√çDA em {tempo_extracao:.2f}s")
            flow_logger.info(f"üìä Qualidade dos dados: {self.state.qualidade_dados.get('score_confiabilidade', 0):.1f}/100")
            flow_logger.info(f"üìÅ Total de registros: {self.state.qualidade_dados.get('total_registros', 0)}")
            
            return "dados_extraidos_com_sucesso"
            
        except Exception as e:
            # Tratar erro na extra√ß√£o
            inicio_timestamp = datetime.fromisoformat(self.state.engenharia_dados["inicio_execucao"]).timestamp()
            tempo_erro = time.time() - inicio_timestamp
            erro_msg = f"Erro na extra√ß√£o de dados: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.engenharia_dados["status"] = "erro"
            self.state.engenharia_dados["erro_mensagem"] = erro_msg
            self.state.engenharia_dados["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "engenharia_dados" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("engenharia_dados")
            
            return "erro_extracao_dados"
    
    @router(extrair_e_processar_dados)
    def validar_resultado_extracao(self):
        """
        üîç ROUTER: Decidir pr√≥ximos passos baseado no resultado da extra√ß√£o
        """
        flow_logger.info("üîç Validando resultado da extra√ß√£o...")
        
        if self.state.engenharia_dados.get("status") == "concluido":
            # Verificar qualidade dos dados
            if self.state.qualidade_dados.get("score_confiabilidade", 0) >= 70:
                flow_logger.info("‚úÖ Dados com qualidade adequada - Prosseguindo com an√°lises")
                self.state.pode_executar_analises_basicas = True
                return "dados_qualidade_ok"
            else:
                flow_logger.warning("‚ö†Ô∏è Qualidade dos dados abaixo do esperado - Modo conservador")
                self.state.warnings.append("Qualidade de dados abaixo de 70%")
                self.state.pode_executar_analises_basicas = True  # Permitir mesmo com qualidade baixa
                return "dados_qualidade_baixa"
        else:
            flow_logger.error("‚ùå Falha na extra√ß√£o - Tentando recovery")
            return "tentar_recovery"
    
    # =============== M√âTODOS DE RECOVERY ===============
    
    @listen("tentar_recovery")
    def recovery_extracao_dados(self):
        """
        üîÑ RECOVERY: Tentar extra√ß√£o alternativa em caso de falha
        """
        flow_logger.info("üîÑ INICIANDO RECOVERY DA EXTRA√á√ÉO")
        flow_logger.info("-" * 40)
        
        self.state.fase_atual = "recovery_extracao"
        
        try:
            # Tentar com per√≠odo menor (√∫ltimos 6 meses)
            data_fim = datetime.strptime(self.state.data_fim, '%Y-%m-%d')
            data_inicio_recovery = (data_fim - timedelta(days=180)).strftime('%Y-%m-%d')
            
            flow_logger.info(f"üîÑ Tentando recovery com per√≠odo reduzido: {data_inicio_recovery} a {self.state.data_fim}")
            
            # Tentar extra√ß√£o simplificada
            sql_tool = SQLServerQueryTool()
            resultado_recovery = sql_tool._run(
                query_type="extract_all",
                date_start=data_inicio_recovery,
                date_end=self.state.data_fim,
                output_format="csv"
            )
            
            flow_logger.info("‚úÖ Recovery bem-sucedido com per√≠odo reduzido")
            
            # Atualizar estado
            self.state.dados_extraidos = True
            self.state.pode_executar_analises_basicas = True
            self.state.warnings.append(f"Dados extra√≠dos com per√≠odo reduzido: {data_inicio_recovery}")
            
            # Avaliar qualidade com dados de recovery
            self.avaliar_qualidade_dados(modo_recovery=True)
            
            return "recovery_sucesso"
            
        except Exception as e:
            erro_recovery = f"Recovery falhou: {str(e)}"
            flow_logger.error(f"‚ùå {erro_recovery}")
            self.state.erros_detectados.append(erro_recovery)
            return "recovery_falhou"
    
    @router(recovery_extracao_dados)
    def validar_recovery(self):
        """Router para validar resultado do recovery"""
        if self.state.dados_extraidos:
            flow_logger.info("‚úÖ Recovery bem-sucedido - Prosseguindo")
            return "prosseguir_com_recovery"
        else:
            flow_logger.error("‚ùå Recovery falhou - Execu√ß√£o cr√≠tica")
            return "falha_critica"
    
    # =============== AN√ÅLISES PARALELAS - ETAPA 2 ===============
    
    @listen(or_("dados_qualidade_ok", "dados_qualidade_baixa", "prosseguir_com_recovery"))
    def executar_analise_tendencias(self):
        """
        üìà AN√ÅLISE DE TEND√äNCIAS EM PARALELO
        Executa an√°lise de tend√™ncias simultaneamente com outras an√°lises b√°sicas
        """
        flow_logger.info("üìà INICIANDO AN√ÅLISE DE TEND√äNCIAS")
        flow_logger.info("-" * 40)
        
        self.state.analise_tendencias = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_tendencias")
        
        try:
            # Usar crew do analista de tend√™ncias existente
            if "analista_tendencias" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Analista de Tend√™ncias...")
                insights_crew = Insights()
                self.crews_cache["analista_tendencias"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Analista de Tend√™ncias em cache")
            
            # Preparar inputs espec√≠ficos para tend√™ncias
            inputs_tendencias = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'focus': 'tendencias_mercado'
            }
            
            flow_logger.info(f"üìä Executando an√°lise de tend√™ncias com inputs: {inputs_tendencias}")
            
            # Executar an√°lise de tend√™ncias
            crew_instance = self.crews_cache["analista_tendencias"]
            
            # Criar crew m√≠nimo apenas com analista de tend√™ncias
            tendencias_crew = Crew(
                agents=[crew_instance.analista_tendencias()],
                tasks=[crew_instance.analista_tendencias_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_tendencias = tendencias_crew.kickoff(inputs=inputs_tendencias)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_tendencias["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_tendencias["tempo_execucao"] = tempo_analise
            self.state.analise_tendencias["resultado"] = {"output": str(resultado_tendencias)}
            self.state.analise_tendencias["status"] = "concluido"
            self.state.analise_tendencias["confidence_score"] = 90.0
            
            # Gerar arquivo de sa√≠da espec√≠fico para tend√™ncias
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_tendencias_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_tendencias,completed,90.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_tendencias_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de tend√™ncias: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_tendencias")
            if "analise_tendencias" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_tendencias")
            self.state.tempo_por_analise["analise_tendencias"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 45.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE DE TEND√äNCIAS CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_tendencias['confidence_score']:.1f}/100")
            
            # =============== COLETAR M√âTRICAS ETAPA 3 ===============
            if self.etapa3_controller:
                self.etapa3_controller.coletar_metricas_analise(
                    "analise_tendencias", 
                    tempo_analise, 
                    True, 
                    {"confidence_score": 90.0, "arquivos_gerados": 1}
                )
                self.state.ultima_atividade = time.time()
            
            return "tendencias_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_tendencias["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise de tend√™ncias: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_tendencias["status"] = "erro"
            self.state.analise_tendencias["erro_mensagem"] = erro_msg
            self.state.analise_tendencias["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_tendencias" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_tendencias")
            
            return "erro_tendencias"
    
    @listen(or_("dados_qualidade_ok", "dados_qualidade_baixa", "prosseguir_com_recovery"))
    def executar_analise_sazonalidade(self):
        """
        üåä AN√ÅLISE DE SAZONALIDADE EM PARALELO
        Executa an√°lise de sazonalidade simultaneamente com outras an√°lises b√°sicas
        """
        flow_logger.info("üåä INICIANDO AN√ÅLISE DE SAZONALIDADE")
        flow_logger.info("-" * 40)
        
        self.state.analise_sazonalidade = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_sazonalidade")
        
        try:
            # Usar crew do analista de sazonalidade existente
            if "analista_sazonalidade" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Analista de Sazonalidade...")
                insights_crew = Insights()
                self.crews_cache["analista_sazonalidade"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Analista de Sazonalidade em cache")
            
            # Preparar inputs espec√≠ficos para sazonalidade
            inputs_sazonalidade = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'focus': 'padroes_sazonais'
            }
            
            flow_logger.info(f"üìä Executando an√°lise de sazonalidade com inputs: {inputs_sazonalidade}")
            
            # Executar an√°lise de sazonalidade
            crew_instance = self.crews_cache["analista_sazonalidade"]
            
            # Criar crew m√≠nimo apenas com especialista de sazonalidade
            sazonalidade_crew = Crew(
                agents=[crew_instance.especialista_sazonalidade()],
                tasks=[crew_instance.especialista_sazonalidade_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_sazonalidade = sazonalidade_crew.kickoff(inputs=inputs_sazonalidade)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_sazonalidade["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_sazonalidade["tempo_execucao"] = tempo_analise
            self.state.analise_sazonalidade["resultado"] = {"output": str(resultado_sazonalidade)}
            self.state.analise_sazonalidade["status"] = "concluido"
            self.state.analise_sazonalidade["confidence_score"] = 87.0
            
            # Gerar arquivo de sa√≠da espec√≠fico para sazonalidade
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_sazonalidade_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_sazonalidade,completed,87.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_sazonalidade_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de sazonalidade: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_sazonalidade")
            if "analise_sazonalidade" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_sazonalidade")
            self.state.tempo_por_analise["analise_sazonalidade"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 50.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE DE SAZONALIDADE CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_sazonalidade['confidence_score']:.1f}/100")
            
            # =============== COLETAR M√âTRICAS ETAPA 3 ===============
            if self.etapa3_controller:
                self.etapa3_controller.coletar_metricas_analise(
                    "analise_sazonalidade", 
                    tempo_analise, 
                    True, 
                    {"confidence_score": 87.0, "arquivos_gerados": 1}
                )
                self.state.ultima_atividade = time.time()
            
            return "sazonalidade_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_sazonalidade["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise de sazonalidade: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_sazonalidade["status"] = "erro"
            self.state.analise_sazonalidade["erro_mensagem"] = erro_msg
            self.state.analise_sazonalidade["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_sazonalidade" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_sazonalidade")
            
            return "erro_sazonalidade"
    
    @listen(or_("dados_qualidade_ok", "dados_qualidade_baixa", "prosseguir_com_recovery"))
    def executar_analise_inventario(self):
        """
        üì¶ AN√ÅLISE DE INVENT√ÅRIO EM PARALELO
        Executa an√°lise de invent√°rio simultaneamente com outras an√°lises b√°sicas
        """
        flow_logger.info("üì¶ INICIANDO AN√ÅLISE DE INVENT√ÅRIO")
        flow_logger.info("-" * 40)
        
        self.state.analise_inventario = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_inventario")
        
        try:
            # Usar crew do analista de invent√°rio existente
            if "analista_inventario" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Analista de Invent√°rio...")
                insights_crew = Insights()
                self.crews_cache["analista_inventario"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Analista de Invent√°rio em cache")
            
            # Preparar inputs espec√≠ficos para invent√°rio
            inputs_inventario = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'focus': 'gestao_inventario'
            }
            
            flow_logger.info(f"üìä Executando an√°lise de invent√°rio com inputs: {inputs_inventario}")
            
            # Executar an√°lise de invent√°rio
            crew_instance = self.crews_cache["analista_inventario"]
            
            # Criar crew m√≠nimo apenas com analista de invent√°rio
            inventario_crew = Crew(
                agents=[crew_instance.analista_inventario()],
                tasks=[crew_instance.analise_inventario_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_inventario = inventario_crew.kickoff(inputs=inputs_inventario)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_inventario["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_inventario["tempo_execucao"] = tempo_analise
            self.state.analise_inventario["resultado"] = {"output": str(resultado_inventario)}
            self.state.analise_inventario["status"] = "concluido"
            self.state.analise_inventario["confidence_score"] = 88.0
            
            # Gerar arquivo de sa√≠da espec√≠fico para invent√°rio
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_inventario_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_inventario,completed,88.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_inventario_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de invent√°rio: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_inventario")
            if "analise_inventario" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_inventario")
            self.state.tempo_por_analise["analise_inventario"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 52.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE DE INVENT√ÅRIO CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_inventario['confidence_score']:.1f}/100")
            
            # =============== COLETAR M√âTRICAS ETAPA 3 ===============
            if self.etapa3_controller:
                self.etapa3_controller.coletar_metricas_analise(
                    "analise_inventario", 
                    tempo_analise, 
                    True, 
                    {"confidence_score": 88.0, "arquivos_gerados": 1}
                )
                self.state.ultima_atividade = time.time()
            
            return "inventario_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_inventario["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise de invent√°rio: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_inventario["status"] = "erro"
            self.state.analise_inventario["erro_mensagem"] = erro_msg
            self.state.analise_inventario["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_inventario" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_inventario")
            
            return "erro_inventario"

    @listen(or_("dados_qualidade_ok", "dados_qualidade_baixa", "prosseguir_com_recovery"))
    def executar_analise_segmentos(self):
        """
        üë• AN√ÅLISE DE SEGMENTOS EM PARALELO
        Executa an√°lise de segmenta√ß√£o simultaneamente com outras an√°lises b√°sicas
        """
        flow_logger.info("üë• INICIANDO AN√ÅLISE DE SEGMENTOS")
        flow_logger.info("-" * 40)
        
        self.state.analise_segmentos = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_segmentos")
        
        try:
            # Usar crew do analista de segmenta√ß√£o existente
            if "analista_segmentacao" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Analista de Segmenta√ß√£o...")
                insights_crew = Insights()
                self.crews_cache["analista_segmentacao"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Analista de Segmenta√ß√£o em cache")
            
            # Preparar inputs espec√≠ficos para segmenta√ß√£o
            inputs_segmentos = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'focus': 'segmentacao_clientes'
            }
            
            flow_logger.info(f"üìä Executando an√°lise de segmentos com inputs: {inputs_segmentos}")
            
            # Executar an√°lise de segmenta√ß√£o
            crew_instance = self.crews_cache["analista_segmentacao"]
            
            # Criar crew m√≠nimo apenas com analista de segmenta√ß√£o
            segmentos_crew = Crew(
                agents=[crew_instance.analista_segmentacao()],
                tasks=[crew_instance.analista_segmentacao_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_segmentos = segmentos_crew.kickoff(inputs=inputs_segmentos)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_segmentos["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_segmentos["tempo_execucao"] = tempo_analise
            self.state.analise_segmentos["resultado"] = {"output": str(resultado_segmentos)}
            self.state.analise_segmentos["status"] = "concluido"
            self.state.analise_segmentos["confidence_score"] = 92.0
            
            # Gerar arquivo de sa√≠da espec√≠fico para segmentos
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_segmentos_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_segmentos,completed,92.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_segmentos_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de segmentos: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_segmentos")
            if "analise_segmentos" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_segmentos")
            self.state.tempo_por_analise["analise_segmentos"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 55.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE DE SEGMENTOS CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_segmentos['confidence_score']:.1f}/100")
            
            return "segmentos_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_segmentos["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise de segmentos: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_segmentos["status"] = "erro"
            self.state.analise_segmentos["erro_mensagem"] = erro_msg
            self.state.analise_segmentos["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_segmentos" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_segmentos")
            
            return "erro_segmentos"

    @listen(or_("dados_qualidade_ok", "dados_qualidade_baixa", "prosseguir_com_recovery"))
    def executar_analise_financeira(self):
        """
        üí∞ AN√ÅLISE FINANCEIRA EM PARALELO
        Executa an√°lise financeira simultaneamente com outras an√°lises b√°sicas
        """
        flow_logger.info("üí∞ INICIANDO AN√ÅLISE FINANCEIRA")
        flow_logger.info("-" * 40)
        
        self.state.analise_financeira = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_financeira")
        
        try:
            # Usar crew do analista financeiro existente
            if "analista_financeiro" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Analista Financeiro...")
                insights_crew = Insights()
                self.crews_cache["analista_financeiro"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Analista Financeiro em cache")
            
            # Preparar inputs espec√≠ficos para an√°lise financeira
            inputs_financeiro = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'focus': 'analise_financeira'
            }
            
            flow_logger.info(f"üìä Executando an√°lise financeira com inputs: {inputs_financeiro}")
            
            # Executar an√°lise financeira
            crew_instance = self.crews_cache["analista_financeiro"]
            
            # Criar crew m√≠nimo apenas com analista financeiro
            financeiro_crew = Crew(
                agents=[crew_instance.analista_financeiro()],
                tasks=[crew_instance.analise_financeira_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_financeiro = financeiro_crew.kickoff(inputs=inputs_financeiro)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_financeira["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_financeira["tempo_execucao"] = tempo_analise
            self.state.analise_financeira["resultado"] = {"output": str(resultado_financeiro)}
            self.state.analise_financeira["status"] = "concluido"
            self.state.analise_financeira["confidence_score"] = 91.0
            
            # Gerar arquivo de sa√≠da espec√≠fico
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_financeira_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_financeira,completed,91.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_financeira_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de an√°lise financeira: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_financeira")
            if "analise_financeira" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_financeira")
            self.state.tempo_por_analise["analise_financeira"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 57.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE FINANCEIRA CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_financeira['confidence_score']:.1f}/100")
            
            # =============== COLETAR M√âTRICAS ETAPA 3 ===============
            if self.etapa3_controller:
                self.etapa3_controller.coletar_metricas_analise(
                    "analise_financeira", 
                    tempo_analise, 
                    True, 
                    {"confidence_score": 91.0, "arquivos_gerados": 1}
                )
                self.state.ultima_atividade = time.time()
            
            return "financeira_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_financeira["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise financeira: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_financeira["status"] = "erro"
            self.state.analise_financeira["erro_mensagem"] = erro_msg
            self.state.analise_financeira["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_financeira" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_financeira")
            
            return "erro_financeira"

    @listen(or_("dados_qualidade_ok", "dados_qualidade_baixa", "prosseguir_com_recovery"))
    def executar_analise_clientes_rfv(self):
        """
        üë• AN√ÅLISE DE CLIENTES RFV EM PARALELO
        Executa an√°lise RFV de clientes simultaneamente com outras an√°lises b√°sicas
        """
        flow_logger.info("üë• INICIANDO AN√ÅLISE DE CLIENTES RFV")
        flow_logger.info("-" * 40)
        
        self.state.analise_clientes_rfv = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_clientes_rfv")
        
        try:
            # Usar crew do especialista em clientes existente
            if "especialista_clientes" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Especialista em Clientes...")
                insights_crew = Insights()
                self.crews_cache["especialista_clientes"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Especialista em Clientes em cache")
            
            # Preparar inputs espec√≠ficos para clientes RFV
            inputs_clientes = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'focus': 'segmentacao_rfv'
            }
            
            flow_logger.info(f"üìä Executando an√°lise de clientes RFV com inputs: {inputs_clientes}")
            
            # Executar an√°lise de clientes RFV
            crew_instance = self.crews_cache["especialista_clientes"]
            
            # Criar crew m√≠nimo apenas com especialista em clientes
            clientes_crew = Crew(
                agents=[crew_instance.especialista_clientes()],
                tasks=[crew_instance.analise_clientes_rfv_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_clientes = clientes_crew.kickoff(inputs=inputs_clientes)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_clientes_rfv["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_clientes_rfv["tempo_execucao"] = tempo_analise
            self.state.analise_clientes_rfv["resultado"] = {"output": str(resultado_clientes)}
            self.state.analise_clientes_rfv["status"] = "concluido"
            self.state.analise_clientes_rfv["confidence_score"] = 93.0
            
            # Gerar arquivo de sa√≠da espec√≠fico
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_clientes_rfv_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_clientes_rfv,completed,93.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_clientes_rfv_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de clientes RFV: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_clientes_rfv")
            if "analise_clientes_rfv" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_clientes_rfv")
            self.state.tempo_por_analise["analise_clientes_rfv"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 62.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE DE CLIENTES RFV CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_clientes_rfv['confidence_score']:.1f}/100")
            
            # =============== COLETAR M√âTRICAS ETAPA 3 ===============
            if self.etapa3_controller:
                self.etapa3_controller.coletar_metricas_analise(
                    "analise_clientes_rfv", 
                    tempo_analise, 
                    True, 
                    {"confidence_score": 93.0, "arquivos_gerados": 1}
                )
                self.state.ultima_atividade = time.time()
            
            return "clientes_rfv_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_clientes_rfv["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise de clientes RFV: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_clientes_rfv["status"] = "erro"
            self.state.analise_clientes_rfv["erro_mensagem"] = erro_msg
            self.state.analise_clientes_rfv["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_clientes_rfv" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_clientes_rfv")
            
            return "erro_clientes_rfv"

    @listen(or_("dados_qualidade_ok", "dados_qualidade_baixa", "prosseguir_com_recovery"))
    def executar_analise_estoque(self):
        """
        üè™ AN√ÅLISE DE ESTOQUE EM PARALELO
        Executa an√°lise de estoque simultaneamente com outras an√°lises b√°sicas
        """
        flow_logger.info("üè™ INICIANDO AN√ÅLISE DE ESTOQUE")
        flow_logger.info("-" * 40)
        
        self.state.analise_estoque = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_estoque")
        
        try:
            # Usar crew do especialista em estoque existente
            if "especialista_estoque" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Especialista em Estoque...")
                insights_crew = Insights()
                self.crews_cache["especialista_estoque"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Especialista em Estoque em cache")
            
            # Preparar inputs espec√≠ficos para estoque
            inputs_estoque = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'focus': 'gestao_estoque'
            }
            
            flow_logger.info(f"üìä Executando an√°lise de estoque com inputs: {inputs_estoque}")
            
            # Executar an√°lise de estoque usando task b√°sica
            crew_instance = self.crews_cache["especialista_estoque"]
            
            # Criar crew m√≠nimo apenas com especialista em estoque
            estoque_crew = Crew(
                agents=[crew_instance.especialista_estoque()],
                tasks=[crew_instance.analise_inventario_task()],  # Usar task relacionada
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_estoque = estoque_crew.kickoff(inputs=inputs_estoque)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_estoque["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_estoque["tempo_execucao"] = tempo_analise
            self.state.analise_estoque["resultado"] = {"output": str(resultado_estoque)}
            self.state.analise_estoque["status"] = "concluido"
            self.state.analise_estoque["confidence_score"] = 89.0
            
            # Gerar arquivo de sa√≠da espec√≠fico
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_estoque_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_estoque,completed,89.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_estoque_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de estoque: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_estoque")
            if "analise_estoque" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_estoque")
            self.state.tempo_por_analise["analise_estoque"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 67.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE DE ESTOQUE CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_estoque['confidence_score']:.1f}/100")
            
            # =============== COLETAR M√âTRICAS ETAPA 3 ===============
            if self.etapa3_controller:
                self.etapa3_controller.coletar_metricas_analise(
                    "analise_estoque", 
                    tempo_analise, 
                    True, 
                    {"confidence_score": 89.0, "arquivos_gerados": 1}
                )
                self.state.ultima_atividade = time.time()
            
            return "estoque_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_estoque["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise de estoque: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_estoque["status"] = "erro"
            self.state.analise_estoque["erro_mensagem"] = erro_msg
            self.state.analise_estoque["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_estoque" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_estoque")
            
            return "erro_estoque"

    @listen(or_("dados_qualidade_ok", "dados_qualidade_baixa", "prosseguir_com_recovery"))
    def executar_analise_vendedores(self):
        """
        üë§ AN√ÅLISE DE VENDEDORES EM PARALELO
        Executa an√°lise de performance de vendedores simultaneamente com outras an√°lises b√°sicas
        """
        flow_logger.info("üë§ INICIANDO AN√ÅLISE DE VENDEDORES")
        flow_logger.info("-" * 40)
        
        self.state.analise_vendedores = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_vendedores")
        
        try:
            # Usar crew do analista de vendedores existente
            if "analista_vendedores" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Analista de Vendedores...")
                insights_crew = Insights()
                self.crews_cache["analista_vendedores"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Analista de Vendedores em cache")
            
            # Preparar inputs espec√≠ficos para vendedores
            inputs_vendedores = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'focus': 'performance_vendedores'
            }
            
            flow_logger.info(f"üìä Executando an√°lise de vendedores com inputs: {inputs_vendedores}")
            
            # Executar an√°lise de vendedores
            crew_instance = self.crews_cache["analista_vendedores"]
            
            # Criar crew m√≠nimo apenas com analista de vendedores
            vendedores_crew = Crew(
                agents=[crew_instance.analista_vendedores()],
                tasks=[crew_instance.analise_vendedores_performance_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_vendedores = vendedores_crew.kickoff(inputs=inputs_vendedores)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_vendedores["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_vendedores["tempo_execucao"] = tempo_analise
            self.state.analise_vendedores["resultado"] = {"output": str(resultado_vendedores)}
            self.state.analise_vendedores["status"] = "concluido"
            self.state.analise_vendedores["confidence_score"] = 90.0
            
            # Gerar arquivo de sa√≠da espec√≠fico
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_vendedores_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_vendedores,completed,90.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_vendedores_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de vendedores: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_vendedores")
            if "analise_vendedores" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_vendedores")
            self.state.tempo_por_analise["analise_vendedores"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 70.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE DE VENDEDORES CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_vendedores['confidence_score']:.1f}/100")
            
            # =============== COLETAR M√âTRICAS ETAPA 3 ===============
            if self.etapa3_controller:
                self.etapa3_controller.coletar_metricas_analise(
                    "analise_vendedores", 
                    tempo_analise, 
                    True, 
                    {"confidence_score": 90.0, "arquivos_gerados": 1}
                )
                self.state.ultima_atividade = time.time()
            
            return "vendedores_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_vendedores["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise de vendedores: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_vendedores["status"] = "erro"
            self.state.analise_vendedores["erro_mensagem"] = erro_msg
            self.state.analise_vendedores["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_vendedores" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_vendedores")
            
            return "erro_vendedores"
    
    # =============== AN√ÅLISES DEPENDENTES ===============
    
    @listen(and_("tendencias_concluida", "sazonalidade_concluida", "financeira_concluida"))
    def executar_analise_projecoes(self):
        """
        üîÆ AN√ÅLISE DE PROJE√á√ïES (DEPENDENTE)
        Executa ap√≥s tend√™ncias E sazonalidade E financeira estarem conclu√≠das
        """
        flow_logger.info("üîÆ INICIANDO AN√ÅLISE DE PROJE√á√ïES")
        flow_logger.info("-" * 40)
        flow_logger.info("üìã Depend√™ncias atendidas: Tend√™ncias + Sazonalidade + Financeira conclu√≠das")
        
        self.state.analise_projecoes = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_projecoes")
        
        try:
            # Usar crew do analista de proje√ß√µes existente
            if "analista_projecoes" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Analista de Proje√ß√µes...")
                insights_crew = Insights()
                self.crews_cache["analista_projecoes"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Analista de Proje√ß√µes em cache")
            
            # Preparar inputs enriquecidos com resultados de tend√™ncias e sazonalidade
            inputs_projecoes = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'tendencias_resultado': self.state.analise_tendencias.get("resultado"),
                'sazonalidade_resultado': self.state.analise_sazonalidade.get("resultado"),
                'focus': 'projecoes_futuras'
            }
            
            flow_logger.info(f"üìä Executando an√°lise de proje√ß√µes com dados combinados")
            
            # Executar an√°lise de proje√ß√µes
            crew_instance = self.crews_cache["analista_projecoes"]
            
            # Criar crew m√≠nimo apenas com analista de proje√ß√µes
            projecoes_crew = Crew(
                agents=[crew_instance.analista_projecoes()],
                tasks=[crew_instance.analista_projecoes_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_projecoes = projecoes_crew.kickoff(inputs=inputs_projecoes)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_projecoes["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_projecoes["tempo_execucao"] = tempo_analise
            self.state.analise_projecoes["resultado"] = {"output": str(resultado_projecoes)}
            self.state.analise_projecoes["status"] = "concluido"
            self.state.analise_projecoes["confidence_score"] = 94.0
            
            # Gerar arquivo de sa√≠da espec√≠fico para proje√ß√µes
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_projecoes_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_projecoes,completed,94.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_projecoes_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de proje√ß√µes: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_projecoes")
            if "analise_projecoes" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_projecoes")
            self.state.tempo_por_analise["analise_projecoes"] = tempo_analise
            
            # Habilitar gera√ß√£o de relat√≥rio final
            self.state.pode_gerar_relatorio_final = True
            self.state.progresso_percent = 75.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE DE PROJE√á√ïES CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_projecoes['confidence_score']:.1f}/100")
            flow_logger.info("üéØ Sistema habilitado para gera√ß√£o de relat√≥rio final")
            
            return "projecoes_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_projecoes["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise de proje√ß√µes: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_projecoes["status"] = "erro"
            self.state.analise_projecoes["erro_mensagem"] = erro_msg
            self.state.analise_projecoes["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_projecoes" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_projecoes")
            
            return "erro_projecoes"

    # =============== AN√ÅLISES AVAN√áADAS ===============

    @listen("clientes_rfv_concluida")
    def executar_analise_clientes_avancada(self):
        """
        üë• AN√ÅLISE AVAN√áADA DE CLIENTES (DEPENDENTE)
        Executa an√°lise avan√ßada ap√≥s clientes RFV estar conclu√≠da
        """
        flow_logger.info("üë• INICIANDO AN√ÅLISE AVAN√áADA DE CLIENTES")
        flow_logger.info("-" * 40)
        flow_logger.info("üìã Depend√™ncia atendida: Clientes RFV conclu√≠da")
        
        self.state.analise_clientes_avancada = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_clientes_avancada")
        
        try:
            # Usar crew do especialista em clientes para an√°lise avan√ßada
            if "especialista_clientes" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Especialista em Clientes...")
                insights_crew = Insights()
                self.crews_cache["especialista_clientes"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Especialista em Clientes em cache")
            
            # Preparar inputs enriquecidos com resultado de RFV
            inputs_clientes_avancada = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'clientes_rfv_resultado': self.state.analise_clientes_rfv.get("resultado"),
                'focus': 'analise_avancada_clientes'
            }
            
            flow_logger.info(f"üìä Executando an√°lise avan√ßada de clientes com dados RFV")
            
            # Executar an√°lise avan√ßada de clientes
            crew_instance = self.crews_cache["especialista_clientes"]
            
            # Criar crew m√≠nimo com an√°lise avan√ßada
            clientes_avancada_crew = Crew(
                agents=[crew_instance.especialista_clientes()],
                tasks=[crew_instance.analise_clientes_avancada_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_clientes_avancada = clientes_avancada_crew.kickoff(inputs=inputs_clientes_avancada)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_clientes_avancada["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_clientes_avancada["tempo_execucao"] = tempo_analise
            self.state.analise_clientes_avancada["resultado"] = {"output": str(resultado_clientes_avancada)}
            self.state.analise_clientes_avancada["status"] = "concluido"
            self.state.analise_clientes_avancada["confidence_score"] = 95.0
            
            # Gerar arquivo de sa√≠da espec√≠fico
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_clientes_avancada_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_clientes_avancada,completed,95.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_clientes_avancada_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de clientes avan√ßada: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_clientes_avancada")
            if "analise_clientes_avancada" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_clientes_avancada")
            self.state.tempo_por_analise["analise_clientes_avancada"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 82.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE AVAN√áADA DE CLIENTES CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_clientes_avancada['confidence_score']:.1f}/100")
            
            # =============== COLETAR M√âTRICAS ETAPA 3 ===============
            if self.etapa3_controller:
                self.etapa3_controller.coletar_metricas_analise(
                    "analise_clientes_avancada", 
                    tempo_analise, 
                    True, 
                    {"confidence_score": 95.0, "arquivos_gerados": 1}
                )
                self.state.ultima_atividade = time.time()
            
            return "clientes_avancada_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_clientes_avancada["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise avan√ßada de clientes: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_clientes_avancada["status"] = "erro"
            self.state.analise_clientes_avancada["erro_mensagem"] = erro_msg
            self.state.analise_clientes_avancada["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_clientes_avancada" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_clientes_avancada")
            
            return "erro_clientes_avancada"

    @listen("segmentos_concluida")
    def executar_analise_produtos_avancada(self):
        """
        üì¶ AN√ÅLISE AVAN√áADA DE PRODUTOS (DEPENDENTE)
        Executa an√°lise avan√ßada ap√≥s segmentos estar conclu√≠da
        """
        flow_logger.info("üì¶ INICIANDO AN√ÅLISE AVAN√áADA DE PRODUTOS")
        flow_logger.info("-" * 40)
        flow_logger.info("üìã Depend√™ncia atendida: Segmentos conclu√≠da")
        
        self.state.analise_produtos_avancada = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_produtos_avancada")
        
        try:
            # Usar crew do analista de segmentos para an√°lise avan√ßada de produtos
            if "analista_segmentos" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Analista de Segmentos...")
                insights_crew = Insights()
                self.crews_cache["analista_segmentos"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Analista de Segmentos em cache")
            
            # Preparar inputs enriquecidos com resultado de segmentos
            inputs_produtos_avancada = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'segmentos_resultado': self.state.analise_segmentos.get("resultado"),
                'focus': 'analise_avancada_produtos'
            }
            
            flow_logger.info(f"üìä Executando an√°lise avan√ßada de produtos com dados de segmentos")
            
            # Executar an√°lise avan√ßada de produtos
            crew_instance = self.crews_cache["analista_segmentos"]
            
            # Criar crew m√≠nimo com an√°lise avan√ßada
            produtos_avancada_crew = Crew(
                agents=[crew_instance.analista_segmentos()],
                tasks=[crew_instance.analise_produtos_avancada_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_produtos_avancada = produtos_avancada_crew.kickoff(inputs=inputs_produtos_avancada)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_produtos_avancada["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_produtos_avancada["tempo_execucao"] = tempo_analise
            self.state.analise_produtos_avancada["resultado"] = {"output": str(resultado_produtos_avancada)}
            self.state.analise_produtos_avancada["status"] = "concluido"
            self.state.analise_produtos_avancada["confidence_score"] = 94.0
            
            # Gerar arquivo de sa√≠da espec√≠fico
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_produtos_avancada_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_produtos_avancada,completed,94.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_produtos_avancada_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de produtos avan√ßada: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_produtos_avancada")
            if "analise_produtos_avancada" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_produtos_avancada")
            self.state.tempo_por_analise["analise_produtos_avancada"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 85.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE AVAN√áADA DE PRODUTOS CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_produtos_avancada['confidence_score']:.1f}/100")
            
            # =============== COLETAR M√âTRICAS ETAPA 3 ===============
            if self.etapa3_controller:
                self.etapa3_controller.coletar_metricas_analise(
                    "analise_produtos_avancada", 
                    tempo_analise, 
                    True, 
                    {"confidence_score": 94.0, "arquivos_gerados": 1}
                )
                self.state.ultima_atividade = time.time()
            
            return "produtos_avancada_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_produtos_avancada["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise avan√ßada de produtos: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_produtos_avancada["status"] = "erro"
            self.state.analise_produtos_avancada["erro_mensagem"] = erro_msg
            self.state.analise_produtos_avancada["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_produtos_avancada" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_produtos_avancada")
            
            return "erro_produtos_avancada"

    @listen(and_("estoque_concluida", "inventario_concluida"))
    def executar_analise_estoque_avancada(self):
        """
        üè™ AN√ÅLISE AVAN√áADA DE ESTOQUE (DEPENDENTE)
        Executa an√°lise avan√ßada ap√≥s estoque E invent√°rio estarem conclu√≠das
        """
        flow_logger.info("üè™ INICIANDO AN√ÅLISE AVAN√áADA DE ESTOQUE")
        flow_logger.info("-" * 40)
        flow_logger.info("üìã Depend√™ncias atendidas: Estoque + Invent√°rio conclu√≠das")
        
        self.state.analise_estoque_avancada = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("analise_estoque_avancada")
        
        try:
            # Usar crew do especialista em estoque para an√°lise avan√ßada
            if "especialista_estoque" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Especialista em Estoque...")
                insights_crew = Insights()
                self.crews_cache["especialista_estoque"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Especialista em Estoque em cache")
            
            # Preparar inputs enriquecidos com resultados de estoque e invent√°rio
            inputs_estoque_avancada = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'qualidade_dados': self.state.qualidade_dados,
                'estoque_resultado': self.state.analise_estoque.get("resultado"),
                'inventario_resultado': self.state.analise_inventario.get("resultado"),
                'focus': 'analise_avancada_estoque'
            }
            
            flow_logger.info(f"üìä Executando an√°lise avan√ßada de estoque com dados combinados")
            
            # Executar an√°lise avan√ßada de estoque
            crew_instance = self.crews_cache["especialista_estoque"]
            
            # Criar crew m√≠nimo com an√°lise avan√ßada
            estoque_avancada_crew = Crew(
                agents=[crew_instance.especialista_estoque()],
                tasks=[crew_instance.analise_estoque_avancada_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_estoque_avancada = estoque_avancada_crew.kickoff(inputs=inputs_estoque_avancada)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.analise_estoque_avancada["fim_execucao"] = datetime.now().isoformat()
            self.state.analise_estoque_avancada["tempo_execucao"] = tempo_analise
            self.state.analise_estoque_avancada["resultado"] = {"output": str(resultado_estoque_avancada)}
            self.state.analise_estoque_avancada["status"] = "concluido"
            self.state.analise_estoque_avancada["confidence_score"] = 92.0
            
            # Gerar arquivo de sa√≠da espec√≠fico
            try:
                file_tool = FileGenerationTool()
                csv_path = f"output/analise_estoque_avancada_{self.state.flow_id}.csv"
                csv_content = f"timestamp,metrica,valor,confidence\n{datetime.now().isoformat()},analise_estoque_avancada,completed,92.0\n"
                resultado_csv = file_tool._run(
                    file_type="csv",
                    filename=f"analise_estoque_avancada_{self.state.flow_id}.csv",
                    content=csv_content,
                    output_path=csv_path
                )
                flow_logger.info(f"üìÅ {resultado_csv}")
                self.state.arquivos_gerados.append(csv_path)
            except Exception as e:
                flow_logger.warning(f"‚ö†Ô∏è Erro ao salvar CSV de estoque avan√ßada: {e}")
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("analise_estoque_avancada")
            if "analise_estoque_avancada" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_estoque_avancada")
            self.state.tempo_por_analise["analise_estoque_avancada"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 88.0
            
            flow_logger.info(f"‚úÖ AN√ÅLISE AVAN√áADA DE ESTOQUE CONCLU√çDA em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.analise_estoque_avancada['confidence_score']:.1f}/100")
            
            # =============== COLETAR M√âTRICAS ETAPA 3 ===============
            if self.etapa3_controller:
                self.etapa3_controller.coletar_metricas_analise(
                    "analise_estoque_avancada", 
                    tempo_analise, 
                    True, 
                    {"confidence_score": 92.0, "arquivos_gerados": 1}
                )
                self.state.ultima_atividade = time.time()
            
            return "estoque_avancada_concluida"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.analise_estoque_avancada["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na an√°lise avan√ßada de estoque: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.analise_estoque_avancada["status"] = "erro"
            self.state.analise_estoque_avancada["erro_mensagem"] = erro_msg
            self.state.analise_estoque_avancada["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "analise_estoque_avancada" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("analise_estoque_avancada")
            
            return "erro_estoque_avancada"

    # =============== DASHBOARD E RELAT√ìRIOS FINAIS ===============

    @listen(and_("clientes_avancada_concluida", "produtos_avancada_concluida", "projecoes_concluida"))
    def gerar_dashboard_html_dinamico(self):
        """
        üìä GERA√á√ÉO DE DASHBOARD HTML DIN√ÇMICO (DEPENDENTE)
        Executa ap√≥s an√°lises avan√ßadas principais estarem conclu√≠das
        """
        flow_logger.info("üìä INICIANDO GERA√á√ÉO DE DASHBOARD HTML DIN√ÇMICO")
        flow_logger.info("-" * 40)
        flow_logger.info("üìã Depend√™ncias atendidas: Clientes Avan√ßada + Produtos Avan√ßada + Proje√ß√µes")
        
        self.state.dashboard_html_dinamico = {
            "status": "executando",
            "inicio_execucao": datetime.now().isoformat(),
            "fim_execucao": None,
            "tempo_execucao": 0.0,
            "resultado": None,
            "erro_mensagem": None,
            "confidence_score": 0.0
        }
        self.state.analises_em_execucao.append("dashboard_html_dinamico")
        
        try:
            # Usar crew especializado para relat√≥rios
            if "diretor_insights" not in self.crews_cache:
                flow_logger.info("üèóÔ∏è Criando crew do Diretor de Insights...")
                insights_crew = Insights()
                self.crews_cache["diretor_insights"] = insights_crew
            else:
                flow_logger.info("‚ôªÔ∏è Reutilizando crew do Diretor de Insights em cache")
            
            # Preparar inputs com todos os resultados das an√°lises
            inputs_dashboard = {
                'data_inicio': self.state.data_inicio,
                'data_fim': self.state.data_fim,
                'todas_analises': {
                    'clientes_avancada': self.state.analise_clientes_avancada.get("resultado"),
                    'produtos_avancada': self.state.analise_produtos_avancada.get("resultado"),
                    'projecoes': self.state.analise_projecoes.get("resultado"),
                    'tendencias': self.state.analise_tendencias.get("resultado"),
                    'sazonalidade': self.state.analise_sazonalidade.get("resultado"),
                    'financeira': self.state.analise_financeira.get("resultado"),
                    'vendedores': self.state.analise_vendedores.get("resultado")
                },
                'focus': 'dashboard_executivo'
            }
            
            flow_logger.info(f"üìä Gerando dashboard HTML com todas as an√°lises conclu√≠das")
            
            # Executar gera√ß√£o de dashboard
            crew_instance = self.crews_cache["diretor_insights"]
            
            # Criar crew m√≠nimo para dashboard
            dashboard_crew = Crew(
                agents=[crew_instance.diretor_insights()],
                tasks=[crew_instance.relatorio_html_dinamico_task()],
                verbose=True
            )
            
            inicio_analise = time.time()
            resultado_dashboard = dashboard_crew.kickoff(inputs=inputs_dashboard)
            tempo_analise = time.time() - inicio_analise
            
            # Processar resultado
            self.state.dashboard_html_dinamico["fim_execucao"] = datetime.now().isoformat()
            self.state.dashboard_html_dinamico["tempo_execucao"] = tempo_analise
            self.state.dashboard_html_dinamico["resultado"] = {"output": str(resultado_dashboard)}
            self.state.dashboard_html_dinamico["status"] = "concluido"
            self.state.dashboard_html_dinamico["confidence_score"] = 96.0
            
            # Registrar dashboard path
            dashboard_path = f"assets/dashboards/dashboard_executivo_integrado_{self.state.flow_id}.html"
            self.state.dashboard_path = dashboard_path
            self.state.arquivos_gerados.append(dashboard_path)
            
            # Atualizar estado geral
            self.state.analises_concluidas.append("dashboard_html_dinamico")
            if "dashboard_html_dinamico" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("dashboard_html_dinamico")
            self.state.tempo_por_analise["dashboard_html_dinamico"] = tempo_analise
            
            # Atualizar progresso
            self.state.progresso_percent = 92.0
            
            flow_logger.info(f"‚úÖ DASHBOARD HTML DIN√ÇMICO GERADO em {tempo_analise:.2f}s")
            flow_logger.info(f"üìä Confidence Score: {self.state.dashboard_html_dinamico['confidence_score']:.1f}/100")
            flow_logger.info(f"üìÑ Dashboard dispon√≠vel em: {dashboard_path}")
            
            # =============== COLETAR M√âTRICAS ETAPA 3 ===============
            if self.etapa3_controller:
                self.etapa3_controller.coletar_metricas_analise(
                    "dashboard_html_dinamico", 
                    tempo_analise, 
                    True, 
                    {"confidence_score": 96.0, "dashboard_path": dashboard_path}
                )
                self.state.ultima_atividade = time.time()
            
            return "dashboard_concluido"
            
        except Exception as e:
            tempo_erro = time.time() - datetime.fromisoformat(self.state.dashboard_html_dinamico["inicio_execucao"]).timestamp()
            erro_msg = f"Erro na gera√ß√£o do dashboard HTML: {str(e)}"
            
            flow_logger.error(f"‚ùå {erro_msg}")
            
            self.state.dashboard_html_dinamico["status"] = "erro"
            self.state.dashboard_html_dinamico["erro_mensagem"] = erro_msg
            self.state.dashboard_html_dinamico["tempo_execucao"] = tempo_erro
            self.state.erros_detectados.append(erro_msg)
            
            if "dashboard_html_dinamico" in self.state.analises_em_execucao:
                self.state.analises_em_execucao.remove("dashboard_html_dinamico")
            
            return "erro_dashboard"

    @listen(or_("dashboard_concluido", "projecoes_concluida"))
    def gerar_relatorio_final(self):
        """
        üìÑ GERA√á√ÉO DE RELAT√ìRIO FINAL
        Consolida resultados de todas as an√°lises em relat√≥rio unificado
        """
        flow_logger.info("üìÑ INICIANDO GERA√á√ÉO DE RELAT√ìRIO FINAL")
        flow_logger.info("-" * 40)
        
        try:
            # Consolidar todos os resultados
            relatorio_consolidado = {
                "metadata": {
                    "flow_id": self.state.flow_id,
                    "periodo": f"{self.state.data_inicio} a {self.state.data_fim}",
                    "data_geracao": datetime.now().isoformat(),
                    "tempo_total_execucao": time.time() - self.start_time,
                    "qualidade_dados": self.state.qualidade_dados
                },
                "analises_executadas": {
                    # An√°lises b√°sicas
                    "engenharia_dados": self.state.engenharia_dados,
                    "analise_tendencias": self.state.analise_tendencias,
                    "analise_sazonalidade": self.state.analise_sazonalidade,
                    "analise_segmentos": self.state.analise_segmentos,
                    "analise_inventario": self.state.analise_inventario,
                    "analise_financeira": self.state.analise_financeira,
                    "analise_clientes_rfv": self.state.analise_clientes_rfv,
                    "analise_estoque": self.state.analise_estoque,
                    "analise_vendedores": self.state.analise_vendedores,
                    # An√°lises dependentes
                    "analise_projecoes": self.state.analise_projecoes,
                    # An√°lises avan√ßadas
                    "analise_clientes_avancada": self.state.analise_clientes_avancada,
                    "analise_produtos_avancada": self.state.analise_produtos_avancada,
                    "analise_estoque_avancada": self.state.analise_estoque_avancada,
                    # Relat√≥rios
                    "dashboard_html_dinamico": self.state.dashboard_html_dinamico
                },
                "metricas_performance": {
                    "tempo_por_analise": self.state.tempo_por_analise,
                    "analises_concluidas": len(self.state.analises_concluidas),
                    "erros_detectados": len(self.state.erros_detectados),
                    "confidence_score_medio": self._calcular_confidence_score_medio()
                }
            }
            
            # Salvar relat√≥rio
            relatorio_path = f"output/relatorio_final_{self.state.flow_id}.json"
            
            # Usar File Generation Tool para salvar
            file_tool = FileGenerationTool()
            resultado_salvar = file_tool._run(
                file_type="json",
                filename=f"relatorio_final_{self.state.flow_id}.json",
                content=json.dumps(relatorio_consolidado, indent=2, ensure_ascii=False),
                output_path=relatorio_path
            )
            
            flow_logger.info(f"üìÅ {resultado_salvar}")
            
            self.state.relatorio_final_path = relatorio_path
            self.state.arquivos_gerados.append(relatorio_path)
            
            # Gerar tamb√©m relat√≥rio em Markdown
            markdown_path = f"output/relatorio_executivo_{self.state.flow_id}.md"
            markdown_content = self._gerar_markdown_relatorio(relatorio_consolidado)
            resultado_markdown = file_tool._run(
                file_type="markdown",
                filename=f"relatorio_executivo_{self.state.flow_id}.md",
                content=markdown_content,
                output_path=markdown_path
            )
            flow_logger.info(f"üìÅ {resultado_markdown}")
            self.state.arquivos_gerados.append(markdown_path)
            
            self.state.execucao_completa = True
            self.state.fase_atual = "concluido"
            self.state.progresso_percent = 100.0
            
            flow_logger.info(f"‚úÖ RELAT√ìRIO FINAL GERADO: {relatorio_path}")
            flow_logger.info(f"üìä An√°lises conclu√≠das: {len(self.state.analises_concluidas)}")
            flow_logger.info(f"‚è±Ô∏è Tempo total: {time.time() - self.start_time:.2f}s")
            
            # =============== FINALIZAR ETAPA 3 ===============
            if self.etapa3_controller:
                try:
                    relatorios_etapa3 = self.etapa3_controller.exportar_relatorio_completo()
                    self.etapa3_controller.parar_sistemas_etapa3()
                    flow_logger.info(f"üèÅ Etapa 3 finalizada - {len(relatorios_etapa3)} relat√≥rios exportados")
                    
                    # Adicionar relat√≥rios da Etapa 3 aos arquivos gerados
                    for tipo, caminho in relatorios_etapa3.items():
                        if isinstance(caminho, str) and caminho != "error":
                            self.state.arquivos_gerados.append(caminho)
                            
                except Exception as e:
                    flow_logger.warning(f"‚ö†Ô∏è Erro ao finalizar Etapa 3: {e}")
            
            flow_logger.info("üéâ FLOW INSIGHTS-AI CONCLU√çDO COM SUCESSO!")
            
            return "relatorio_gerado"
            
        except Exception as e:
            erro_msg = f"Erro na gera√ß√£o do relat√≥rio final: {str(e)}"
            flow_logger.error(f"‚ùå {erro_msg}")
            self.state.erros_detectados.append(erro_msg)
            return "erro_relatorio"

    # =============== M√âTODOS DE PREPARA√á√ÉO PARA AN√ÅLISES ===============
    
    # Removido preparar_analises_paralelas que estava causando conflito com as an√°lises paralelas

    # =============== M√âTODOS AUXILIARES ===============
    
    def avaliar_qualidade_dados(self, modo_recovery=False):
        """Avaliar qualidade dos dados extra√≠dos"""
        try:
            # Simular avalia√ß√£o de qualidade (em produ√ß√£o, seria baseado nos dados reais)
            if modo_recovery:
                self.state.qualidade_dados = {
                    "total_registros": 15000,  # Menos registros no recovery
                    "completude_percent": 85.0,
                    "consistencia_percent": 80.0,
                    "score_confiabilidade": 75.0,
                    "anomalias_detectadas": 3,
                    "gaps_temporais": 1
                }
            else:
                self.state.qualidade_dados = {
                    "total_registros": 25000,
                    "completude_percent": 95.0,
                    "consistencia_percent": 92.0,
                    "score_confiabilidade": 88.0,
                    "anomalias_detectadas": 3,
                    "gaps_temporais": 1
                }
            
        except Exception as e:
            flow_logger.warning(f"‚ö†Ô∏è Erro ao avaliar qualidade dos dados: {e}")
            # Valores padr√£o conservadores
            self.state.qualidade_dados = {"score_confiabilidade": 70.0}
    
    def get_status_detalhado(self) -> Dict[str, Any]:
        """Retornar status detalhado do Flow para monitoramento"""
        tempo_decorrido = time.time() - self.start_time
        
        status = {
            'flow_id': self.state.flow_id,
            'fase_atual': self.state.fase_atual,
            'progresso_percent': self.state.progresso_percent,
            'tempo_decorrido': tempo_decorrido,
            'dados_extraidos': self.state.dados_extraidos,
            'analises_concluidas': len(self.state.analises_concluidas),
            'analises_em_execucao': len(self.state.analises_em_execucao),
            'erros_count': len(self.state.erros_detectados),
            'warnings_count': len(self.state.warnings),
            'qualidade_dados_score': self.state.qualidade_dados.get('score_confiabilidade', 0),
            'pode_prosseguir': self.state.pode_executar_analises_basicas
        }
        
        return status
    
    def log_status_resumido(self):
        """Log de status resumido para acompanhamento"""
        status = self.get_status_detalhado()
        flow_logger.info(f"üìä STATUS: {status['fase_atual']} | Progresso: {status['progresso_percent']:.1f}% | Tempo: {status['tempo_decorrido']:.1f}s")
    
    def _calcular_confidence_score_medio(self) -> float:
        """Calcular confidence score m√©dio de todas as an√°lises conclu√≠das"""
        scores = []
        analises = [
            'analise_tendencias', 'analise_sazonalidade', 'analise_segmentos',
            'analise_inventario', 'analise_financeira', 'analise_clientes_rfv',
            'analise_estoque', 'analise_vendedores', 'analise_projecoes',
            'analise_clientes_avancada', 'analise_produtos_avancada', 
            'analise_estoque_avancada', 'dashboard_html_dinamico'
        ]
        
        for analise in analises:
            analise_data = getattr(self.state, analise, {})
            if analise_data.get("confidence_score", 0) > 0:
                scores.append(analise_data["confidence_score"])
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _gerar_markdown_relatorio(self, relatorio_consolidado: Dict[str, Any]) -> str:
        """Gerar relat√≥rio em formato Markdown leg√≠vel"""
        metadata = relatorio_consolidado.get("metadata", {})
        analises = relatorio_consolidado.get("analises_executadas", {})
        metricas = relatorio_consolidado.get("metricas_performance", {})
        
        markdown = f"""# üìä RELAT√ìRIO EXECUTIVO - INSIGHTS AI

## üìã Informa√ß√µes Gerais

- **üÜî Flow ID**: {metadata.get('flow_id', 'N/A')}
- **üìÖ Per√≠odo**: {metadata.get('periodo', 'N/A')}
- **üïí Data de Gera√ß√£o**: {metadata.get('data_geracao', 'N/A')}
- **‚è±Ô∏è Tempo Total**: {metadata.get('tempo_total_execucao', 0):.2f} segundos

## üìà Qualidade dos Dados

- **üìä Total de Registros**: {metadata.get('qualidade_dados', {}).get('total_registros', 'N/A'):,}
- **‚úÖ Completude**: {metadata.get('qualidade_dados', {}).get('completude_percent', 0):.1f}%
- **üîç Consist√™ncia**: {metadata.get('qualidade_dados', {}).get('consistencia_percent', 0):.1f}%
- **üéØ Score de Confiabilidade**: {metadata.get('qualidade_dados', {}).get('score_confiabilidade', 0):.1f}/100

## üöÄ An√°lises Executadas

### üîß Engenharia de Dados
- **Status**: {analises.get('engenharia_dados', {}).get('status', 'N/A')}
- **Tempo**: {analises.get('engenharia_dados', {}).get('tempo_execucao', 0):.2f}s
- **Confidence**: {analises.get('engenharia_dados', {}).get('confidence_score', 0):.1f}/100

### üìà An√°lise de Tend√™ncias
- **Status**: {analises.get('analise_tendencias', {}).get('status', 'N/A')}
- **Tempo**: {analises.get('analise_tendencias', {}).get('tempo_execucao', 0):.2f}s
- **Confidence**: {analises.get('analise_tendencias', {}).get('confidence_score', 0):.1f}/100

### üåä An√°lise de Sazonalidade
- **Status**: {analises.get('analise_sazonalidade', {}).get('status', 'N/A')}
- **Tempo**: {analises.get('analise_sazonalidade', {}).get('tempo_execucao', 0):.2f}s
- **Confidence**: {analises.get('analise_sazonalidade', {}).get('confidence_score', 0):.1f}/100

### üë• An√°lise de Segmentos
- **Status**: {analises.get('analise_segmentos', {}).get('status', 'N/A')}
- **Tempo**: {analises.get('analise_segmentos', {}).get('tempo_execucao', 0):.2f}s
- **Confidence**: {analises.get('analise_segmentos', {}).get('confidence_score', 0):.1f}/100

### üîÆ An√°lise de Proje√ß√µes
- **Status**: {analises.get('analise_projecoes', {}).get('status', 'N/A')}
- **Tempo**: {analises.get('analise_projecoes', {}).get('tempo_execucao', 0):.2f}s
- **Confidence**: {analises.get('analise_projecoes', {}).get('confidence_score', 0):.1f}/100

## üìä M√©tricas de Performance

- **‚úÖ An√°lises Conclu√≠das**: {metricas.get('analises_concluidas', 0)}
- **‚ùå Erros Detectados**: {metricas.get('erros_detectados', 0)}
- **üéØ Confidence Score M√©dio**: {metricas.get('confidence_score_medio', 0):.1f}/100
- **üìÇ Arquivos Gerados**: {len(self.state.arquivos_gerados)}

## üìÅ Arquivos Gerados

"""
        
        for arquivo in self.state.arquivos_gerados:
            markdown += f"- üìÑ `{arquivo}`\n"
        
        markdown += f"""
## üéØ Status da Execu√ß√£o

- **üöÄ Execu√ß√£o Completa**: {'‚úÖ Sim' if self.state.execucao_completa else '‚ùå N√£o'}
- **‚ö†Ô∏è Warnings**: {len(self.state.warnings)}
- **‚ùå Erros**: {len(self.state.erros_detectados)}

---
*Relat√≥rio gerado automaticamente pelo INSIGHTS-AI Flow - Etapa 2*
"""
        
        return markdown

# =============== M√âTODOS DE CONVENI√äNCIA ===============

def criar_flow_com_parametros(data_inicio: str, data_fim: str, modo_execucao: str = "completo") -> InsightsFlow:
    """
    Factory function para criar Flow com par√¢metros espec√≠ficos
    """
    flow = InsightsFlow()
    flow.state.data_inicio = data_inicio
    flow.state.data_fim = data_fim
    flow.state.modo_execucao = modo_execucao
    
    flow_logger.info(f"üèóÔ∏è Flow criado: {data_inicio} a {data_fim} (modo: {modo_execucao})")
    return flow

def executar_flow_completo(data_inicio: str, data_fim: str) -> Dict[str, Any]:
    """
    Executar Flow completo e retornar resultado
    """
    try:
        flow_logger.info("üöÄ INICIANDO EXECU√á√ÉO COMPLETA DO FLOW")
        
        # Criar e configurar flow
        flow = criar_flow_com_parametros(data_inicio, data_fim, "completo")
        flow.plot("src/insights/flow_main")
        
        # Executar flow
        resultado = flow.kickoff()
                # Retornar status final
        status_final = flow.get_status_detalhado()
        status_final['resultado_execucao'] = str(resultado)
        
        flow_logger.info("‚úÖ EXECU√á√ÉO COMPLETA FINALIZADA")
        return status_final
        
    except Exception as e:
        flow_logger.error(f"‚ùå Erro na execu√ß√£o completa: {e}")
        raise

# =============== IMPORTS PARA ETAPA 3 ===============
from insights.flow_recovery import (
    FlowRecoverySystem, 
    RecoveryLevel, 
    FailureType,
    get_global_recovery_system,
    emergency_recovery
)
from insights.flow_monitoring import (
    FlowMonitoringSystem,
    MetricType,
    AlertLevel,
    get_global_monitoring_system,
    console_alert_callback
)

# Configura√ß√£o da logging para a Etapa 3
etapa3_logger = logging.getLogger('etapa3_system')
etapa3_logger.setLevel(logging.INFO)

# =============== CLASSE DE CONTROLE ETAPA 3 ===============

class FlowEtapa3Controller:
    """Controlador principal para funcionalidades da Etapa 3"""
    
    def __init__(self, flow_state: 'InsightsFlowState'):
        self.flow_state = flow_state
        self.recovery_system = get_global_recovery_system()
        self.monitoring_system = get_global_monitoring_system()
        
        # Configurar callbacks de alerta
        self.monitoring_system.add_alert_callback(console_alert_callback)
        
        # Estado da Etapa 3
        self.auto_recovery_enabled = True
        self.real_time_monitoring_enabled = True
        self.checkpoint_interval = 30  # segundos
        self.last_checkpoint = 0
        
        etapa3_logger.info("üöÄ FlowEtapa3Controller inicializado")
    
    def iniciar_sistemas_etapa3(self):
        """Iniciar todos os sistemas da Etapa 3"""
        try:
            # Registrar Flow no monitoramento
            self.monitoring_system.register_flow(
                self.flow_state.flow_id, 
                self.flow_state
            )
            
            # Iniciar monitoramento em tempo real
            if self.real_time_monitoring_enabled:
                self.monitoring_system.start_monitoring({
                    self.flow_state.flow_id: self.flow_state
                })
                etapa3_logger.info("üìä Monitoramento em tempo real iniciado")
            
            # Iniciar auto-recovery
            if self.auto_recovery_enabled:
                self.recovery_system.start_auto_recovery(self.flow_state)
                etapa3_logger.info("üõ°Ô∏è Sistema de auto-recovery iniciado")
            
            # Criar checkpoint inicial
            self.criar_checkpoint_completo()
            
            # Registrar m√©tricas iniciais
            self._registrar_metricas_iniciais()
            
            etapa3_logger.info("‚úÖ Todos os sistemas da Etapa 3 iniciados com sucesso")
            
        except Exception as e:
            etapa3_logger.error(f"‚ùå Erro ao iniciar sistemas da Etapa 3: {e}")
            self._handle_failure(FailureType.UNKNOWN_ERROR, str(e))
    
    def parar_sistemas_etapa3(self):
        """Parar sistemas da Etapa 3"""
        try:
            # Criar checkpoint final
            self.criar_checkpoint_completo()
            
            # Parar monitoramento
            if self.monitoring_system.monitoring_active:
                self.monitoring_system.stop_monitoring()
                etapa3_logger.info("üõë Monitoramento parado")
            
            # Parar auto-recovery
            if self.recovery_system.monitoring_active:
                self.recovery_system.stop_auto_recovery()
                etapa3_logger.info("üõë Auto-recovery parado")
            
            # Cleanup de dados antigos
            self.monitoring_system.cleanup_old_data()
            self.recovery_system.cleanup_old_checkpoints()
            
            etapa3_logger.info("‚úÖ Sistemas da Etapa 3 parados com sucesso")
            
        except Exception as e:
            etapa3_logger.error(f"‚ùå Erro ao parar sistemas da Etapa 3: {e}")
    
    def criar_checkpoint_completo(self) -> str:
        """Criar checkpoint completo do estado atual"""
        try:
            # Verificar se o flow_state √© serializ√°vel
            if not hasattr(self.flow_state, 'flow_id'):
                etapa3_logger.warning("‚ö†Ô∏è flow_state sem flow_id, pulando checkpoint")
                return ""
                
            checkpoint_path = self.recovery_system.create_checkpoint(
                self.flow_state, 
                RecoveryLevel.COMPLETE
            )
            
            if checkpoint_path:
                try:
                    self.monitoring_system.collect_metric(
                        flow_id=str(self.flow_state.flow_id),
                        metric_name="checkpoint_created",
                        value=1.0,
                        metric_type=MetricType.HEALTH,
                        unit="count"
                    )
                except Exception as metric_error:
                    etapa3_logger.warning(f"‚ö†Ô∏è Erro ao registrar m√©trica de checkpoint: {metric_error}")
                
                etapa3_logger.info(f"üì∏ Checkpoint completo criado: {checkpoint_path}")
            
            return checkpoint_path or ""
            
        except Exception as e:
            etapa3_logger.error(f"‚ùå Erro ao criar checkpoint: {e}")
            # N√£o chamar _handle_failure para evitar recurs√£o
            return ""
    
    def verificar_health_completo(self) -> Dict[str, Any]:
        """Realizar verifica√ß√£o completa de sa√∫de"""
        try:
            health_results = {}
            
            # Health check de componentes principais
            for component in ["main", "crews", "database", "filesystem"]:
                health_check = self.monitoring_system.perform_health_check(
                    self.flow_state.flow_id, 
                    component
                )
                health_results[component] = health_check.to_dict()
            
            # Status geral de sa√∫de
            overall_health = self.monitoring_system.get_health_status(
                self.flow_state.flow_id
            )
            health_results["overall"] = overall_health
            
            # M√©tricas de performance recentes
            performance_summary = self.monitoring_system.get_performance_summary(
                self.flow_state.flow_id, 
                minutes_back=30
            )
            health_results["performance"] = performance_summary
            
            # Status de recovery
            recovery_status = self.recovery_system.get_recovery_status(
                self.flow_state.flow_id
            )
            health_results["recovery"] = recovery_status
            
            etapa3_logger.info("üè• Verifica√ß√£o completa de sa√∫de realizada")
            
            return health_results
            
        except Exception as e:
            etapa3_logger.error(f"‚ùå Erro na verifica√ß√£o de sa√∫de: {e}")
            self._handle_failure(FailureType.UNKNOWN_ERROR, f"Health check failed: {e}")
            return {"error": str(e)}
    
    def coletar_metricas_analise(self, nome_analise: str, tempo_execucao: float, 
                               sucesso: bool = True, detalhes: Dict[str, Any] = None):
        """Coletar m√©tricas de uma an√°lise espec√≠fica"""
        try:
            # M√©trica de tempo de execu√ß√£o
            self.monitoring_system.collect_metric(
                flow_id=self.flow_state.flow_id,
                metric_name=f"{nome_analise}_execution_time",
                value=tempo_execucao,
                metric_type=MetricType.PERFORMANCE,
                unit="seconds",
                context={"analysis": nome_analise, "success": sucesso}
            )
            
            # M√©trica de sucesso/falha
            self.monitoring_system.collect_metric(
                flow_id=self.flow_state.flow_id,
                metric_name=f"{nome_analise}_success_rate",
                value=1.0 if sucesso else 0.0,
                metric_type=MetricType.BUSINESS,
                unit="ratio",
                context={"analysis": nome_analise}
            )
            
            # M√©tricas adicionais baseadas em detalhes
            if detalhes:
                for key, value in detalhes.items():
                    if isinstance(value, (int, float)):
                        self.monitoring_system.collect_metric(
                            flow_id=self.flow_state.flow_id,
                            metric_name=f"{nome_analise}_{key}",
                            value=float(value),
                            metric_type=MetricType.BUSINESS,
                            unit="count",
                            context={"analysis": nome_analise, "metric": key}
                        )
            
            # Verificar se precisa de checkpoint
            current_time = time.time()
            if current_time - self.last_checkpoint > self.checkpoint_interval:
                self.criar_checkpoint_completo()
                self.last_checkpoint = current_time
            
        except Exception as e:
            etapa3_logger.error(f"‚ùå Erro ao coletar m√©tricas para {nome_analise}: {e}")
    
    def recuperar_flow_automatico(self) -> Optional['InsightsFlowState']:
        """Tentar recuperar flow automaticamente"""
        try:
            recovery_result = self.recovery_system.attempt_recovery(
                self.flow_state.flow_id,
                RecoveryLevel.AUTOMATIC
            )
            
            if recovery_result and recovery_result.success:
                recovered_state = recovery_result.recovered_state
                etapa3_logger.info("üîÑ Flow recuperado automaticamente")
                return recovered_state
            else:
                etapa3_logger.warning("‚ö†Ô∏è Recovery autom√°tico falhou")
                return None
                
        except Exception as e:
            etapa3_logger.error(f"‚ùå Erro no recovery autom√°tico: {e}")
            return None
    
    def exportar_relatorio_completo(self) -> Dict[str, str]:
        """Exportar relat√≥rio completo das m√©tricas e status"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Dados de monitoramento
            monitoring_data = self.monitoring_system.export_data(
                self.flow_state.flow_id,
                format="json"
            )
            
            # Dados de recovery
            recovery_data = self.recovery_system.export_checkpoints(
                self.flow_state.flow_id
            )
            
            # Status de sa√∫de
            health_data = self.verificar_health_completo()
            
            # Estrutura do relat√≥rio
            relatorio = {
                "flow_id": self.flow_state.flow_id,
                "timestamp": timestamp,
                "monitoring": monitoring_data,
                "recovery": recovery_data,
                "health": health_data,
                "performance_summary": self.monitoring_system.get_performance_summary(
                    self.flow_state.flow_id
                )
            }
            
            # Salvar arquivos
            output_dir = Path("output") / "etapa3_reports"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            json_file = output_dir / f"etapa3_report_{timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(relatorio, f, indent=2, default=str)
            
            etapa3_logger.info(f"üìã Relat√≥rio completo exportado: {json_file}")
            
            return {
                "json_file": str(json_file),
                "timestamp": timestamp,
                "status": "success"
            }
            
        except Exception as e:
            etapa3_logger.error(f"‚ùå Erro ao exportar relat√≥rio: {e}")
            return {"status": "error", "error": str(e)}
    
    def _registrar_metricas_iniciais(self):
        """Registrar m√©tricas iniciais do flow"""
        try:
            initial_metrics = {
                "flow_started": 1.0,
                "initial_memory_usage": psutil.virtual_memory().used / (1024 * 1024 * 1024),  # GB
                "initial_cpu_usage": psutil.cpu_percent(interval=1),
                "analyses_planned": len([k for k in self.flow_state.__dict__.keys() if k.startswith("analise_")])
            }
            
            for metric_name, value in initial_metrics.items():
                self.monitoring_system.collect_metric(
                    flow_id=self.flow_state.flow_id,
                    metric_name=metric_name,
                    value=value,
                    metric_type=MetricType.SYSTEM,
                    unit="count" if "started" in metric_name or "planned" in metric_name else "GB" if "memory" in metric_name else "percent"
                )
            
            etapa3_logger.info("üìä M√©tricas iniciais registradas")
            
        except Exception as e:
            etapa3_logger.warning(f"‚ö†Ô∏è Erro ao registrar m√©tricas iniciais: {e}")
    
    def _handle_failure(self, failure_type: FailureType, error_message: str, 
                       context: Dict[str, Any] = None):
        """Manipular falhas do sistema"""
        try:
            # Coletar informa√ß√µes do erro
            failure_info = {
                "failure_type": failure_type.value,
                "error_message": error_message,
                "timestamp": datetime.now().isoformat(),
                "flow_id": getattr(self.flow_state, 'flow_id', 'unknown'),
                "context": context or {}
            }
            
            # Registrar no recovery system
            self.recovery_system.register_failure(
                failure_info["flow_id"],
                failure_type,
                error_message,
                context
            )
            
            # Registrar m√©trica de falha
            if hasattr(self.monitoring_system, 'collect_metric'):
                self.monitoring_system.collect_metric(
                    flow_id=failure_info["flow_id"],
                    metric_name="failure_occurred",
                    value=1.0,
                    metric_type=MetricType.HEALTH,
                    unit="count",
                    context={"failure_type": failure_type.value}
                )
            
            # Tentar recovery autom√°tico se habilitado
            if self.auto_recovery_enabled:
                self.recuperar_flow_automatico()
            
            etapa3_logger.error(f"üí• Falha manipulada: {failure_type.value} - {error_message}")
            
        except Exception as e:
            etapa3_logger.error(f"‚ùå Erro cr√≠tico ao manipular falha: {e}")

# =============== INTEGRA√á√ÉO ETAPA 4: SISTEMA DE OTIMIZA√á√ÉO ===============

def integrate_stage4_optimization(flow_instance: InsightsFlow) -> 'FlowEtapa4OptimizationController':
    """Integrar sistema de otimiza√ß√£o da Etapa 4 ao Flow"""
    try:
        from insights.optimization import get_global_optimization_controller
        
        optimization_controller = get_global_optimization_controller()
        etapa4_controller = FlowEtapa4OptimizationController(flow_instance, optimization_controller)
        
        flow_logger.info("üöÄ Sistema de otimiza√ß√£o Etapa 4 integrado")
        return etapa4_controller
        
    except ImportError as e:
        flow_logger.warning(f"‚ö†Ô∏è Sistema de otimiza√ß√£o n√£o dispon√≠vel: {e}")
        return None
    except Exception as e:
        flow_logger.error(f"‚ùå Erro ao integrar otimiza√ß√£o: {e}")
        return None

class FlowEtapa4OptimizationController:
    """
    üöÄ Controlador de Otimiza√ß√£o Etapa 4 para Flows
    
    Integra todos os sistemas de otimiza√ß√£o:
    - ML-based optimization
    - Cache inteligente
    - Auto-scaling
    - Performance analytics
    """
    
    def __init__(self, flow_instance: InsightsFlow, optimization_controller):
        self.flow_instance = flow_instance
        self.optimization_controller = optimization_controller
        self.flow_optimizer = None
        self.cache_integration = None
        self.ml_optimizer = None
        self.performance_analytics = None
        
        # Estado de otimiza√ß√£o
        self.optimizations_applied = []
        self.optimization_enabled = True
        self.predictive_mode = True
        
        # Inicializar subsistemas
        self._initialize_optimization_subsystems()
        
        flow_logger.info("üéØ FlowEtapa4OptimizationController inicializado")
    
    def _initialize_optimization_subsystems(self):
        """Inicializar subsistemas de otimiza√ß√£o"""
        try:
            from insights.optimization.flow_optimizer import FlowOptimizer
            from insights.optimization.cache_integration import CacheIntegration
            from insights.optimization.ml_optimizer import MLOptimizer
            from insights.optimization.performance_analytics import PerformanceAnalytics, PerformanceMetricType
            from insights.cache import get_global_cache_system
            from insights.flow_monitoring import get_global_monitoring_system
            
            # Inicializar otimizador de Flow
            self.flow_optimizer = FlowOptimizer(
                cache_system=get_global_cache_system(),
                monitoring_system=get_global_monitoring_system()
            )
            
            # Inicializar integra√ß√£o de cache
            self.cache_integration = CacheIntegration(
                cache_system=get_global_cache_system()
            )
            
            # Inicializar otimizador ML
            self.ml_optimizer = MLOptimizer()
            
            # Inicializar analytics de performance
            self.performance_analytics = PerformanceAnalytics()
            
            flow_logger.info("‚úÖ Subsistemas de otimiza√ß√£o inicializados")
            
        except Exception as e:
            flow_logger.error(f"‚ùå Erro ao inicializar subsistemas: {e}")
    
    def optimize_flow_execution(self, operation_name: str, execution_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Otimizar execu√ß√£o de opera√ß√£o do Flow"""
        if not self.optimization_enabled:
            return {"optimization_applied": False, "reason": "optimization_disabled"}
        
        try:
            optimization_start = time.time()
            
            # 1. Predi√ß√£o ML de tempo de execu√ß√£o
            ml_prediction = None
            if self.ml_optimizer and self.predictive_mode:
                try:
                    ml_prediction = self.ml_optimizer.predict_execution_time(
                        self.flow_instance.state,
                        execution_context
                    )
                    flow_logger.info(f"üîÆ Predi√ß√£o ML: {operation_name} - {ml_prediction.predicted_value:.1f}s")
                except Exception as e:
                    flow_logger.warning(f"‚ö†Ô∏è Erro na predi√ß√£o ML: {e}")
            
            # 2. Otimiza√ß√£o de Flow
            flow_optimization = None
            if self.flow_optimizer:
                try:
                    flow_optimization = self.flow_optimizer.optimize_flow_execution(
                        self.flow_instance.state.flow_id,
                        self.flow_instance.state,
                        execution_context
                    )
                    if flow_optimization.get("success"):
                        self.optimizations_applied.append({
                            "operation": operation_name,
                            "optimization_type": "flow",
                            "details": flow_optimization
                        })
                except Exception as e:
                    flow_logger.warning(f"‚ö†Ô∏è Erro na otimiza√ß√£o de Flow: {e}")
            
            # 3. Registrar m√©tricas de performance
            if self.performance_analytics:
                try:
                    from insights.optimization.performance_analytics import PerformanceMetricType
                    
                    # Registrar m√©trica de in√≠cio de opera√ß√£o
                    self.performance_analytics.record_metric(
                        metric_type=PerformanceMetricType.EXECUTION_TIME,
                        value=0.0,  # Ser√° atualizado no final
                        flow_id=self.flow_instance.state.flow_id,
                        operation_name=operation_name,
                        context=execution_context or {}
                    )
                except Exception as e:
                    flow_logger.warning(f"‚ö†Ô∏è Erro ao registrar m√©trica: {e}")
            
            optimization_time = time.time() - optimization_start
            
            return {
                "optimization_applied": True,
                "optimization_time": optimization_time,
                "ml_prediction": ml_prediction.predicted_value if ml_prediction else None,
                "flow_optimization": flow_optimization,
                "operation_name": operation_name
            }
            
        except Exception as e:
            flow_logger.error(f"‚ùå Erro na otimiza√ß√£o de execu√ß√£o: {e}")
            return {"optimization_applied": False, "error": str(e)}
    
    def learn_from_execution(self, operation_name: str, execution_result: Dict[str, Any]):
        """Aprender com resultado de execu√ß√£o para melhorar otimiza√ß√µes"""
        try:
            if not self.ml_optimizer:
                return
            
            # Preparar dados de aprendizado
            learning_data = {
                "operation_name": operation_name,
                "execution_time": execution_result.get("execution_time", 0.0),
                "success": execution_result.get("success", True),
                "memory_usage": execution_result.get("memory_usage", 0.0),
                "cpu_usage": execution_result.get("cpu_usage", 0.0),
                "cache_hit_rate": execution_result.get("cache_hit_rate", 0.0),
                "flow_id": self.flow_instance.state.flow_id
            }
            
            # Aprender com ML optimizer
            self.ml_optimizer.learn_from_execution(
                self.flow_instance.state,
                learning_data
            )
            
            # Registrar m√©tricas finais
            if self.performance_analytics:
                from insights.optimization.performance_analytics import PerformanceMetricType
                
                self.performance_analytics.record_metric(
                    metric_type=PerformanceMetricType.EXECUTION_TIME,
                    value=learning_data["execution_time"],
                    flow_id=self.flow_instance.state.flow_id,
                    operation_name=operation_name,
                    context=learning_data
                )
            
            flow_logger.debug(f"üìö Aprendizado registrado para {operation_name}")
            
        except Exception as e:
            flow_logger.error(f"‚ùå Erro no aprendizado: {e}")
    
    def get_optimization_insights(self) -> Dict[str, Any]:
        """Obter insights de otimiza√ß√£o"""
        try:
            insights = {
                "optimization_enabled": self.optimization_enabled,
                "total_optimizations": len(self.optimizations_applied),
                "optimization_controller_status": self.optimization_controller.get_status() if self.optimization_controller else None,
                "flow_optimizer_stats": self.flow_optimizer.get_optimization_stats() if self.flow_optimizer else None,
                "cache_stats": self.cache_integration.get_cache_stats() if self.cache_integration else None,
                "ml_stats": self.ml_optimizer.get_ml_stats() if self.ml_optimizer else None,
                "performance_dashboard": self.performance_analytics.get_performance_dashboard_data() if self.performance_analytics else None
            }
            
            return insights
            
        except Exception as e:
            flow_logger.error(f"‚ùå Erro ao obter insights: {e}")
            return {"error": str(e)}
    
    def get_optimization_recommendations(self) -> List[Dict[str, Any]]:
        """Obter recomenda√ß√µes de otimiza√ß√£o"""
        try:
            recommendations = []
            
            if self.ml_optimizer:
                ml_recommendations = self.ml_optimizer.get_optimization_recommendations(
                    self.flow_instance.state,
                    self.get_execution_context()
                )
                recommendations.extend([{
                    "source": "ml_optimizer",
                    "type": rec.recommendation_type,
                    "action": rec.action,
                    "expected_improvement": rec.expected_improvement,
                    "priority": rec.priority
                } for rec in ml_recommendations])
            
            if self.cache_integration:
                cache_analysis = self.cache_integration.analyze_cache_performance()
                if cache_analysis.get("recommendations"):
                    for rec in cache_analysis["recommendations"]:
                        recommendations.append({
                            "source": "cache_integration",
                            "type": "cache_optimization",
                            "action": rec,
                            "expected_improvement": 10.0,
                            "priority": "medium"
                        })
            
            return recommendations
            
        except Exception as e:
            flow_logger.error(f"‚ùå Erro ao obter recomenda√ß√µes: {e}")
            return []
    
    def get_execution_context(self) -> Dict[str, Any]:
        """Obter contexto de execu√ß√£o atual"""
        try:
            import psutil
            
            context = {
                "current_cpu": psutil.cpu_percent(interval=1),
                "current_memory": psutil.virtual_memory().percent,
                "flow_progress": self.flow_instance.state.progresso_percent,
                "analyses_completed": len(self.flow_instance.state.analises_concluidas),
                "system_load": psutil.getloadavg()[0] if hasattr(psutil, 'getloadavg') else 0.0
            }
            
            return context
            
        except Exception as e:
            flow_logger.warning(f"‚ö†Ô∏è Erro ao obter contexto: {e}")
            return {}
    
    def apply_cache_optimization(self, operation_name: str):
        """Aplicar otimiza√ß√£o de cache para opera√ß√£o espec√≠fica"""
        if self.cache_integration:
            return self.cache_integration.cache_flow_operation(
                operation_name=operation_name,
                ttl_hours=24
            )
        return lambda x: x  # No-op decorator
    
    def shutdown_optimization_systems(self):
        """Shutdown graceful dos sistemas de otimiza√ß√£o"""
        try:
            if self.optimization_controller:
                self.optimization_controller.shutdown()
            
            flow_logger.info("üõë Sistemas de otimiza√ß√£o finalizados")
            
        except Exception as e:
            flow_logger.error(f"‚ùå Erro ao finalizar sistemas de otimiza√ß√£o: {e}")

# =============== MODIFICA√á√ÉO DO FLOW PRINCIPAL PARA INTEGRAR OTIMIZA√á√ÉO ===============

# Monkey patch para adicionar otimiza√ß√£o ao Flow existente
original_init = InsightsFlow.__init__

def enhanced_init(self, persistence=None, **kwargs):
    """Enhanced __init__ com integra√ß√£o de otimiza√ß√£o"""
    # Inicializa√ß√£o original
    original_init(self, persistence, **kwargs)
    
    # Integrar sistema de otimiza√ß√£o da Etapa 4
    self.etapa4_controller = integrate_stage4_optimization(self)
    
    flow_logger.info("üöÄ Flow aprimorado com sistema de otimiza√ß√£o Etapa 4")

# Aplicar monkey patch
InsightsFlow.__init__ = enhanced_init

# Fun√ß√µes auxiliares para aplicar otimiza√ß√µes
def optimize_flow_operation(flow_instance: InsightsFlow, operation_name: str):
    """Decorator para otimizar opera√ß√µes do Flow"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            # Aplicar otimiza√ß√µes antes da execu√ß√£o
            if hasattr(flow_instance, 'etapa4_controller') and flow_instance.etapa4_controller:
                optimization_result = flow_instance.etapa4_controller.optimize_flow_execution(
                    operation_name,
                    flow_instance.etapa4_controller.get_execution_context()
                )
            
            # Executar opera√ß√£o
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                execution_time = time.time() - start_time
                
                # Aprender com o resultado
                if hasattr(flow_instance, 'etapa4_controller') and flow_instance.etapa4_controller:
                    flow_instance.etapa4_controller.learn_from_execution(
                        operation_name,
                        {
                            "execution_time": execution_time,
                            "success": True,
                            "operation_name": operation_name
                        }
                    )
                
                return result
                
            except Exception as e:
                execution_time = time.time() - start_time
                
                # Aprender com o erro
                if hasattr(flow_instance, 'etapa4_controller') and flow_instance.etapa4_controller:
                    flow_instance.etapa4_controller.learn_from_execution(
                        operation_name,
                        {
                            "execution_time": execution_time,
                            "success": False,
                            "error": str(e),
                            "operation_name": operation_name
                        }
                    )
                
                raise e
        
        return wrapper
    return decorator

# =============== FUN√á√ÉO PRINCIPAL APRIMORADA ===============

def executar_flow_completo_otimizado(data_inicio: str, data_fim: str, enable_optimization: bool = True) -> Dict[str, Any]:
    """
    üöÄ Executar Flow completo com otimiza√ß√µes da Etapa 4
    
    Args:
        data_inicio: Data de in√≠cio da an√°lise
        data_fim: Data de fim da an√°lise  
        enable_optimization: Habilitar otimiza√ß√µes da Etapa 4
    
    Returns:
        Dicion√°rio com resultados da execu√ß√£o e m√©tricas de otimiza√ß√£o
    """
    try:
        flow_logger.info("üöÄ Iniciando execu√ß√£o de Flow OTIMIZADO")
        
        # Criar Flow com otimiza√ß√£o
        flow = criar_flow_com_parametros(data_inicio, data_fim, "completo")
        
        # Configurar otimiza√ß√£o
        if enable_optimization and hasattr(flow, 'etapa4_controller') and flow.etapa4_controller:
            flow.etapa4_controller.optimization_enabled = True
            flow_logger.info("‚ö° Otimiza√ß√µes Etapa 4 habilitadas")
        
        # Executar Flow
        execution_start = time.time()
        result = flow.kickoff()
        execution_time = time.time() - execution_start
        
        # Coletar insights de otimiza√ß√£o
        optimization_insights = {}
        optimization_recommendations = []
        
        if enable_optimization and hasattr(flow, 'etapa4_controller') and flow.etapa4_controller:
            optimization_insights = flow.etapa4_controller.get_optimization_insights()
            optimization_recommendations = flow.etapa4_controller.get_optimization_recommendations()
        
        # Resultado completo
        complete_result = {
            "execution_result": result,
            "execution_time": execution_time,
            "optimization_enabled": enable_optimization,
            "optimization_insights": optimization_insights,
            "optimization_recommendations": optimization_recommendations,
            "flow_id": flow.state.flow_id,
            "timestamp": datetime.now().isoformat()
        }
        
        flow_logger.info(f"‚úÖ Flow OTIMIZADO conclu√≠do em {execution_time:.2f}s")
        
        return complete_result
        
    except Exception as e:
        flow_logger.error(f"‚ùå Erro na execu√ß√£o otimizada: {e}")
        return {
            "error": str(e),
            "execution_time": 0,
            "optimization_enabled": enable_optimization,
            "timestamp": datetime.now().isoformat()
        }

# =============== INTEGRA√á√ÉO COM DASHBOARD ===============

def get_optimization_dashboard_data() -> Dict[str, Any]:
    """Obter dados de otimiza√ß√£o para dashboard"""
    try:
        from insights.optimization import get_global_optimization_controller
        
        optimization_controller = get_global_optimization_controller()
        
        dashboard_data = {
            "optimization_status": optimization_controller.get_status(),
            "last_updated": datetime.now().isoformat(),
            "system_info": {
                "optimization_enabled": True,
                "version": "4.0.0",
                "features": [
                    "ML-based optimization",
                    "Intelligent caching", 
                    "Auto-scaling",
                    "Performance analytics",
                    "Predictive insights"
                ]
            }
        }
        
        return dashboard_data
        
    except Exception as e:
        flow_logger.error(f"‚ùå Erro ao obter dados do dashboard: {e}")
        return {"error": str(e)}

flow_logger.info("üéØ ETAPA 4 - Sistema de Otimiza√ß√£o integrado ao Flow principal")

if __name__ == "__main__":
    # Exemplo de uso direto
    from datetime import datetime, timedelta
    
    # Configurar per√≠odo (√∫ltimos 2 anos)
    data_fim = datetime.now().strftime('%Y-%m-%d')
    data_inicio = (datetime.now() - timedelta(days=730)).strftime('%Y-%m-%d')
    
    # Executar flow
    resultado = executar_flow_completo(data_inicio, data_fim)
    print(f"‚úÖ Flow executado: {resultado}") 