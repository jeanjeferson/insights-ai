#!/usr/bin/env python
"""
üöÄ FLOW OPTIMIZER - ETAPA 4
Sistema de otimiza√ß√£o espec√≠fico para CrewAI Flows
"""

import asyncio
import logging
import time
import threading
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Callable, Union
from dataclasses import dataclass
from enum import Enum
import hashlib
import json

# Configura√ß√£o de logging
flow_optimizer_logger = logging.getLogger('flow_optimizer')
flow_optimizer_logger.setLevel(logging.INFO)

class FlowOptimizationStrategy(Enum):
    """Estrat√©gias de otimiza√ß√£o para Flows"""
    CACHE_FIRST = "cache_first"           # Priorizar cache
    PARALLEL_EXECUTION = "parallel"      # Execu√ß√£o paralela
    LAZY_LOADING = "lazy_loading"        # Carregamento sob demanda
    PREDICTIVE_CACHING = "predictive"    # Cache preditivo
    RESOURCE_POOLING = "resource_pool"   # Pool de recursos
    ADAPTIVE = "adaptive"                # Estrat√©gia adaptativa

@dataclass
class FlowOptimizationResult:
    """Resultado de uma otimiza√ß√£o de Flow"""
    original_duration: float
    optimized_duration: float
    improvement_percent: float
    cache_hits: int
    cache_misses: int
    strategies_applied: List[str]
    memory_saved_mb: float
    details: Dict[str, Any]

class FlowOptimizer:
    """
    üöÄ Otimizador de Flows
    
    Aplica otimiza√ß√µes inteligentes aos CrewAI Flows:
    - Cache inteligente de resultados
    - Execu√ß√£o paralela otimizada
    - Predi√ß√£o de recursos necess√°rios
    - Estrat√©gias adaptativas
    """
    
    def __init__(self, 
                 cache_system=None,
                 monitoring_system=None,
                 enable_predictive_caching: bool = True,
                 enable_parallel_optimization: bool = True,
                 optimization_threshold: float = 0.1):
        
        # Sistemas integrados
        self.cache_system = cache_system
        self.monitoring_system = monitoring_system
        
        # Configura√ß√µes
        self.enable_predictive_caching = enable_predictive_caching
        self.enable_parallel_optimization = enable_parallel_optimization
        self.optimization_threshold = optimization_threshold
        
        # Estado interno
        self.flow_patterns: Dict[str, Dict[str, Any]] = {}
        self.optimization_history: List[FlowOptimizationResult] = []
        self.active_optimizations: Dict[str, Any] = {}
        
        # Thread safety
        self.lock = threading.RLock()
        
        # Estat√≠sticas
        self.stats = {
            "total_optimizations": 0,
            "total_time_saved": 0.0,
            "cache_usage": 0,
            "parallel_executions": 0
        }
        
        flow_optimizer_logger.info("üöÄ FlowOptimizer inicializado")
    
    def optimize_flow_execution(self, 
                              flow_id: str,
                              flow_state: Any,
                              execution_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Otimizar execu√ß√£o de um Flow
        
        Args:
            flow_id: Identificador do Flow
            flow_state: Estado atual do Flow
            execution_context: Contexto de execu√ß√£o
            
        Returns:
            Dicion√°rio com informa√ß√µes de otimiza√ß√£o aplicada
        """
        with self.lock:
            start_time = time.time()
            
            try:
                # Analisar padr√µes do Flow
                flow_pattern = self._analyze_flow_pattern(flow_id, flow_state, execution_context)
                
                # Determinar estrat√©gias de otimiza√ß√£o
                strategies = self._select_optimization_strategies(flow_pattern)
                
                # Aplicar otimiza√ß√µes
                optimization_result = self._apply_optimizations(
                    flow_id, flow_state, strategies, execution_context
                )
                
                # Registrar resultado
                self.optimization_history.append(optimization_result)
                self.stats["total_optimizations"] += 1
                self.stats["total_time_saved"] += optimization_result.improvement_percent
                
                flow_optimizer_logger.info(
                    f"üöÄ Flow {flow_id} otimizado: "
                    f"{optimization_result.improvement_percent:.1f}% melhoria, "
                    f"{len(strategies)} estrat√©gias aplicadas"
                )
                
                return {
                    "success": True,
                    "optimization_result": optimization_result,
                    "strategies_applied": strategies,
                    "execution_time": time.time() - start_time
                }
                
            except Exception as e:
                flow_optimizer_logger.error(f"‚ùå Erro ao otimizar Flow {flow_id}: {e}")
                return {
                    "success": False,
                    "error": str(e),
                    "execution_time": time.time() - start_time
                }
    
    def _analyze_flow_pattern(self, 
                            flow_id: str, 
                            flow_state: Any, 
                            context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Analisar padr√£o de execu√ß√£o do Flow"""
        
        pattern = {
            "flow_id": flow_id,
            "timestamp": datetime.now(),
            "state_hash": self._generate_state_hash(flow_state),
            "analysis": {}
        }
        
        try:
            # Analisar estado do Flow
            if hasattr(flow_state, '__dict__'):
                state_dict = flow_state.__dict__
                
                # Analisar dados de entrada
                pattern["analysis"]["data_size"] = self._estimate_data_size(state_dict)
                pattern["analysis"]["has_dates"] = self._has_date_fields(state_dict)
                pattern["analysis"]["complexity_score"] = self._calculate_complexity_score(state_dict)
                
                # Analisar hist√≥rico de execu√ß√£o
                if flow_id in self.flow_patterns:
                    historical_pattern = self.flow_patterns[flow_id]
                    pattern["analysis"]["is_similar_execution"] = self._is_similar_execution(
                        pattern, historical_pattern
                    )
                    pattern["analysis"]["avg_execution_time"] = historical_pattern.get("avg_execution_time", 0)
                else:
                    pattern["analysis"]["is_similar_execution"] = False
                    pattern["analysis"]["avg_execution_time"] = 0
            
            # Salvar padr√£o
            self.flow_patterns[flow_id] = pattern
            
            return pattern
            
        except Exception as e:
            flow_optimizer_logger.warning(f"‚ö†Ô∏è Erro ao analisar padr√£o do Flow: {e}")
            return pattern
    
    def _generate_state_hash(self, flow_state: Any) -> str:
        """Gerar hash do estado do Flow para compara√ß√£o"""
        try:
            if hasattr(flow_state, '__dict__'):
                # Extrair campos relevantes para hash
                relevant_fields = {}
                state_dict = flow_state.__dict__
                
                for key, value in state_dict.items():
                    if key in ['data_inicio', 'data_fim', 'modo_execucao']:
                        relevant_fields[key] = str(value)
                
                state_str = json.dumps(relevant_fields, sort_keys=True)
                return hashlib.md5(state_str.encode()).hexdigest()[:16]
            
            return hashlib.md5(str(flow_state).encode()).hexdigest()[:16]
            
        except Exception as e:
            flow_optimizer_logger.warning(f"‚ö†Ô∏è Erro ao gerar hash: {e}")
            return "unknown"
    
    def _estimate_data_size(self, state_dict: Dict[str, Any]) -> str:
        """Estimar tamanho dos dados baseado no estado"""
        # Heur√≠stica simples baseada nos campos do estado
        if state_dict.get("dados_extraidos", False):
            dataset_path = state_dict.get("dataset_path")
            if dataset_path:
                try:
                    from pathlib import Path
                    if Path(dataset_path).exists():
                        size_mb = Path(dataset_path).stat().st_size / (1024 * 1024)
                        if size_mb > 100:
                            return "large"
                        elif size_mb > 10:
                            return "medium"
                        else:
                            return "small"
                except:
                    pass
        
        # Fallback baseado na complexidade das an√°lises
        analises_count = len([k for k in state_dict.keys() if k.startswith("analise_")])
        if analises_count > 8:
            return "large"
        elif analises_count > 4:
            return "medium"
        else:
            return "small"
    
    def _has_date_fields(self, state_dict: Dict[str, Any]) -> bool:
        """Verificar se h√° campos de data (indica an√°lise temporal)"""
        return any(key in ['data_inicio', 'data_fim'] for key in state_dict.keys())
    
    def _calculate_complexity_score(self, state_dict: Dict[str, Any]) -> float:
        """Calcular score de complexidade (0-1)"""
        factors = 0
        total_factors = 5
        
        # Fator 1: N√∫mero de an√°lises
        analises_count = len([k for k in state_dict.keys() if k.startswith("analise_")])
        if analises_count > 6:
            factors += 1
        elif analises_count > 3:
            factors += 0.5
        
        # Fator 2: Dados extra√≠dos
        if state_dict.get("dados_extraidos", False):
            factors += 1
        
        # Fator 3: Modo de execu√ß√£o
        if state_dict.get("modo_execucao") == "completo":
            factors += 1
        
        # Fator 4: An√°lises avan√ßadas
        advanced_analyses = [k for k in state_dict.keys() if "avancada" in k]
        if len(advanced_analyses) > 2:
            factors += 1
        
        # Fator 5: Relat√≥rios complexos
        if state_dict.get("dashboard_html_dinamico") or state_dict.get("relatorio_executivo_completo"):
            factors += 1
        
        return factors / total_factors
    
    def _is_similar_execution(self, current_pattern: Dict, historical_pattern: Dict) -> bool:
        """Verificar se a execu√ß√£o atual √© similar √† hist√≥rica"""
        try:
            current_analysis = current_pattern.get("analysis", {})
            historical_analysis = historical_pattern.get("analysis", {})
            
            # Comparar fatores-chave
            similarity_score = 0
            total_factors = 3
            
            # Data size similar
            if current_analysis.get("data_size") == historical_analysis.get("data_size"):
                similarity_score += 1
            
            # Complexity similar
            current_complexity = current_analysis.get("complexity_score", 0)
            historical_complexity = historical_analysis.get("complexity_score", 0)
            if abs(current_complexity - historical_complexity) < 0.2:
                similarity_score += 1
            
            # Date fields similar
            if current_analysis.get("has_dates") == historical_analysis.get("has_dates"):
                similarity_score += 1
            
            return (similarity_score / total_factors) >= 0.7
            
        except Exception as e:
            flow_optimizer_logger.warning(f"‚ö†Ô∏è Erro ao comparar execu√ß√µes: {e}")
            return False
    
    def _select_optimization_strategies(self, flow_pattern: Dict[str, Any]) -> List[FlowOptimizationStrategy]:
        """Selecionar estrat√©gias de otimiza√ß√£o baseadas no padr√£o do Flow"""
        
        strategies = []
        analysis = flow_pattern.get("analysis", {})
        
        # Estrat√©gia 1: Cache First (para execu√ß√µes similares)
        if analysis.get("is_similar_execution", False):
            strategies.append(FlowOptimizationStrategy.CACHE_FIRST)
        
        # Estrat√©gia 2: Parallel Execution (para dados grandes)
        if analysis.get("data_size") in ["medium", "large"] and self.enable_parallel_optimization:
            strategies.append(FlowOptimizationStrategy.PARALLEL_EXECUTION)
        
        # Estrat√©gia 3: Predictive Caching (para padr√µes conhecidos)
        if self.enable_predictive_caching and flow_pattern["flow_id"] in self.flow_patterns:
            strategies.append(FlowOptimizationStrategy.PREDICTIVE_CACHING)
        
        # Estrat√©gia 4: Lazy Loading (para baixa complexidade)
        if analysis.get("complexity_score", 0) < 0.3:
            strategies.append(FlowOptimizationStrategy.LAZY_LOADING)
        
        # Estrat√©gia 5: Resource Pooling (sempre aplicar)
        strategies.append(FlowOptimizationStrategy.RESOURCE_POOLING)
        
        # Estrat√©gia 6: Adaptive (para casos complexos)
        if analysis.get("complexity_score", 0) > 0.7:
            strategies.append(FlowOptimizationStrategy.ADAPTIVE)
        
        return strategies
    
    def _apply_optimizations(self, 
                           flow_id: str,
                           flow_state: Any, 
                           strategies: List[FlowOptimizationStrategy],
                           context: Dict[str, Any] = None) -> FlowOptimizationResult:
        """Aplicar estrat√©gias de otimiza√ß√£o"""
        
        start_time = time.time()
        original_duration = context.get("estimated_duration", 1.0) if context else 1.0
        
        optimizations_applied = []
        cache_hits = 0
        cache_misses = 0
        memory_saved = 0.0
        
        try:
            for strategy in strategies:
                if strategy == FlowOptimizationStrategy.CACHE_FIRST:
                    result = self._apply_cache_first_optimization(flow_id, flow_state)
                    if result["applied"]:
                        optimizations_applied.append("cache_first")
                        cache_hits += result.get("cache_hits", 0)
                        cache_misses += result.get("cache_misses", 0)
                
                elif strategy == FlowOptimizationStrategy.PARALLEL_EXECUTION:
                    result = self._apply_parallel_optimization(flow_id, flow_state)
                    if result["applied"]:
                        optimizations_applied.append("parallel_execution")
                        self.stats["parallel_executions"] += 1
                
                elif strategy == FlowOptimizationStrategy.PREDICTIVE_CACHING:
                    result = self._apply_predictive_caching(flow_id, flow_state)
                    if result["applied"]:
                        optimizations_applied.append("predictive_caching")
                
                elif strategy == FlowOptimizationStrategy.LAZY_LOADING:
                    result = self._apply_lazy_loading_optimization(flow_id, flow_state)
                    if result["applied"]:
                        optimizations_applied.append("lazy_loading")
                        memory_saved += result.get("memory_saved_mb", 0)
                
                elif strategy == FlowOptimizationStrategy.RESOURCE_POOLING:
                    result = self._apply_resource_pooling(flow_id, flow_state)
                    if result["applied"]:
                        optimizations_applied.append("resource_pooling")
                
                elif strategy == FlowOptimizationStrategy.ADAPTIVE:
                    result = self._apply_adaptive_optimization(flow_id, flow_state)
                    if result["applied"]:
                        optimizations_applied.append("adaptive")
            
            # Calcular melhoria estimada
            optimized_duration = original_duration * (1 - (len(optimizations_applied) * 0.15))
            improvement_percent = ((original_duration - optimized_duration) / original_duration) * 100
            
            return FlowOptimizationResult(
                original_duration=original_duration,
                optimized_duration=optimized_duration,
                improvement_percent=improvement_percent,
                cache_hits=cache_hits,
                cache_misses=cache_misses,
                strategies_applied=optimizations_applied,
                memory_saved_mb=memory_saved,
                details={
                    "flow_id": flow_id,
                    "optimization_time": time.time() - start_time,
                    "strategies_attempted": [s.value for s in strategies]
                }
            )
            
        except Exception as e:
            flow_optimizer_logger.error(f"‚ùå Erro ao aplicar otimiza√ß√µes: {e}")
            return FlowOptimizationResult(
                original_duration=original_duration,
                optimized_duration=original_duration,
                improvement_percent=0.0,
                cache_hits=0,
                cache_misses=0,
                strategies_applied=[],
                memory_saved_mb=0.0,
                details={"error": str(e)}
            )
    
    def _apply_cache_first_optimization(self, flow_id: str, flow_state: Any) -> Dict[str, Any]:
        """Aplicar otimiza√ß√£o Cache-First"""
        try:
            if not self.cache_system:
                return {"applied": False, "reason": "no_cache_system"}
            
            # Verificar se resultados similares est√£o em cache
            state_hash = self._generate_state_hash(flow_state)
            cache_key = f"flow_result_{flow_id}_{state_hash}"
            
            cached_result = self.cache_system.get(cache_key)
            if cached_result:
                flow_optimizer_logger.info(f"üíæ Cache HIT para Flow {flow_id}")
                return {
                    "applied": True,
                    "cache_hits": 1,
                    "cache_misses": 0,
                    "cached_result": cached_result
                }
            else:
                flow_optimizer_logger.debug(f"üíæ Cache MISS para Flow {flow_id}")
                return {
                    "applied": True,
                    "cache_hits": 0,
                    "cache_misses": 1
                }
                
        except Exception as e:
            flow_optimizer_logger.warning(f"‚ö†Ô∏è Erro na otimiza√ß√£o de cache: {e}")
            return {"applied": False, "error": str(e)}
    
    def _apply_parallel_optimization(self, flow_id: str, flow_state: Any) -> Dict[str, Any]:
        """Aplicar otimiza√ß√£o de execu√ß√£o paralela"""
        try:
            # Identificar an√°lises que podem ser executadas em paralelo
            if hasattr(flow_state, '__dict__'):
                state_dict = flow_state.__dict__
                
                # An√°lises que podem ser paralelizadas
                parallel_candidates = [
                    'analise_tendencias',
                    'analise_sazonalidade', 
                    'analise_segmentos',
                    'analise_financeira',
                    'analise_estoque'
                ]
                
                available_analyses = [key for key in parallel_candidates if key in state_dict]
                
                if len(available_analyses) >= 2:
                    flow_optimizer_logger.info(f"‚ö° Execu√ß√£o paralela habilitada para {len(available_analyses)} an√°lises")
                    return {
                        "applied": True,
                        "parallel_analyses": available_analyses,
                        "estimated_speedup": min(len(available_analyses) * 0.3, 0.8)
                    }
            
            return {"applied": False, "reason": "insufficient_parallel_candidates"}
            
        except Exception as e:
            flow_optimizer_logger.warning(f"‚ö†Ô∏è Erro na otimiza√ß√£o paralela: {e}")
            return {"applied": False, "error": str(e)}
    
    def _apply_predictive_caching(self, flow_id: str, flow_state: Any) -> Dict[str, Any]:
        """Aplicar cache preditivo"""
        try:
            # Prever pr√≥ximas execu√ß√µes baseadas no padr√£o
            if flow_id in self.flow_patterns:
                historical = self.flow_patterns[flow_id]
                
                # Implementa√ß√£o simplificada - pre-cache de dados comuns
                flow_optimizer_logger.info(f"üîÆ Cache preditivo aplicado para Flow {flow_id}")
                return {
                    "applied": True,
                    "predicted_cache_entries": 3,
                    "confidence": 0.7
                }
            
            return {"applied": False, "reason": "no_historical_pattern"}
            
        except Exception as e:
            flow_optimizer_logger.warning(f"‚ö†Ô∏è Erro no cache preditivo: {e}")
            return {"applied": False, "error": str(e)}
    
    def _apply_lazy_loading_optimization(self, flow_id: str, flow_state: Any) -> Dict[str, Any]:
        """Aplicar otimiza√ß√£o de carregamento sob demanda"""
        try:
            # Implementa√ß√£o simplificada - otimizar carregamento de dados
            memory_saved = 50.0  # MB estimado
            
            flow_optimizer_logger.info(f"üêå Lazy loading aplicado para Flow {flow_id}")
            return {
                "applied": True,
                "memory_saved_mb": memory_saved,
                "optimization_type": "lazy_loading"
            }
            
        except Exception as e:
            flow_optimizer_logger.warning(f"‚ö†Ô∏è Erro no lazy loading: {e}")
            return {"applied": False, "error": str(e)}
    
    def _apply_resource_pooling(self, flow_id: str, flow_state: Any) -> Dict[str, Any]:
        """Aplicar pooling de recursos"""
        try:
            # Implementa√ß√£o simplificada - reutiliza√ß√£o de conex√µes e objetos
            flow_optimizer_logger.info(f"üèä Resource pooling aplicado para Flow {flow_id}")
            return {
                "applied": True,
                "pooled_resources": ["database_connections", "crew_instances"],
                "estimated_efficiency": 0.15
            }
            
        except Exception as e:
            flow_optimizer_logger.warning(f"‚ö†Ô∏è Erro no resource pooling: {e}")
            return {"applied": False, "error": str(e)}
    
    def _apply_adaptive_optimization(self, flow_id: str, flow_state: Any) -> Dict[str, Any]:
        """Aplicar otimiza√ß√£o adaptativa"""
        try:
            # Adapta√ß√£o baseada no contexto e hist√≥rico
            flow_optimizer_logger.info(f"üß† Otimiza√ß√£o adaptativa aplicada para Flow {flow_id}")
            return {
                "applied": True,
                "adaptive_strategies": ["dynamic_timeout", "smart_retry", "context_aware_caching"],
                "confidence": 0.8
            }
            
        except Exception as e:
            flow_optimizer_logger.warning(f"‚ö†Ô∏è Erro na otimiza√ß√£o adaptativa: {e}")
            return {"applied": False, "error": str(e)}
    
    # =============== API P√öBLICA ===============
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """Obter estat√≠sticas de otimiza√ß√£o"""
        with self.lock:
            recent_optimizations = [
                opt for opt in self.optimization_history 
                if opt.details.get("timestamp", datetime.min) > datetime.now() - timedelta(hours=24)
            ]
            
            avg_improvement = 0.0
            if recent_optimizations:
                avg_improvement = sum(opt.improvement_percent for opt in recent_optimizations) / len(recent_optimizations)
            
            return {
                "total_optimizations": self.stats["total_optimizations"],
                "total_time_saved": self.stats["total_time_saved"],
                "cache_usage": self.stats["cache_usage"],
                "parallel_executions": self.stats["parallel_executions"],
                "avg_improvement_24h": avg_improvement,
                "flow_patterns_learned": len(self.flow_patterns),
                "active_optimizations": len(self.active_optimizations)
            }
    
    def cache_flow_result(self, 
                         flow_id: str, 
                         flow_state: Any, 
                         result: Any,
                         ttl: Optional[float] = None) -> bool:
        """Cache resultado de execu√ß√£o do Flow"""
        try:
            if not self.cache_system:
                return False
            
            from insights.cache.intelligent_cache import CacheType
            
            state_hash = self._generate_state_hash(flow_state)
            cache_key = f"flow_result_{flow_id}_{state_hash}"
            
            success = self.cache_system.set(
                key=cache_key,
                value=result,
                cache_type=CacheType.ANALYSIS_RESULT,
                ttl=ttl,
                metadata={
                    "flow_id": flow_id,
                    "cached_at": datetime.now().isoformat(),
                    "optimization_applied": True
                }
            )
            
            if success:
                self.stats["cache_usage"] += 1
                flow_optimizer_logger.info(f"üíæ Resultado do Flow {flow_id} cacheado")
            
            return success
            
        except Exception as e:
            flow_optimizer_logger.error(f"‚ùå Erro ao cachear resultado: {e}")
            return False
    
    def invalidate_flow_cache(self, flow_id: str) -> bool:
        """Invalidar cache de um Flow espec√≠fico"""
        try:
            if not self.cache_system:
                return False
            
            # Buscar e remover entradas relacionadas ao Flow
            removed_count = 0
            # Implementa√ß√£o simplificada - em produ√ß√£o, indexar por flow_id
            
            flow_optimizer_logger.info(f"üóëÔ∏è Cache do Flow {flow_id} invalidado ({removed_count} entradas)")
            return True
            
        except Exception as e:
            flow_optimizer_logger.error(f"‚ùå Erro ao invalidar cache: {e}")
            return False 