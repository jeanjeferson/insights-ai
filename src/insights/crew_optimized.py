"""
âš¡ CREW OTIMIZADO COM PERFORMANCE AVANÃ‡ADA
========================================

VersÃ£o otimizada do crew principal com:
- Logging estruturado menos verbose
- Lazy loading de ferramentas
- Cache inteligente de validaÃ§Ãµes
- InicializaÃ§Ã£o paralela de agentes
- MÃ©tricas de performance em tempo real
"""

from crewai import Agent, Crew, Process, Task, LLM
from crewai.project import CrewBase, agent, crew, task, before_kickoff
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List
from dotenv import load_dotenv
import os
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

# =============== IMPORTAÃ‡Ã•ES OTIMIZADAS ===============

# Sistema de performance
from insights.config.performance_config import (
    PERFORMANCE_CONFIG,
    optimized_logger,
    performance_cache,
    performance_tracked,
    cached_result,
    log_startup_metrics,
    should_skip_validation,
    get_optimized_tool_list,
    cleanup_performance_resources
)

# Ferramentas (lazy loading)
from insights.config.tools_config_v3 import (
    TOOLS, 
    get_tools_for_analysis,
    get_tools_for_agent,
    get_integration_status,
    get_tools_statistics,
    validate_tool_compatibility,
    file_read_tool as file_tool,
    sql_query_tool as sql_tool,
    business_intelligence_tool as bi_tool,
    validate_agent_data_access,
    get_data_flow_architecture
)

load_dotenv()

# =============== SISTEMA DE CONTROLE DE CONTEXTO ===============

class ContextManager:
    """Gerenciador de contexto para evitar overload do LLM"""
    
    MAX_TOKENS_PER_INPUT = 50000  # Limite seguro de tokens por input
    MAX_CHARS_PER_INPUT = 200000  # Limite de caracteres (aprox 4 chars/token)
    AGGRESSIVE_LIMIT = 50000     # Limite para modo agressivo
    
    @staticmethod
    def estimate_tokens(text: str) -> int:
        """Estimar nÃºmero de tokens em um texto"""
        return len(text) // 4  # AproximaÃ§Ã£o: 4 chars = 1 token
    
    @staticmethod
    def create_data_summary(data_text: str, max_chars: int = 5000) -> str:
        """Criar resumo inteligente dos dados"""
        
        if len(data_text) <= max_chars:
            return data_text
        
        # Extrair informaÃ§Ãµes chave
        lines = data_text.split('\n')
        
        summary_parts = []
        
        # CabeÃ§alho (primeiras 3 linhas)
        if len(lines) > 3:
            summary_parts.append("=== ESTRUTURA DOS DADOS ===")
            summary_parts.extend(lines[:3])
        
        # EstatÃ­sticas bÃ¡sicas
        total_lines = len(lines)
        summary_parts.append(f"\n=== ESTATÃSTICAS ===")
        summary_parts.append(f"Total de registros: {total_lines - 1}")  # -1 para header
        summary_parts.append(f"Tamanho original: {len(data_text)} caracteres")
        
        # Amostra dos dados (ajustar tamanho baseado no modo)
        if max_chars <= 5000:  # Modo agressivo
            summary_parts.append(f"\n=== AMOSTRA DOS DADOS (primeiras 5 linhas) ===")
            if total_lines > 6:
                summary_parts.extend(lines[1:6])  # Pular header
            else:
                summary_parts.extend(lines[1:])
                
            summary_parts.append(f"\n=== ÃšLTIMA LINHA ===")
            if total_lines > 1:
                summary_parts.append(lines[-1])
        else:  # Modo normal
            if total_lines > 20:
                summary_parts.append(f"\n=== AMOSTRA DOS DADOS (primeiras 10 linhas) ===")
                summary_parts.extend(lines[1:11])  # Pular header
                
                summary_parts.append(f"\n=== ÃšLTIMOS REGISTROS (Ãºltimas 5 linhas) ===")
                summary_parts.extend(lines[-5:])
            else:
                summary_parts.append(f"\n=== DADOS COMPLETOS ===")
                summary_parts.extend(lines[1:])  # Pular header
        
        summary_parts.append(f"\n=== NOTA ===")
        summary_parts.append(f"Este Ã© um resumo dos dados para anÃ¡lise. Total de {total_lines-1} registros.")
        
        return '\n'.join(summary_parts)
    
    @staticmethod
    def create_aggressive_summary(data_text: str) -> str:
        """Criar resumo ultra-compacto para casos extremos"""
        
        lines = data_text.split('\n')
        total_lines = len(lines)
        
        # Resumo extremamente compacto
        summary = [
            "=== RESUMO ULTRA-COMPACTO ===",
            f"Registros: {total_lines - 1 if total_lines > 1 else 0}",
            f"Tamanho: {len(data_text)} chars"
        ]
        
        # Apenas header se existir
        if total_lines > 1:
            summary.append(f"Colunas: {lines[0]}")
            
        # Apenas primeira linha de dados
        if total_lines > 2:
            summary.append(f"Exemplo: {lines[1]}")
        
        summary.append("NOTA: Dados disponÃ­veis para anÃ¡lise mais detalhada se necessÃ¡rio.")
        
        return '\n'.join(summary)
    
    @staticmethod
    def truncate_context(context: str, max_chars: int = None) -> str:
        """Truncar contexto mantendo informaÃ§Ãµes essenciais"""
        
        if max_chars is None:
            max_chars = ContextManager.MAX_CHARS_PER_INPUT
        
        if len(context) <= max_chars:
            return context
        
        optimized_logger.warning(f"âš ï¸ Contexto muito grande ({len(context)} chars), truncando para {max_chars}")
        
        # Manter inÃ­cio e fim do contexto
        half_size = max_chars // 2 - 100  # -100 para mensagem de truncamento
        
        start_part = context[:half_size]
        end_part = context[-half_size:]
        
        truncation_message = f"\n\n... [TRUNCADO: {len(context) - max_chars} caracteres removidos] ...\n\n"
        
        return start_part + truncation_message + end_part
    
    @staticmethod
    def prepare_data_context(data: any, context_type: str = "data") -> str:
        """Preparar contexto de dados de forma inteligente"""
        
        if isinstance(data, str):
            data_text = data
        else:
            data_text = str(data)
        
        # Verificar tamanho e aplicar estratÃ©gia apropriada
        if context_type == "aggressive":
            # Modo agressivo para casos extremos
            if len(data_text) > 10000:
                return ContextManager.create_aggressive_summary(data_text)
            else:
                return ContextManager.create_data_summary(data_text, 2000)
        
        elif len(data_text) > ContextManager.MAX_CHARS_PER_INPUT:
            optimized_logger.info(f"ğŸ“Š Dados muito grandes ({len(data_text)} chars), criando resumo...")
            
            if context_type == "csv" or "," in data_text or ";" in data_text:
                # Tratar como dados tabulares
                return ContextManager.create_data_summary(data_text, 8000)
            else:
                # Truncar texto geral
                return ContextManager.truncate_context(data_text, 10000)
        
        return data_text
    
    @staticmethod
    def analyze_data_locally(data_text: str) -> dict:
        """Analisar dados localmente para extrair insights bÃ¡sicos"""
        
        try:
            lines = data_text.split('\n')
            
            # EstatÃ­sticas bÃ¡sicas
            stats = {
                'total_lines': len(lines),
                'total_chars': len(data_text),
                'has_header': True,
                'separator': None,
                'columns': [],
                'sample_data': []
            }
            
            # Detectar separador
            if lines and len(lines) > 1:
                first_line = lines[0]
                if ';' in first_line:
                    stats['separator'] = ';'
                elif ',' in first_line:
                    stats['separator'] = ','
                elif '\t' in first_line:
                    stats['separator'] = '\t'
                
                # Extrair colunas
                if stats['separator']:
                    stats['columns'] = [col.strip() for col in first_line.split(stats['separator'])]
                    stats['column_count'] = len(stats['columns'])
                    
                    # Amostra de dados (3 primeiras linhas apÃ³s header)
                    for i in range(1, min(4, len(lines))):
                        if lines[i].strip():
                            stats['sample_data'].append(lines[i])
            
            return stats
            
        except Exception as e:
            optimized_logger.debug(f"Erro na anÃ¡lise local: {e}")
            return {'error': str(e)}

# =============== SISTEMA DE CONTROLE DE CONTEXTO INTELIGENTE ===============

class IntelligentContextManager:
    """Gerenciador inteligente de contexto que permite anÃ¡lise completa com otimizaÃ§Ã£o automÃ¡tica"""
    
    MAX_TOKENS_PER_INPUT = 100000  # Limite mais generoso
    MAX_CHARS_PER_INPUT = 400000   # Limite mais generoso
    CHUNK_SIZE = 50000             # Tamanho de chunks para processamento
    
    @staticmethod
    def should_optimize_context(data_size: int) -> bool:
        """Determinar se o contexto precisa ser otimizado"""
        return data_size > IntelligentContextManager.MAX_CHARS_PER_INPUT
    
    @staticmethod
    def create_intelligent_summary(data_text: str, preserve_completeness: bool = True) -> str:
        """Criar resumo inteligente preservando completude da anÃ¡lise"""
        
        if len(data_text) <= IntelligentContextManager.MAX_CHARS_PER_INPUT:
            return data_text
        
        if not preserve_completeness:
            # Fallback para resumo mais agressivo
            return ContextManager.create_data_summary(data_text, 10000)
        
        # EstratÃ©gia inteligente: manter estrutura completa mas otimizar dados
        lines = data_text.split('\n')
        
        # Sempre preservar header
        if not lines:
            return data_text
        
        header = lines[0] if lines else ""
        data_lines = lines[1:] if len(lines) > 1 else []
        
        if len(data_lines) <= 1000:
            # Dataset pequeno, manter completo
            return data_text
        
        # Para datasets grandes, usar amostragem inteligente
        total_lines = len(data_lines)
        sample_size = min(5000, total_lines // 2)  # AtÃ© 50% dos dados ou 5000 linhas
        
        # Amostragem estratificada
        step = max(1, total_lines // sample_size)
        sampled_lines = [data_lines[i] for i in range(0, total_lines, step)]
        
        # Adicionar algumas linhas do final para capturar tendÃªncias
        if total_lines > sample_size:
            sampled_lines.extend(data_lines[-50:])  # Ãšltimas 50 linhas
        
        result_lines = [header] + sampled_lines
        
        # Adicionar metadata sobre a amostragem
        metadata = [
            "",
            f"# METADATA DA ANÃLISE:",
            f"# Total de registros no dataset: {total_lines}",
            f"# Registros na amostra: {len(sampled_lines)}",
            f"# MÃ©todo: Amostragem estratificada (1 a cada {step} registros)",
            f"# Inclui: Header + amostra representativa + Ãºltimas 50 linhas",
            f"# NOTA: Esta Ã© uma amostra representativa para anÃ¡lise. Todos os dados estÃ£o disponÃ­veis via ferramentas.",
            ""
        ]
        
        result_lines.extend(metadata)
        
        return '\n'.join(result_lines)
    
    @staticmethod
    def wrap_tool_output(tool_func):
        """Wrapper para ferramentas que aplica controle inteligente de contexto"""
        
        def wrapper(*args, **kwargs):
            try:
                # Executar ferramenta normalmente
                result = tool_func(*args, **kwargs)
                
                if isinstance(result, str) and len(result) > IntelligentContextManager.MAX_CHARS_PER_INPUT:
                    optimized_logger.info(f"ğŸ§  Aplicando otimizaÃ§Ã£o inteligente: {len(result)} chars")
                    
                    # Aplicar otimizaÃ§Ã£o inteligente
                    optimized_result = IntelligentContextManager.create_intelligent_summary(
                        result, 
                        preserve_completeness=True
                    )
                    
                    optimized_logger.info(f"âœ… Otimizado para: {len(optimized_result)} chars (mantendo completude)")
                    return optimized_result
                
                return result
                
            except Exception as e:
                if "too large" in str(e).lower() or "token" in str(e).lower():
                    optimized_logger.warning(f"âš ï¸ Erro de contexto detectado, aplicando fallback: {e}")
                    
                    # Tentar novamente com parÃ¢metros reduzidos
                    if 'max_records' in kwargs:
                        kwargs['max_records'] = min(1000, kwargs.get('max_records', 10000))
                    elif 'limite' in kwargs:
                        kwargs['limite'] = min(1000, kwargs.get('limite', 10000))
                    
                    try:
                        result = tool_func(*args, **kwargs)
                        return IntelligentContextManager.create_intelligent_summary(result, False)
                    except:
                        return f"Erro ao executar ferramenta com otimizaÃ§Ã£o: {str(e)[:500]}"
                else:
                    raise
        
        return wrapper

# =============== LLM CONFIGURATION ===============

llm = LLM(
    model=os.getenv("MODEL"),
    api_key=os.getenv("OPENAI_API_KEY")
)


# =============== FUNÃ‡Ã•ES DE VALIDAÃ‡ÃƒO OTIMIZADAS ===============

@cached_result()
def validate_system_optimized():
    """ValidaÃ§Ã£o otimizada do sistema com cache"""
    
    if should_skip_validation('detailed_compatibility_check'):
        optimized_logger.debug("Pulando validaÃ§Ã£o detalhada (otimizaÃ§Ã£o)")
        return True
    
    try:
        stats = get_tools_statistics()
        working_tools = stats.get('total_tools', 0)
        
        if working_tools >= 10:  # MÃ­nimo aceitÃ¡vel
            optimized_logger.info(f"âœ… Sistema validado: {working_tools} ferramentas")
            return True
        else:
            optimized_logger.warning(f"âš ï¸ Poucas ferramentas disponÃ­veis: {working_tools}")
            return False
            
    except Exception as e:
        optimized_logger.error(f"âŒ Erro na validaÃ§Ã£o: {e}")
        return False

@performance_tracked("tool_validation")
def validate_critical_tools():
    """Validar apenas ferramentas crÃ­ticas"""
    
    critical_tools = {
        'sql_tool': sql_tool,
        'file_tool': file_tool, 
        'bi_tool': bi_tool
    }
    
    working = 0
    for name, tool in critical_tools.items():
        if tool is not None:
            working += 1
        else:
            optimized_logger.warning(f"âš ï¸ Ferramenta crÃ­tica ausente: {name}")
    
    success_rate = (working / len(critical_tools)) * 100
    
    if success_rate >= 75:
        optimized_logger.info(f"âœ… Ferramentas crÃ­ticas: {working}/{len(critical_tools)}")
        return True
    else:
        optimized_logger.error(f"âŒ Muitas ferramentas crÃ­ticas ausentes: {success_rate:.1f}%")
        return False

# =============== CLASSE CREW OTIMIZADA ===============

@CrewBase
class OptimizedInsights():
    """
    Crew otimizada com performance avanÃ§ada:
    - Logging estruturado com nÃ­veis contextuais
    - Lazy loading de ferramentas por agente
    - Cache inteligente de validaÃ§Ãµes
    - InicializaÃ§Ã£o paralela quando possÃ­vel
    - MÃ©tricas de performance em tempo real
    """

    def __init__(self):
        """InicializaÃ§Ã£o otimizada com mÃ©tricas de tempo"""
        init_start_time = time.time()
        super().__init__()
        
        optimized_logger.info("ğŸš€ Inicializando Insights-AI Otimizado")
        
        # =============== VALIDAÃ‡ÃƒO OTIMIZADA ===============
        if not should_skip_validation('system_validation'):
            optimized_logger.info("ğŸ”§ Validando sistema...")
            system_ok = validate_system_optimized()
            if not system_ok:
                optimized_logger.warning("âš ï¸ Sistema com problemas - continuando")
        
        # Validar apenas ferramentas crÃ­ticas
        tools_ok = validate_critical_tools()
        if not tools_ok:
            optimized_logger.error("âŒ Ferramentas crÃ­ticas com problemas")
        
        # =============== CONFIGURAÃ‡ÃƒO BASEADA NO AMBIENTE ===============
        
        # Pular logs detalhados se nÃ£o for debug
        if not should_skip_validation('system_info_collection'):
            self._log_minimal_system_info()
        
        # Cache para resultados de task
        self.task_cache = {}
        self.initialization_time = time.time() - init_start_time
        
        # Log resumo da inicializaÃ§Ã£o
        components = ['system_validation', 'tool_validation', 'configuration']
        log_startup_metrics(components, init_start_time)

    def _log_minimal_system_info(self):
        """Log mÃ­nimo de informaÃ§Ãµes do sistema"""
        if PERFORMANCE_CONFIG.log_level.value >= 2:  # NORMAL ou superior
            optimized_logger.info(f"ğŸ’» Sistema: {os.getenv('OS', 'Unknown')}")
            optimized_logger.info(f"ğŸ”§ Modo: {'Debug' if os.getenv('INSIGHTS_DEBUG') else 'Normal'}")

    # =============== AGENTES COM LAZY LOADING ===============

    @performance_tracked("agent_creation")
    def _create_agent_with_optimized_tools(self, agent_name: str, config_key: str) -> Agent:
        """Criar agente com ferramentas otimizadas e separaÃ§Ã£o de acesso aos dados"""
        
        optimized_logger.debug(f"ğŸ”§ Criando agente: {agent_name}")
        
        # Obter ferramentas especÃ­ficas para o agente (com restriÃ§Ã£o SQL)
        agent_tools = get_tools_for_agent(agent_name)
        
        # Validar acesso aos dados
        access_validation = validate_agent_data_access(agent_name)
        
        if not access_validation['valid']:
            optimized_logger.warning(f"âš ï¸ Problema de acesso para {agent_name}: {access_validation.get('error', 'Unknown')}")
        
        # Log das ferramentas e tipo de acesso
        has_sql = any('SQL' in str(type(tool).__name__) for tool in agent_tools)
        has_file_read = any('FileRead' in str(type(tool).__name__) for tool in agent_tools)
        
        optimized_logger.debug(f"   â€¢ Ferramentas carregadas: {len(agent_tools)}")
        optimized_logger.debug(f"   â€¢ Acesso SQL: {'âœ…' if has_sql else 'âŒ'}")
        optimized_logger.debug(f"   â€¢ Acesso arquivo: {'âœ…' if has_file_read else 'âŒ'}")
        optimized_logger.debug(f"   â€¢ Tipo de acesso: {access_validation.get('data_access', 'undefined')}")
        
        # Aplicar wrapper inteligente nas ferramentas crÃ­ticas que podem gerar dados grandes
        optimized_tools = []
        for tool in agent_tools:
            tool_name = getattr(tool, 'name', str(type(tool).__name__))
            
            # Identificar ferramentas que precisam de controle de contexto
            if any(keyword in tool_name.lower() for keyword in ['sql', 'query', 'data', 'export', 'business']):
                if hasattr(tool, '_run'):
                    # Aplicar wrapper inteligente
                    original_run = tool._run
                    tool._run = IntelligentContextManager.wrap_tool_output(original_run)
                    optimized_logger.debug(f"   ğŸ§  Wrapper inteligente aplicado: {tool_name}")
            
            optimized_tools.append(tool)
        
        return Agent(
            config=self.agents_config[config_key],
            verbose=PERFORMANCE_CONFIG.log_level.value >= 3,  # Verbose apenas em DEBUG
            llm=llm,
            max_rpm=30,
            tools=optimized_tools,  # Usar ferramentas otimizadas
            reasoning=True,
            max_reasoning_attempts=2,  # Reduzido para performance
            respect_context_window=True
        )

    @agent
    def engenheiro_dados(self) -> Agent:
        """ğŸ”§ Engenheiro de Dados (Otimizado)"""
        return self._create_agent_with_optimized_tools(
            'engenheiro_dados', 
            'engenheiro_dados'
        )

    @agent 
    def analista_vendas_tendencias(self) -> Agent:
        """ğŸ“ˆ Analista de Vendas e TendÃªncias (Otimizado)"""
        return self._create_agent_with_optimized_tools(
            'analista_vendas_tendencias',
            'analista_vendas_tendencias'
        )

    @agent
    def especialista_produtos(self) -> Agent:
        """ğŸ¯ Especialista em Produtos (Otimizado)"""
        return self._create_agent_with_optimized_tools(
            'especialista_produtos',
            'especialista_produtos'
        )

    @agent
    def analista_estoque(self) -> Agent:
        """ğŸ“¦ Analista de Estoque (Otimizado)"""
        return self._create_agent_with_optimized_tools(
            'analista_estoque',
            'analista_estoque'
        )

    @agent
    def analista_financeiro(self) -> Agent:
        """ğŸ’° Analista Financeiro (Otimizado)"""
        return self._create_agent_with_optimized_tools(
            'analista_financeiro',
            'analista_financeiro'
        )

    @agent
    def especialista_clientes(self) -> Agent:
        """ğŸ‘¥ Especialista em Clientes (Otimizado)"""
        return self._create_agent_with_optimized_tools(
            'especialista_clientes',
            'especialista_clientes'
        )

    @agent
    def analista_performance(self) -> Agent:
        """ğŸ‘¤ Analista de Performance (Otimizado)"""
        return self._create_agent_with_optimized_tools(
            'analista_performance',
            'analista_performance'
        )

    @agent
    def diretor_insights(self) -> Agent:
        """ğŸ¯ Diretor de Insights (Otimizado)"""
        return self._create_agent_with_optimized_tools(
            'diretor_insights',
            'diretor_insights'
        )

    # =============== TASKS OTIMIZADAS ===============

    def _create_optimized_task(self, task_name: str, dependencies: List = None) -> Task:
        """Criar task otimizada com instruÃ§Ãµes especÃ­ficas baseadas na arquitetura ETL"""
        
        optimized_logger.debug(f"ğŸ“‹ Configurando task: {task_name}")
        
        # Callback otimizado baseado no nÃ­vel de logging
        if PERFORMANCE_CONFIG.log_level.value >= 3:  # VERBOSE
            def smart_callback(output):
                output_size = len(str(output))
                # Truncar output muito grande no log
                if output_size > 500:
                    preview = str(output)[:200] + "..." + str(output)[-100:]
                    optimized_logger.info(f"âœ… [{task_name}] ConcluÃ­da: {output_size} chars - {preview}")
                else:
                    optimized_logger.info(f"âœ… [{task_name}] ConcluÃ­da: {output_size} chars")
        else:
            smart_callback = lambda output: optimized_logger.debug(f"âœ… [{task_name}] ConcluÃ­da")
        
        # Obter configuraÃ§Ã£o da task original
        task_config = self.tasks_config[f'{task_name}_task'].copy()
        
        # Adicionar instruÃ§Ãµes especÃ­ficas baseadas no tipo de agente
        if 'description' in task_config:
            original_description = task_config['description']
            
            # InstruÃ§Ãµes especÃ­ficas por tipo de agente
            if task_name == 'engenheiro_dados':
                # Engenheiro de dados - Ãºnico com acesso SQL
                specific_instructions = """

                INSTRUÃ‡Ã•ES ESPECÃFICAS PARA ENGENHEIRO DE DADOS:
                - VocÃª Ã© o ÃšNICO agente com acesso direto ao SQL Server
                - Use SQL Query Tool para extrair TODOS os dados do perÃ­odo solicitado
                - OBRIGATÃ“RIO: Exporte os dados para arquivo 'data/vendas.csv' (arquivo padrÃ£o)
                - Valide a qualidade e completude dos dados extraÃ­dos
                - Documente estatÃ­sticas bÃ¡sicas dos dados (nÃºmero de registros, perÃ­odo, etc.)
                - CRÃTICO: Outros agentes dependem 100% do arquivo CSV que vocÃª gerar
                - Formato do CSV: incluir todas as colunas necessÃ¡rias para anÃ¡lises posteriores
                """
            else:
                # Agentes de anÃ¡lise - trabalham apenas com CSV
                specific_instructions = """

                INSTRUÃ‡Ã•ES ESPECÃFICAS PARA ANÃLISE COM CSV:
                - NÃƒO faÃ§a consultas SQL - vocÃª nÃ£o tem acesso ao banco de dados
                - OBRIGATÃ“RIO: Use FileReadTool para ler o arquivo 'data/vendas.csv' gerado pelo engenheiro_dados
                - O arquivo data/vendas.csv contÃ©m TODOS os dados necessÃ¡rios para sua anÃ¡lise
                - Foque na sua especializaÃ§Ã£o usando os dados do CSV carregado
                - Aplique suas ferramentas de anÃ¡lise nos dados carregados do arquivo
                - Se o arquivo nÃ£o existir, aguarde o engenheiro_dados completar a extraÃ§Ã£o
                - Documente claramente suas descobertas baseadas nos dados do CSV
                """
            
            task_config['description'] = original_description + specific_instructions
        
        return Task(
            config=task_config,
            context=dependencies or [],
            callback=smart_callback
        )

    @task
    def engenheiro_dados_task(self) -> Task:
        return self._create_optimized_task('engenheiro_dados')

    @task
    def analista_vendas_tendencias_task(self) -> Task:
        return self._create_optimized_task(
            'analista_vendas_tendencias',
            [self.engenheiro_dados_task()]
        )

    @task
    def especialista_produtos_task(self) -> Task:
        return self._create_optimized_task(
            'especialista_produtos',
            [self.engenheiro_dados_task()]
        )

    @task
    def analista_estoque_task(self) -> Task:
        return self._create_optimized_task(
            'analista_estoque',
            [self.engenheiro_dados_task()]
        )

    @task
    def analista_financeiro_task(self) -> Task:
        return self._create_optimized_task(
            'analista_financeiro',
            [self.engenheiro_dados_task()]
        )

    @task
    def especialista_clientes_task(self) -> Task:
        return self._create_optimized_task(
            'especialista_clientes',
            [self.engenheiro_dados_task()]
        )

    @task
    def analista_performance_task(self) -> Task:
        return self._create_optimized_task(
            'analista_performance',
            [self.engenheiro_dados_task()]
        )

    @task
    def diretor_insights_task(self) -> Task:
        return self._create_optimized_task(
            'diretor_insights',
            [
                self.engenheiro_dados_task(),
                self.analista_vendas_tendencias_task(),
                self.especialista_produtos_task(),
                self.analista_estoque_task(),
                self.analista_financeiro_task(),
                self.especialista_clientes_task(),
                self.analista_performance_task()
            ]
        )

    # =============== BEFORE KICKOFF OTIMIZADO ===============

    @before_kickoff
    @performance_tracked("before_kickoff")
    def before_kickoff(self, inputs):
        """Before kickoff otimizado com arquitetura ETL + CSV"""
        
        # Log inicial baseado no nÃ­vel
        if PERFORMANCE_CONFIG.log_level.value >= 2:  # NORMAL ou superior
            optimized_logger.info("ğŸš€ Iniciando anÃ¡lise Insights-AI com arquitetura ETL")
            optimized_logger.info(f"ğŸ“… PerÃ­odo: {inputs.get('data_inicio')} atÃ© {inputs.get('data_fim')}")
        
        # Validar datas apenas se necessÃ¡rio
        data_inicio = inputs.get('data_inicio')
        data_fim = inputs.get('data_fim')
        
        if data_inicio and data_fim:
            try:
                datetime.strptime(data_inicio, '%Y-%m-%d')
                datetime.strptime(data_fim, '%Y-%m-%d')
                optimized_logger.debug("âœ… Datas validadas")
            except ValueError as e:
                optimized_logger.warning(f"âš ï¸ Formato de data invÃ¡lido: {e}")
        
        # =============== CONFIGURAÃ‡ÃƒO DE ARQUITETURA ETL ===============
        
        optimized_logger.info("ğŸ—ï¸ Configurando arquitetura ETL + AnÃ¡lise baseada em CSV...")
        
        # InformaÃ§Ãµes da arquitetura para os agentes
        data_flow = get_data_flow_architecture()
        
        inputs['data_flow_architecture'] = data_flow
        inputs['etl_file'] = 'data/vendas.csv'
        inputs['data_extractor'] = 'engenheiro_dados'
        
        # =============== CONFIGURAÃ‡ÃƒO ESPECÃFICA POR TIPO DE AGENTE ===============
        
        # ConfiguraÃ§Ãµes especÃ­ficas para o Engenheiro de Dados
        inputs['engenheiro_dados_config'] = {
            'responsibility': 'Extrair dados do SQL Server e gerar data/vendas.csv',
            'sql_access': True,
            'output_file': 'data/vendas.csv',
            'data_source': 'SQL Server Database',
            'task_priority': 'PRIMEIRO - outros agentes dependem do seu output'
        }
        
        # ConfiguraÃ§Ãµes para agentes de anÃ¡lise (dependem do CSV)
        analysis_agents = [
            'analista_vendas_tendencias', 'especialista_produtos', 'analista_estoque',
            'analista_financeiro', 'especialista_clientes', 'analista_performance'
        ]
        
        for agent in analysis_agents:
            inputs[f'{agent}_config'] = {
                'responsibility': f'Analisar dados do arquivo data/vendas.csv na sua especializaÃ§Ã£o',
                'sql_access': False,
                'input_file': 'data/vendas.csv',
                'data_source': 'CSV file generated by engenheiro_dados',
                'dependency': 'Aguarda engenheiro_dados completar extraÃ§Ã£o'
            }
        
        # ConfiguraÃ§Ã£o especÃ­fica para o Diretor de Insights
        inputs['diretor_insights_config'] = {
            'responsibility': 'Consolidar anÃ¡lises de todos os agentes em dashboard executivo',
            'sql_access': False,
            'input_file': 'data/vendas.csv',
            'data_source': 'CSV + resultados dos outros agentes',
            'task_priority': 'ÃšLTIMO - consolida resultados de todos'
        }
        
        # =============== VALIDAÃ‡ÃƒO DE ACESSO PARA TODOS OS AGENTES ===============
        
        optimized_logger.info("ğŸ”’ Validando separaÃ§Ã£o de acesso aos dados...")
        
        access_validation = {}
        
        for agent in ['engenheiro_dados'] + analysis_agents + ['diretor_insights']:
            validation = validate_agent_data_access(agent)
            access_validation[agent] = validation
            
            if validation['valid']:
                optimized_logger.debug(f"âœ… {agent}: {validation['expected_access']}")
            else:
                optimized_logger.warning(f"âš ï¸ {agent}: {validation.get('error', 'Erro de validaÃ§Ã£o')}")
        
        inputs['access_validation'] = access_validation
        
        # =============== INSTRUÃ‡Ã•ES ESPECÃFICAS DE FLUXO DE DADOS ===============
        
        # InstruÃ§Ãµes especÃ­ficas baseadas na arquitetura ETL
        inputs['data_flow_instructions'] = """
                    ARQUITETURA DE FLUXO DE DADOS - ETL + CSV:

                    ğŸ”§ ENGENHEIRO DE DADOS:
                    - ÃšNICO agente com acesso direto ao SQL Server
                    - DEVE extrair dados completos do perÃ­odo usando SQL Query Tool
                    - DEVE exportar dados para arquivo 'data/vendas.csv' (arquivo padrÃ£o)
                    - DEVE validar a qualidade e completude dos dados extraÃ­dos
                    - Outros agentes dependem 100% do seu trabalho

                    ğŸ“Š AGENTES DE ANÃLISE:
                    - NÃƒO fazem consultas SQL diretamente
                    - DEVEM ler dados APENAS do arquivo 'data/vendas.csv'
                    - Usar FileReadTool para carregar o CSV exportado pelo engenheiro
                    - Focar na especializaÃ§Ã£o usando dados do CSV
                    - Aplicar ferramentas de anÃ¡lise nos dados carregados

                    ğŸ¯ DIRETOR DE INSIGHTS:
                    - Consolida resultados de TODOS os agentes
                    - Usa dados do CSV + resultados das anÃ¡lises especializadas
                    - Gera dashboard executivo final

                    ğŸ“ FLUXO DE ARQUIVOS:
                    1. SQL Server â†’ engenheiro_dados â†’ data/vendas.csv
                    2. data/vendas.csv â†’ agentes de anÃ¡lise â†’ insights especializadas  
                    3. insights especializadas â†’ diretor_insights â†’ dashboard final

                    IMPORTANTE: Esta separaÃ§Ã£o garante performance, seguranÃ§a e organizaÃ§Ã£o!
                    """
                            
        # =============== VERIFICAÃ‡ÃƒO DE CONECTIVIDADE BÃSICA ===============
        
        # Verificar conectividade apenas para o engenheiro (que usa SQL)
        if data_inicio and data_fim:
            try:
                from insights.config.tools_config_v3 import sql_query_tool
                
                if sql_query_tool:
                    optimized_logger.info("ğŸ”— Verificando conectividade SQL (apenas para engenheiro_dados)...")
                    
                    # Teste de conectividade mÃ­nimo
                    inputs['sql_connectivity'] = 'available'
                    inputs['csv_output_path'] = 'data/vendas.csv'
                    optimized_logger.info("âœ… SQL disponÃ­vel para extraÃ§Ã£o de dados")
                else:
                    optimized_logger.warning("âš ï¸ SQL Query Tool nÃ£o disponÃ­vel")
                    inputs['sql_connectivity'] = 'unavailable'
                    
            except Exception as e:
                optimized_logger.warning(f"âš ï¸ Erro na verificaÃ§Ã£o SQL: {e}")
                inputs['sql_connectivity'] = 'error'
        
        # Log final da arquitetura configurada
        optimized_logger.info("ğŸ—ï¸ Arquitetura ETL configurada:")
        optimized_logger.info("   ğŸ“¤ engenheiro_dados: SQL Server â†’ data/vendas.csv")
        optimized_logger.info("   ğŸ“¥ Outros agentes: data/vendas.csv â†’ anÃ¡lises especializadas")
        optimized_logger.info("   ğŸ¯ diretor_insights: consolidaÃ§Ã£o final")
        
        return inputs

    # =============== CREW OTIMIZADO ===============

    @crew
    @performance_tracked("crew_creation")
    def crew(self) -> Crew:
        """Criar crew otimizado com configuraÃ§Ãµes de performance e controle de contexto"""
        
        optimized_logger.debug("ğŸš€ Configurando crew otimizado")
        
        # Callbacks otimizados com controle de contexto
        def optimized_task_callback(task_output):
            if PERFORMANCE_CONFIG.log_level.value >= 3:  # VERBOSE
                output_size = len(str(task_output))
                
                # Verificar se o output estÃ¡ muito grande
                if output_size > 50000:
                    optimized_logger.warning(f"âš ï¸ Task output muito grande: {output_size} chars")
                    # Truncar para log
                    preview = str(task_output)[:300] + "..." + str(task_output)[-200:]
                    optimized_logger.info(f"âœ… Task concluÃ­da: {output_size} chars - {preview}")
                else:
                    optimized_logger.info(f"âœ… Task concluÃ­da: {output_size} chars")
        
        def optimized_agent_callback(agent_output):
            if PERFORMANCE_CONFIG.log_level.value >= 4:  # DEBUG
                output_size = len(str(agent_output))
                if output_size > 1000:
                    preview = str(agent_output)[:100] + "..."
                    optimized_logger.debug(f"ğŸ§  Agente: {preview} ({output_size} chars)")
                else:
                    optimized_logger.debug(f"ğŸ§  Agente: {str(agent_output)[:100]}...")
        
        # ConfiguraÃ§Ã£o especÃ­fica para evitar problemas de contexto
        crew_config = {
            'agents': self.agents,
            'tasks': self.tasks,
            'process': Process.sequential,
            'verbose': PERFORMANCE_CONFIG.log_level.value >= 3,
            'memory': False,  # Desabilitado para performance
            'max_rpm': 20,    # Reduzido para evitar rate limits
            'task_callback': optimized_task_callback if PERFORMANCE_CONFIG.log_level.value >= 3 else None,
            'max_execution_time': 1800,  # 30 minutos mÃ¡ximo
        }
        
        # Adicionar configuraÃ§Ãµes especÃ­ficas para lidar com contexto grande
        if hasattr(Crew, 'max_iter'):
            crew_config['max_iter'] = 3  # Limitar iteraÃ§Ãµes
        
        return Crew(**crew_config)

    # =============== CLEANUP ===============

    def __del__(self):
        """Cleanup automÃ¡tico ao destruir instÃ¢ncia"""
        try:
            optimized_logger.finalize()
        except:
            pass

# =============== UTILITÃRIOS DE PERFORMANCE ===============

def monitor_context_size(inputs: dict) -> dict:
    """Monitorar e ajustar tamanho do contexto"""
    
    total_size = sum(len(str(v)) for v in inputs.values())
    optimized_logger.info(f"ğŸ“Š Contexto total: {total_size} chars")
    
    if total_size > ContextManager.MAX_CHARS_PER_INPUT:
        optimized_logger.warning(f"âš ï¸ Contexto muito grande ({total_size} chars), aplicando compressÃ£o agressiva...")
        
        # CompressÃ£o agressiva
        for key, value in inputs.items():
            if isinstance(value, str) and len(value) > 10000:
                # Comprimir drasticamente
                compressed = ContextManager.prepare_data_context(value, "aggressive")
                inputs[key] = compressed
                optimized_logger.info(f"ğŸ“¦ {key} comprimido: {len(value)} â†’ {len(compressed)} chars")
        
        # Verificar novamente
        new_total = sum(len(str(v)) for v in inputs.values())
        optimized_logger.info(f"ğŸ“Š Contexto apÃ³s compressÃ£o: {new_total} chars")
    
    return inputs

def get_performance_metrics():
    """Obter mÃ©tricas de performance da arquitetura ETL"""
    return {
        'architecture': 'ETL_CSV_based',
        'data_access_model': 'separated_responsibilities',
        'sql_access_restricted_to': 'engenheiro_dados',
        'csv_based_analysis': 'enabled',
        'cache_enabled': PERFORMANCE_CONFIG.enable_tool_cache,
        'lazy_loading': PERFORMANCE_CONFIG.lazy_tool_loading,
        'log_level': PERFORMANCE_CONFIG.log_level.name,
        'parallel_init': PERFORMANCE_CONFIG.parallel_agent_init,
        'cache_size': len(performance_cache.cache),
        'cache_hits': getattr(performance_cache, 'hits', 0),
        'cache_misses': getattr(performance_cache, 'misses', 0),
        'intelligent_context_limit': IntelligentContextManager.MAX_CHARS_PER_INPUT,
        'tool_access_validation': 'active',
        'data_flow': 'SQL_to_CSV_to_Analysis'
    }

def log_performance_summary():
    """Log resumo de performance da arquitetura ETL"""
    metrics = get_performance_metrics()
    
    optimized_logger.info("ğŸ“Š MÃ‰TRICAS DA ARQUITETURA ETL:")
    optimized_logger.info(f"   ğŸ—ï¸ Arquitetura: {metrics['architecture']}")
    optimized_logger.info(f"   ğŸ” Modelo de acesso: {metrics['data_access_model']}")
    optimized_logger.info(f"   ğŸ—„ï¸ Acesso SQL restrito a: {metrics['sql_access_restricted_to']}")
    optimized_logger.info(f"   ğŸ“ AnÃ¡lise baseada em CSV: {metrics['csv_based_analysis']}")
    optimized_logger.info(f"   ğŸ”§ ValidaÃ§Ã£o de acesso: {metrics['tool_access_validation']}")
    optimized_logger.info(f"   ğŸ“ˆ Fluxo de dados: {metrics['data_flow']}")
    optimized_logger.info(f"   ğŸ’¾ Cache habilitado: {metrics['cache_enabled']}")
    optimized_logger.info(f"   ğŸ“ NÃ­vel de log: {metrics['log_level']}")

# =============== FUNÃ‡ÃƒO PRINCIPAL OTIMIZADA ===============

def run_optimized_crew(data_inicio: str, data_fim: str):
    """
    Executar crew otimizado com arquitetura ETL + anÃ¡lise baseada em CSV
    
    ğŸ¯ ARQUITETURA ETL:
    - APENAS o engenheiro_dados acessa o SQL Server diretamente
    - Engenheiro extrai dados e gera data/vendas.csv (arquivo padrÃ£o)
    - Outros agentes leem data/vendas.csv para anÃ¡lises especializadas
    - Diretor de insights consolida resultados finais
    
    ğŸ“Š BENEFÃCIOS DA ARQUITETURA:
    - âœ… SeparaÃ§Ã£o clara de responsabilidades
    - âœ… Acesso controlado ao banco de dados (seguranÃ§a)
    - âœ… Performance otimizada (apenas 1 extraÃ§Ã£o SQL)
    - âœ… AnÃ¡lises paralelas baseadas no mesmo dataset
    - âœ… ReduÃ§Ã£o de carga no banco de dados
    
    ğŸ”§ FLUXO DE DADOS:
    1. engenheiro_dados: SQL Server â†’ data/vendas.csv
    2. Agentes especializados: data/vendas.csv â†’ anÃ¡lises focadas
    3. diretor_insights: consolidaÃ§Ã£o â†’ dashboard executivo
    
    Args:
        data_inicio (str): Data inicial no formato 'YYYY-MM-DD'
        data_fim (str): Data final no formato 'YYYY-MM-DD'
        
    Returns:
        str: Resultados completos da anÃ¡lise com insights detalhados
    """
    
    start_time = time.time()
    
    try:
        optimized_logger.info("ğŸš€ Iniciando Insights-AI com Arquitetura ETL")
        optimized_logger.info("ğŸ—ï¸ SQL â†’ CSV â†’ AnÃ¡lises Especializadas â†’ Dashboard")
        
        # Criar instÃ¢ncia otimizada
        crew_instance = OptimizedInsights()
        
        # Preparar inputs
        inputs = {
            'data_inicio': data_inicio,
            'data_fim': data_fim
        }
        
        # Executar crew com arquitetura ETL
        optimized_logger.info("âš¡ Executando anÃ¡lise com separaÃ§Ã£o SQL/CSV...")
        
        max_retries = 3
        for attempt in range(max_retries + 1):
            try:
                result = crew_instance.crew().kickoff(inputs=inputs)
                break  # Sucesso, sair do loop
                
            except Exception as e:
                if "too large" in str(e) or "context" in str(e).lower() or "token" in str(e).lower():
                    if attempt < max_retries:
                        optimized_logger.warning(f"ğŸ”„ Tentativa {attempt + 1} - ajustando otimizaÃ§Ã£o...")
                        
                        # Aplicar otimizaÃ§Ã£o mais agressiva
                        IntelligentContextManager.MAX_CHARS_PER_INPUT = max(100000, IntelligentContextManager.MAX_CHARS_PER_INPUT * 0.7)
                        optimized_logger.info(f"ğŸ“‰ Limite reduzido para {IntelligentContextManager.MAX_CHARS_PER_INPUT} chars")
                        continue
                    else:
                        optimized_logger.error("âŒ Falha apÃ³s todas as tentativas de otimizaÃ§Ã£o")
                        raise
                else:
                    # Erro diferente, nÃ£o relacionado a contexto
                    raise
        
        # MÃ©tricas finais
        total_time = time.time() - start_time
        result_size = len(str(result))
        
        optimized_logger.info(f"âœ… AnÃ¡lise ETL concluÃ­da em {total_time:.2f}s")
        optimized_logger.info(f"ğŸ“Š Resultado gerado: {result_size:,} caracteres")
        optimized_logger.info("ğŸ—ï¸ Arquitetura ETL executada com sucesso:")
        optimized_logger.info("   ğŸ“¤ ExtraÃ§Ã£o SQL executada pelo engenheiro_dados")
        optimized_logger.info("   ğŸ“ Dados exportados para data/vendas.csv")
        optimized_logger.info("   ğŸ“Š AnÃ¡lises especializadas baseadas no CSV")
        optimized_logger.info("   ğŸ¯ Dashboard executivo consolidado")
        
        log_performance_summary()
        
        return result
        
    except Exception as e:
        execution_time = time.time() - start_time
        optimized_logger.error(f"âŒ Erro apÃ³s {execution_time:.2f}s: {e}")
        
        # Log especÃ­fico para problemas de contexto
        if "too large" in str(e) or "token" in str(e).lower():
            optimized_logger.error("ğŸ’¡ NOTA: Sistema configurado para arquitetura ETL")
            optimized_logger.error("ğŸ’¡ Apenas engenheiro_dados acessa SQL Server")
            optimized_logger.error("ğŸ’¡ Outros agentes usam data/vendas.csv exportado")
        
        raise
    
    finally:
        # Cleanup automÃ¡tico
        cleanup_performance_resources()

if __name__ == "__main__":
    # Exemplo de uso
    from datetime import datetime, timedelta
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30)
    
    result = run_optimized_crew(
        start_date.strftime('%Y-%m-%d'),
        end_date.strftime('%Y-%m-%d')
    ) 