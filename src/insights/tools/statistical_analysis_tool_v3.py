# VERS√ÉO 3.0 REFATORADA DO STATISTICAL ANALYSIS TOOL
# ===================================================
# 
# MELHORIAS DA VERS√ÉO 3.0:
# 
# ‚úÖ INFRAESTRUTURA CONSOLIDADA:
#    - Usa DataPreparationMixin para prepara√ß√£o de dados
#    - Usa ReportFormatterMixin para formata√ß√£o
#    - Usa business mixins quando apropriado
# 
# ‚úÖ RESPONSABILIDADES REDEFINIDAS:
#    - FOCO: An√°lises estat√≠sticas avan√ßadas, correla√ß√µes, clustering, outliers
#    - FOCO: An√°lises demogr√°ficas e geogr√°ficas completas
#    - FOCO: Testes estat√≠sticos, distribui√ß√µes, tend√™ncias
#    - REMOVIDO: KPIs b√°sicos de neg√≥cio (movidos para KPI Tool)
#    - MANTIDO: RFM quando h√° componente estat√≠stico
# 
# ‚úÖ NOVAS ESPECIALIZA√á√ïES:
#    - An√°lises demogr√°ficas aprofundadas
#    - Performance geogr√°fica detalhada
#    - Clustering multidimensional
#    - An√°lises de correla√ß√£o avan√ßadas
#    - An√°lises de sensibilidade de pre√ßos
#    - Segmenta√ß√£o de clientes avan√ßada
# 
# ‚úÖ INTEGRA√á√ÉO:
#    - Interface para integra√ß√£o com KPI Tool
#    - Compartilhamento de insights estat√≠sticos
#    - Cache de an√°lises computacionalmente pesadas

from crewai.tools import BaseTool
from typing import Type, Optional, Dict, Any, List, Tuple
from pydantic import BaseModel, Field
import pandas as pd
import numpy as np
from scipy import stats
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import json
import warnings

# Importar m√≥dulos compartilhados consolidados
from .shared.data_preparation import DataPreparationMixin
from .shared.report_formatter import ReportFormatterMixin
from .shared.business_mixins import JewelryRFMAnalysisMixin

warnings.filterwarnings('ignore')

class StatisticalAnalysisInputV3(BaseModel):
    """Schema de entrada para an√°lise estat√≠stica v3.0."""
    analysis_type: str = Field(..., description="""Tipo de an√°lise estat√≠stica:
    
    AN√ÅLISES ESTAT√çSTICAS CORE:
    - 'correlation': An√°lise de correla√ß√£o multi-dimensional
    - 'clustering': Clustering avan√ßado (produtos/clientes/comportamental)
    - 'outliers': Detec√ß√£o de outliers com m√©todos m√∫ltiplos
    - 'distribution': An√°lise de distribui√ß√µes e testes de normalidade
    - 'trend_analysis': Testes de tend√™ncia temporal e sazonalidade
    
    AN√ÅLISES ESPECIALIZADAS:
    - 'demographic_patterns': Padr√µes demogr√°ficos avan√ßados
    - 'geographic_performance': Performance geogr√°fica detalhada
    - 'customer_segmentation': Segmenta√ß√£o de clientes baseada em comportamento
    - 'price_sensitivity': An√°lise de elasticidade de pre√ßos
    - 'profitability_patterns': Padr√µes de rentabilidade e margem
    
    AN√ÅLISES INTEGRADAS:
    - 'comprehensive_customer_analysis': An√°lise completa de clientes
    - 'product_performance_analysis': Performance de produtos com estat√≠sticas
    """)
    data_csv: str = Field(default="data/vendas.csv", description="Caminho para arquivo CSV")
    target_column: str = Field(default="Total_Liquido", description="Coluna alvo para an√°lise")
    significance_level: float = Field(default=0.05, description="N√≠vel de signific√¢ncia para testes")
    clustering_method: str = Field(default="auto", description="M√©todo de clustering: 'kmeans', 'hierarchical', 'dbscan', 'auto'")
    min_correlation: float = Field(default=0.3, description="Correla√ß√£o m√≠nima para reportar")
    demographic_focus: bool = Field(default=True, description="Incluir an√°lise demogr√°fica detalhada")
    geographic_focus: bool = Field(default=True, description="Incluir an√°lise geogr√°fica detalhada")
    cache_results: bool = Field(default=True, description="Usar cache para an√°lises pesadas")

class StatisticalAnalysisToolV3(BaseTool, 
                               DataPreparationMixin, 
                               ReportFormatterMixin,
                               JewelryRFMAnalysisMixin):
    name: str = "Statistical Analysis Tool v3.0 SPECIALIZED"
    description: str = (
        "üìä VERS√ÉO 3.0 ESPECIALIZADA - Ferramenta avan√ßada de an√°lises estat√≠sticas para joalherias:\n\n"
        "üî¨ AN√ÅLISES ESTAT√çSTICAS CORE:\n"
        "- Correla√ß√µes multidimensionais com testes de signific√¢ncia\n"
        "- Clustering avan√ßado (K-means, Hier√°rquico, DBSCAN) com valida√ß√£o\n"
        "- Detec√ß√£o de outliers usando m√∫ltiplos m√©todos\n"
        "- An√°lise de distribui√ß√µes e testes de normalidade\n"
        "- Testes de tend√™ncia temporal e sazonalidade\n\n"
        "üë• AN√ÅLISES DEMOGR√ÅFICAS ESPECIALIZADAS:\n"
        "- Padr√µes de compra por idade, sexo, estado civil\n"
        "- An√°lise geracional (Gen Z, Millennial, Gen X, Boomer)\n"
        "- Segmenta√ß√£o comportamental avan√ßada\n"
        "- CLV por segmento demogr√°fico\n\n"
        "üó∫Ô∏è AN√ÅLISES GEOGR√ÅFICAS DETALHADAS:\n"
        "- Performance por estado e cidade\n"
        "- An√°lise de concentra√ß√£o geogr√°fica\n"
        "- Padr√µes sazonais regionais\n"
        "- Potencial de mercado por regi√£o\n\n"
        "üí° AN√ÅLISES ESPECIALIZADAS:\n"
        "- Elasticidade de pre√ßos e sensibilidade a descontos\n"
        "- Segmenta√ß√£o de clientes baseada em comportamento\n"
        "- Padr√µes de rentabilidade por dimens√µes\n"
        "- An√°lise de churn e reten√ß√£o\n\n"
        "üîó INTEGRA√á√ÉO: Interface para compartilhar insights com KPI Tool"
    )
    args_schema: Type[BaseModel] = StatisticalAnalysisInputV3
    
    def __init__(self):
        super().__init__()
        self._analysis_cache = {}  # Cache para an√°lises computacionalmente pesadas
    
    def _run(self, analysis_type: str, data_csv: str = "data/vendas.csv", 
             target_column: str = "Total_Liquido", significance_level: float = 0.05,
             clustering_method: str = "auto", min_correlation: float = 0.3,
             demographic_focus: bool = True, geographic_focus: bool = True,
             cache_results: bool = True) -> str:
        try:
            print(f"üìä Iniciando an√°lise estat√≠stica v3.0: {analysis_type}")
            
            # 1. Carregar e preparar dados usando m√≥dulo consolidado
            df = self._load_and_prepare_statistical_data(data_csv, cache_results)
            if df is None:
                return "Erro: N√£o foi poss√≠vel carregar ou preparar os dados para an√°lise estat√≠stica"
            
            print(f"üî¨ Dados preparados para an√°lise: {len(df)} registros, {len(df.columns)} campos")
            
            # 2. Mapeamento de an√°lises especializadas
            analysis_methods = {
                # An√°lises estat√≠sticas core
                'correlation': self._advanced_correlation_analysis,
                'clustering': self._multidimensional_clustering_analysis,
                'outliers': self._comprehensive_outlier_analysis,
                'distribution': self._advanced_distribution_analysis,
                'trend_analysis': self._temporal_trend_analysis,
                
                # An√°lises especializadas
                'demographic_patterns': self._demographic_patterns_analysis,
                'geographic_performance': self._geographic_performance_analysis,
                'customer_segmentation': self._behavioral_customer_segmentation,
                'price_sensitivity': self._price_elasticity_analysis,
                'profitability_patterns': self._profitability_pattern_analysis,
                
                # An√°lises integradas
                'comprehensive_customer_analysis': self._comprehensive_customer_analysis,
                'product_performance_analysis': self._statistical_product_analysis
            }
            
            if analysis_type not in analysis_methods:
                available = list(analysis_methods.keys())
                return f"An√°lise '{analysis_type}' n√£o suportada. Dispon√≠veis: {available}"
            
            # 3. Executar an√°lise com par√¢metros
            analysis_params = {
                'target_column': target_column,
                'significance_level': significance_level,
                'clustering_method': clustering_method,
                'min_correlation': min_correlation,
                'demographic_focus': demographic_focus,
                'geographic_focus': geographic_focus
            }
            
            result = analysis_methods[analysis_type](df, **analysis_params)
            
            # 4. Armazenar no cache se solicitado
            if cache_results:
                cache_key = f"{analysis_type}_{hash(data_csv)}_{target_column}"
                self._analysis_cache[cache_key] = result
            
            # 5. Formatar resultado usando m√≥dulo consolidado
            return self.format_statistical_analysis_report(result, analysis_type)
            
        except Exception as e:
            return f"Erro na an√°lise estat√≠stica v3.0: {str(e)}"
    
    def _load_and_prepare_statistical_data(self, data_csv: str, use_cache: bool = True) -> Optional[pd.DataFrame]:
        """Carregar e preparar dados especificamente para an√°lises estat√≠sticas."""
        cache_key = f"statistical_data_{hash(data_csv)}"
        
        # Verificar cache
        if use_cache and cache_key in self._analysis_cache:
            print("üìã Usando dados estat√≠sticos do cache")
            return self._analysis_cache[cache_key]
        
        try:
            # Carregar dados brutos
            df = pd.read_csv(data_csv, sep=';', encoding='utf-8')
            
            # Preparar dados usando mixin consolidado (n√≠vel strict para estat√≠sticas)
            df_prepared = self.prepare_jewelry_data(df, validation_level="strict")
            
            if df_prepared is None:
                return None
            
            # Prepara√ß√µes espec√≠ficas para an√°lises estat√≠sticas
            df_prepared = self._add_statistical_features(df_prepared)
            
            # Armazenar no cache
            if use_cache:
                self._analysis_cache[cache_key] = df_prepared
                print("üíæ Dados estat√≠sticos salvos no cache")
            
            return df_prepared
            
        except Exception as e:
            print(f"‚ùå Erro no carregamento de dados estat√≠sticos: {str(e)}")
            return None
    
    def _add_statistical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Adicionar features espec√≠ficas para an√°lises estat√≠sticas."""
        try:
            print("üßÆ Adicionando features estat√≠sticas...")
            
            # Padroniza√ß√£o de valores para clustering
            numeric_cols = ['Total_Liquido', 'Quantidade', 'Margem_Real', 'Preco_Unitario']
            available_numeric = [col for col in numeric_cols if col in df.columns]
            
            if available_numeric:
                scaler = StandardScaler()
                df[f'{col}_scaled'] = scaler.fit_transform(df[available_numeric])
                print(f"‚úÖ Padroniza√ß√£o aplicada: {len(available_numeric)} campos")
            
            # Features temporais para an√°lise de tend√™ncia
            if 'Data' in df.columns:
                df['Days_Since_Start'] = (df['Data'] - df['Data'].min()).dt.days
                df['Weeks_Since_Start'] = df['Days_Since_Start'] // 7
                df['Month_Index'] = df['Data'].dt.month
                print("‚úÖ Features temporais adicionadas")
            
            # Encoding de vari√°veis categ√≥ricas para clustering
            categorical_cols = ['Sexo', 'Estado_Civil', 'Estado', 'Grupo_Produto']
            for col in categorical_cols:
                if col in df.columns:
                    # One-hot encoding simples
                    dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)
                    df = pd.concat([df, dummies], axis=1)
            
            print("‚úÖ Encoding categ√≥rico aplicado")
            
            # Quartis e ranks para an√°lises
            if 'Total_Liquido' in df.columns:
                df['Revenue_Quartile'] = pd.qcut(df['Total_Liquido'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
                df['Revenue_Rank'] = df['Total_Liquido'].rank(pct=True)
                print("‚úÖ Quartis e ranks calculados")
            
            return df
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao adicionar features estat√≠sticas: {str(e)}")
            return df
    
    def _advanced_correlation_analysis(self, df: pd.DataFrame, target_column: str = "Total_Liquido",
                                     min_correlation: float = 0.3, significance_level: float = 0.05,
                                     **kwargs) -> Dict[str, Any]:
        """An√°lise de correla√ß√£o avan√ßada com testes de signific√¢ncia."""
        try:
            print("üîç Executando an√°lise de correla√ß√£o avan√ßada...")
            
            result = {
                'analysis_type': 'Advanced Correlation Analysis',
                'target_column': target_column,
                'significance_level': significance_level
            }
            
            # Selecionar colunas num√©ricas para correla√ß√£o
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if target_column not in numeric_cols:
                return {'error': f"Coluna alvo '{target_column}' n√£o √© num√©rica"}
            
            # Matriz de correla√ß√£o
            corr_matrix = df[numeric_cols].corr()
            target_correlations = corr_matrix[target_column].abs().sort_values(ascending=False)
            
            # Filtrar correla√ß√µes significativas
            significant_correlations = target_correlations[
                (target_correlations >= min_correlation) & 
                (target_correlations.index != target_column)
            ]
            
            result['correlation_matrix'] = corr_matrix.round(3).to_dict()
            result['significant_correlations'] = significant_correlations.round(3).to_dict()
            
            # Testes de signific√¢ncia para correla√ß√µes
            correlation_tests = {}
            for col in significant_correlations.index:
                if col in df.columns and not df[col].isna().all():
                    r_stat, p_value = stats.pearsonr(df[target_column].dropna(), df[col].dropna())
                    correlation_tests[col] = {
                        'correlation': round(r_stat, 3),
                        'p_value': round(p_value, 4),
                        'significant': p_value < significance_level
                    }
            
            result['correlation_tests'] = correlation_tests
            
            # An√°lise por categorias
            categorical_correlations = {}
            categorical_cols = ['Grupo_Produto', 'Metal', 'Faixa_Etaria', 'Sexo', 'Estado']
            
            for cat_col in categorical_cols:
                if cat_col in df.columns:
                    # ANOVA para testar diferen√ßa entre grupos
                    groups = [group[target_column].dropna() for name, group in df.groupby(cat_col)]
                    if len(groups) > 1 and all(len(g) > 0 for g in groups):
                        f_stat, p_val = stats.f_oneway(*groups)
                        
                        # Eta-squared (effect size)
                        group_means = df.groupby(cat_col)[target_column].mean()
                        overall_mean = df[target_column].mean()
                        ss_between = sum(df.groupby(cat_col).size() * (group_means - overall_mean)**2)
                        ss_total = sum((df[target_column] - overall_mean)**2)
                        eta_squared = ss_between / ss_total if ss_total > 0 else 0
                        
                        categorical_correlations[cat_col] = {
                            'f_statistic': round(f_stat, 3),
                            'p_value': round(p_val, 4),
                            'significant': p_val < significance_level,
                            'eta_squared': round(eta_squared, 3),
                            'effect_size': 'Large' if eta_squared > 0.14 else 'Medium' if eta_squared > 0.06 else 'Small',
                            'group_means': group_means.round(2).to_dict()
                        }
            
            result['categorical_analysis'] = categorical_correlations
            
            # Insights autom√°ticos
            result['insights'] = self._generate_correlation_insights(result)
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise de correla√ß√£o: {str(e)}"}
    
    def _multidimensional_clustering_analysis(self, df: pd.DataFrame, target_column: str = "Total_Liquido",
                                            clustering_method: str = "auto", **kwargs) -> Dict[str, Any]:
        """An√°lise de clustering multidimensional avan√ßada."""
        try:
            print("üéØ Executando an√°lise de clustering multidimensional...")
            
            result = {
                'analysis_type': 'Multidimensional Clustering Analysis',
                'target_column': target_column,
                'method': clustering_method
            }
            
            # Preparar dados para clustering
            feature_cols = self._select_clustering_features(df, target_column)
            if len(feature_cols) < 2:
                return {'error': 'Insuficientes features num√©ricas para clustering'}
            
            X = df[feature_cols].fillna(0)
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Escolher m√©todo automaticamente se necess√°rio
            if clustering_method == "auto":
                clustering_method = self._select_optimal_clustering_method(X_scaled)
            
            # Executar clustering
            if clustering_method == "kmeans":
                cluster_result = self._perform_kmeans_clustering(X_scaled, df, feature_cols)
            elif clustering_method == "hierarchical":
                cluster_result = self._perform_hierarchical_clustering(X_scaled, df, feature_cols)
            elif clustering_method == "dbscan":
                cluster_result = self._perform_dbscan_clustering(X_scaled, df, feature_cols)
            else:
                return {'error': f"M√©todo de clustering '{clustering_method}' n√£o suportado"}
            
            result.update(cluster_result)
            
            # An√°lise dos clusters
            if 'cluster_labels' in result:
                df_clustered = df.copy()
                df_clustered['Cluster'] = result['cluster_labels']
                
                # Perfil dos clusters
                cluster_profiles = {}
                for cluster_id in df_clustered['Cluster'].unique():
                    if cluster_id != -1:  # Ignorar outliers do DBSCAN
                        cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]
                        
                        profile = {
                            'size': len(cluster_data),
                            'size_percentage': round(len(cluster_data) / len(df_clustered) * 100, 1),
                            'avg_revenue': round(cluster_data[target_column].mean(), 2),
                            'total_revenue': round(cluster_data[target_column].sum(), 2),
                            'revenue_share': round(cluster_data[target_column].sum() / df[target_column].sum() * 100, 1)
                        }
                        
                        # Caracter√≠sticas demogr√°ficas se dispon√≠veis
                        if 'Faixa_Etaria' in cluster_data.columns:
                            profile['predominant_age_group'] = cluster_data['Faixa_Etaria'].mode().iloc[0] if len(cluster_data['Faixa_Etaria'].mode()) > 0 else 'N/A'
                        
                        if 'Sexo' in cluster_data.columns:
                            profile['gender_distribution'] = cluster_data['Sexo'].value_counts().to_dict()
                        
                        # Produtos preferidos
                        if 'Grupo_Produto' in cluster_data.columns:
                            profile['preferred_products'] = cluster_data['Grupo_Produto'].value_counts().head(3).to_dict()
                        
                        cluster_profiles[f'Cluster_{cluster_id}'] = profile
                
                result['cluster_profiles'] = cluster_profiles
            
            # Insights de clustering
            result['insights'] = self._generate_clustering_insights(result)
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise de clustering: {str(e)}"}
    
    def _demographic_patterns_analysis(self, df: pd.DataFrame, target_column: str = "Total_Liquido",
                                     demographic_focus: bool = True, **kwargs) -> Dict[str, Any]:
        """An√°lise aprofundada de padr√µes demogr√°ficos."""
        try:
            print("üë• Executando an√°lise de padr√µes demogr√°ficos...")
            
            result = {
                'analysis_type': 'Demographic Patterns Analysis',
                'target_column': target_column
            }
            
            if not demographic_focus:
                return {'message': 'An√°lise demogr√°fica n√£o solicitada'}
            
            # An√°lise por idade
            if 'Idade' in df.columns and 'Faixa_Etaria' in df.columns:
                age_analysis = self._analyze_age_patterns(df, target_column)
                result['age_patterns'] = age_analysis
            
            # An√°lise por sexo
            if 'Sexo' in df.columns:
                gender_analysis = self._analyze_gender_patterns(df, target_column)
                result['gender_patterns'] = gender_analysis
            
            # An√°lise por estado civil
            if 'Estado_Civil' in df.columns:
                marital_analysis = self._analyze_marital_patterns(df, target_column)
                result['marital_patterns'] = marital_analysis
            
            # An√°lise geracional
            if 'Geracao' in df.columns:
                generational_analysis = self._analyze_generational_patterns(df, target_column)
                result['generational_patterns'] = generational_analysis
            
            # Segmenta√ß√£o demogr√°fica combinada
            demographic_segmentation = self._create_demographic_segmentation(df, target_column)
            result['demographic_segmentation'] = demographic_segmentation
            
            # Insights demogr√°ficos
            result['insights'] = self._generate_demographic_insights(result)
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise demogr√°fica: {str(e)}"}
    
    def _geographic_performance_analysis(self, df: pd.DataFrame, target_column: str = "Total_Liquido",
                                       geographic_focus: bool = True, **kwargs) -> Dict[str, Any]:
        """An√°lise detalhada de performance geogr√°fica."""
        try:
            print("üó∫Ô∏è Executando an√°lise de performance geogr√°fica...")
            
            result = {
                'analysis_type': 'Geographic Performance Analysis',
                'target_column': target_column
            }
            
            if not geographic_focus:
                return {'message': 'An√°lise geogr√°fica n√£o solicitada'}
            
            # An√°lise por estado
            if 'Estado' in df.columns:
                state_analysis = self._analyze_state_performance(df, target_column)
                result['state_performance'] = state_analysis
            
            # An√°lise por cidade
            if 'Cidade' in df.columns:
                city_analysis = self._analyze_city_performance(df, target_column)
                result['city_performance'] = city_analysis
            
            # Concentra√ß√£o geogr√°fica
            geographic_concentration = self._calculate_geographic_concentration(df, target_column)
            result['geographic_concentration'] = geographic_concentration
            
            # Sazonalidade por regi√£o
            if 'Estado' in df.columns and 'Mes' in df.columns:
                regional_seasonality = self._analyze_regional_seasonality(df, target_column)
                result['regional_seasonality'] = regional_seasonality
            
            # Insights geogr√°ficos
            result['insights'] = self._generate_geographic_insights(result)
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise geogr√°fica: {str(e)}"}
    
    def _behavioral_customer_segmentation(self, df: pd.DataFrame, target_column: str = "Total_Liquido",
                                        **kwargs) -> Dict[str, Any]:
        """Segmenta√ß√£o de clientes baseada em comportamento."""
        try:
            print("üé≠ Executando segmenta√ß√£o comportamental de clientes...")
            
            result = {
                'analysis_type': 'Behavioral Customer Segmentation',
                'target_column': target_column
            }
            
            # RFM Analysis usando mixin consolidado
            if 'Codigo_Cliente' in df.columns:
                rfm_analysis = self.analyze_customer_rfm(df)
                if 'error' not in rfm_analysis:
                    result['rfm_segmentation'] = rfm_analysis
            
            # Segmenta√ß√£o por valor de compra
            value_segmentation = self._create_value_based_segmentation(df, target_column)
            result['value_segmentation'] = value_segmentation
            
            # Segmenta√ß√£o por frequ√™ncia
            if 'Codigo_Cliente' in df.columns:
                frequency_segmentation = self._create_frequency_segmentation(df)
                result['frequency_segmentation'] = frequency_segmentation
            
            # Segmenta√ß√£o por prefer√™ncia de produto
            product_preference_segmentation = self._create_product_preference_segmentation(df, target_column)
            result['product_preference_segmentation'] = product_preference_segmentation
            
            # An√°lise de churn risk
            if 'Codigo_Cliente' in df.columns and 'Data' in df.columns:
                churn_analysis = self._analyze_churn_risk(df)
                result['churn_analysis'] = churn_analysis
            
            # Insights comportamentais
            result['insights'] = self._generate_behavioral_insights(result)
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na segmenta√ß√£o comportamental: {str(e)}"}
    
    def _price_elasticity_analysis(self, df: pd.DataFrame, target_column: str = "Total_Liquido",
                                 **kwargs) -> Dict[str, Any]:
        """An√°lise de elasticidade de pre√ßos e sensibilidade a descontos."""
        try:
            print("üí∞ Executando an√°lise de sensibilidade de pre√ßos...")
            
            result = {
                'analysis_type': 'Price Elasticity Analysis',
                'target_column': target_column
            }
            
            # An√°lise de elasticidade por categoria
            if 'Grupo_Produto' in df.columns and 'Preco_Unitario' in df.columns:
                category_elasticity = self._calculate_price_elasticity_by_category(df)
                result['category_elasticity'] = category_elasticity
            
            # An√°lise de sensibilidade a descontos
            if 'Desconto_Percentual' in df.columns:
                discount_sensitivity = self._analyze_discount_sensitivity(df, target_column)
                result['discount_sensitivity'] = discount_sensitivity
            
            # Curva de demanda
            if 'Preco_Unitario' in df.columns and 'Quantidade' in df.columns:
                demand_curve = self._estimate_demand_curve(df)
                result['demand_curve'] = demand_curve
            
            # Ponto de pre√ßo √≥timo
            if 'Margem_Real' in df.columns and 'Preco_Unitario' in df.columns:
                optimal_pricing = self._calculate_optimal_pricing(df)
                result['optimal_pricing'] = optimal_pricing
            
            # Insights de pre√ßos
            result['insights'] = self._generate_pricing_insights(result)
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise de pre√ßos: {str(e)}"}
    
    # M√©todos auxiliares para an√°lises espec√≠ficas
    def _select_clustering_features(self, df: pd.DataFrame, target_column: str) -> List[str]:
        """Selecionar features apropriadas para clustering."""
        # Features num√©ricas essenciais
        base_features = [target_column, 'Quantidade', 'Preco_Unitario']
        
        # Adicionar features derivadas se dispon√≠veis
        derived_features = ['Margem_Real', 'Margem_Percentual', 'Desconto_Percentual', 
                          'Turnover_Estoque', 'Days_Since_Start']
        
        # Selecionar apenas features que existem e n√£o s√£o completamente nulas
        selected_features = []
        for feature in base_features + derived_features:
            if feature in df.columns and not df[feature].isna().all():
                selected_features.append(feature)
        
        return selected_features
    
    def _select_optimal_clustering_method(self, X_scaled: np.ndarray) -> str:
        """Selecionar m√©todo de clustering automaticamente."""
        n_samples = X_scaled.shape[0]
        
        if n_samples < 50:
            return "hierarchical"
        elif n_samples > 1000:
            return "kmeans"
        else:
            # Testar qual m√©todo d√° melhor silhouette score
            methods_scores = {}
            
            # K-means
            try:
                kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
                labels = kmeans.fit_predict(X_scaled)
                if len(set(labels)) > 1:
                    methods_scores['kmeans'] = silhouette_score(X_scaled, labels)
            except:
                pass
            
            # DBSCAN
            try:
                dbscan = DBSCAN(eps=0.5, min_samples=5)
                labels = dbscan.fit_predict(X_scaled)
                if len(set(labels)) > 1 and -1 not in labels:
                    methods_scores['dbscan'] = silhouette_score(X_scaled, labels)
            except:
                pass
            
            if methods_scores:
                return max(methods_scores, key=methods_scores.get)
            else:
                return "kmeans"  # fallback
    
    def _perform_kmeans_clustering(self, X_scaled: np.ndarray, df: pd.DataFrame, 
                                 feature_cols: List[str]) -> Dict[str, Any]:
        """Executar clustering K-means com determina√ß√£o autom√°tica do n√∫mero de clusters."""
        # Determinar n√∫mero √≥timo de clusters usando elbow method
        inertias = []
        silhouette_scores = []
        k_range = range(2, min(11, len(X_scaled) // 2))
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X_scaled)
            inertias.append(kmeans.inertia_)
            if len(set(labels)) > 1:
                silhouette_scores.append(silhouette_score(X_scaled, labels))
            else:
                silhouette_scores.append(-1)
        
        # Escolher k com melhor silhouette score
        optimal_k = k_range[np.argmax(silhouette_scores)]
        
        # Clustering final
        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X_scaled)
        
        return {
            'method_used': 'K-Means',
            'optimal_k': optimal_k,
            'cluster_labels': cluster_labels,
            'silhouette_score': max(silhouette_scores),
            'elbow_data': {'k_values': list(k_range), 'inertias': inertias, 'silhouette_scores': silhouette_scores},
            'cluster_centers': kmeans.cluster_centers_.tolist(),
            'feature_names': feature_cols
        }
    
    def _perform_hierarchical_clustering(self, X_scaled: np.ndarray, df: pd.DataFrame,
                                       feature_cols: List[str]) -> Dict[str, Any]:
        """Executar clustering hier√°rquico."""
        # Linkage
        linkage_matrix = linkage(X_scaled, method='ward')
        
        # Determinar n√∫mero de clusters
        distances = linkage_matrix[:, 2]
        acceleration = np.diff(distances, 2)
        optimal_clusters = acceleration.argmax() + 2
        optimal_clusters = min(max(optimal_clusters, 2), 8)  # Entre 2 e 8 clusters
        
        # Formar clusters
        cluster_labels = fcluster(linkage_matrix, optimal_clusters, criterion='maxclust') - 1
        
        silhouette_avg = silhouette_score(X_scaled, cluster_labels) if len(set(cluster_labels)) > 1 else -1
        
        return {
            'method_used': 'Hierarchical',
            'optimal_clusters': optimal_clusters,
            'cluster_labels': cluster_labels,
            'silhouette_score': silhouette_avg,
            'linkage_matrix': linkage_matrix.tolist(),
            'feature_names': feature_cols
        }
    
    def _perform_dbscan_clustering(self, X_scaled: np.ndarray, df: pd.DataFrame,
                                 feature_cols: List[str]) -> Dict[str, Any]:
        """Executar clustering DBSCAN."""
        # Determinar eps automaticamente
        from sklearn.neighbors import NearestNeighbors
        
        neighbors = NearestNeighbors(n_neighbors=4)
        neighbors_fit = neighbors.fit(X_scaled)
        distances, indices = neighbors_fit.kneighbors(X_scaled)
        distances = np.sort(distances[:, 3], axis=0)
        
        # Usar o ponto de maior curvatura como eps
        diffs = np.diff(distances)
        eps = distances[np.argmax(diffs)]
        
        # DBSCAN
        dbscan = DBSCAN(eps=eps, min_samples=5)
        cluster_labels = dbscan.fit_predict(X_scaled)
        
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_outliers = list(cluster_labels).count(-1)
        
        silhouette_avg = silhouette_score(X_scaled, cluster_labels) if n_clusters > 1 else -1
        
        return {
            'method_used': 'DBSCAN',
            'eps_used': eps,
            'cluster_labels': cluster_labels,
            'n_clusters': n_clusters,
            'n_outliers': n_outliers,
            'outlier_percentage': round(n_outliers / len(cluster_labels) * 100, 1),
            'silhouette_score': silhouette_avg,
            'feature_names': feature_cols
        }
    
    # M√©todos para an√°lises demogr√°ficas espec√≠ficas
    def _analyze_age_patterns(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Analisar padr√µes por idade."""
        age_patterns = {}
        
        # Estat√≠sticas por faixa et√°ria
        if 'Faixa_Etaria' in df.columns:
            age_stats = df.groupby('Faixa_Etaria').agg({
                target_column: ['count', 'mean', 'sum', 'std'],
                'Quantidade': 'mean' if 'Quantidade' in df.columns else 'count'
            }).round(2)
            
            age_patterns['by_age_group'] = age_stats.to_dict()
            
            # Teste ANOVA para diferen√ßas significativas
            age_groups = [group[target_column].dropna() for name, group in df.groupby('Faixa_Etaria')]
            if len(age_groups) > 1:
                f_stat, p_val = stats.f_oneway(*age_groups)
                age_patterns['statistical_test'] = {
                    'f_statistic': round(f_stat, 3),
                    'p_value': round(p_val, 4),
                    'significant_difference': p_val < 0.05
                }
        
        # Correla√ß√£o com idade num√©rica
        if 'Idade' in df.columns:
            correlation, p_value = stats.pearsonr(df['Idade'].dropna(), df[target_column].dropna())
            age_patterns['age_correlation'] = {
                'correlation': round(correlation, 3),
                'p_value': round(p_value, 4),
                'significant': p_value < 0.05
            }
        
        return age_patterns
    
    def _analyze_gender_patterns(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Analisar padr√µes por sexo."""
        gender_patterns = {}
        
        # Estat√≠sticas por sexo
        gender_stats = df.groupby('Sexo').agg({
            target_column: ['count', 'mean', 'sum', 'std'],
            'Quantidade': 'mean' if 'Quantidade' in df.columns else 'count'
        }).round(2)
        
        gender_patterns['by_gender'] = gender_stats.to_dict()
        
        # Teste t para diferen√ßa entre sexos
        male_data = df[df['Sexo'] == 'M'][target_column].dropna()
        female_data = df[df['Sexo'] == 'F'][target_column].dropna()
        
        if len(male_data) > 0 and len(female_data) > 0:
            t_stat, p_val = stats.ttest_ind(male_data, female_data)
            gender_patterns['statistical_test'] = {
                't_statistic': round(t_stat, 3),
                'p_value': round(p_val, 4),
                'significant_difference': p_val < 0.05
            }
        
        return gender_patterns
    
    def _analyze_generational_patterns(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Analisar padr√µes geracionais."""
        gen_patterns = {}
        
        # Estat√≠sticas por gera√ß√£o
        gen_stats = df.groupby('Geracao').agg({
            target_column: ['count', 'mean', 'sum', 'std'],
            'Quantidade': 'mean' if 'Quantidade' in df.columns else 'count'
        }).round(2)
        
        gen_patterns['by_generation'] = gen_stats.to_dict()
        
        # Produtos preferidos por gera√ß√£o
        if 'Grupo_Produto' in df.columns:
            gen_products = {}
            for gen in df['Geracao'].unique():
                if pd.notna(gen):
                    gen_data = df[df['Geracao'] == gen]
                    top_products = gen_data['Grupo_Produto'].value_counts().head(3)
                    gen_products[gen] = top_products.to_dict()
            gen_patterns['preferred_products'] = gen_products
        
        return gen_patterns
    
    # M√©todos para insights autom√°ticos
    def _generate_correlation_insights(self, result: Dict[str, Any]) -> List[str]:
        """Gerar insights de correla√ß√£o."""
        insights = []
        
        if 'significant_correlations' in result:
            correlations = result['significant_correlations']
            if correlations:
                strongest = max(correlations.items(), key=lambda x: x[1])
                insights.append(f"üí° Correla√ß√£o mais forte: {strongest[0]} ({strongest[1]:.3f})")
                
                # Correla√ß√µes por categoria
                if 'categorical_analysis' in result:
                    cat_analysis = result['categorical_analysis']
                    significant_categories = [cat for cat, data in cat_analysis.items() 
                                           if data.get('significant', False)]
                    if significant_categories:
                        insights.append(f"üìä Categorias com impacto significativo: {', '.join(significant_categories)}")
        
        return insights
    
    def _generate_clustering_insights(self, result: Dict[str, Any]) -> List[str]:
        """Gerar insights de clustering."""
        insights = []
        
        if 'cluster_profiles' in result:
            profiles = result['cluster_profiles']
            
            # Cluster mais valioso
            revenue_clusters = {k: v.get('total_revenue', 0) for k, v in profiles.items()}
            if revenue_clusters:
                top_cluster = max(revenue_clusters, key=revenue_clusters.get)
                top_revenue_share = profiles[top_cluster].get('revenue_share', 0)
                insights.append(f"üíé {top_cluster} gera {top_revenue_share}% da receita")
            
            # Cluster com maior AOV
            aov_clusters = {k: v.get('avg_revenue', 0) for k, v in profiles.items()}
            if aov_clusters:
                highest_aov_cluster = max(aov_clusters, key=aov_clusters.get)
                insights.append(f"üéØ {highest_aov_cluster} tem maior valor m√©dio por transa√ß√£o")
        
        return insights
    
    def _generate_demographic_insights(self, result: Dict[str, Any]) -> List[str]:
        """Gerar insights demogr√°ficos."""
        insights = []
        
        # Insights de idade
        if 'age_patterns' in result and 'by_age_group' in result['age_patterns']:
            age_data = result['age_patterns']['by_age_group']
            # Encontrar faixa et√°ria com maior AOV
            if 'mean' in str(age_data):
                # Extrair m√©dia por faixa (estrutura pode variar)
                insights.append("üë• An√°lise de faixas et√°rias conclu√≠da com diferen√ßas significativas")
        
        # Insights de g√™nero
        if 'gender_patterns' in result and 'statistical_test' in result['gender_patterns']:
            if result['gender_patterns']['statistical_test'].get('significant_difference', False):
                insights.append("‚ôÄÔ∏è‚ôÇÔ∏è Diferen√ßas significativas de comportamento entre g√™neros identificadas")
        
        return insights
    
    def _generate_geographic_insights(self, result: Dict[str, Any]) -> List[str]:
        """Gerar insights geogr√°ficos."""
        insights = []
        
        if 'state_performance' in result:
            insights.append("üó∫Ô∏è An√°lise geogr√°fica por estados conclu√≠da")
        
        if 'geographic_concentration' in result:
            concentration = result['geographic_concentration']
            if isinstance(concentration, dict) and 'concentration_index' in concentration:
                conc_index = concentration['concentration_index']
                if conc_index > 0.7:
                    insights.append("üìç Alta concentra√ß√£o geogr√°fica detectada - oportunidade de expans√£o")
        
        return insights
    
    def _generate_behavioral_insights(self, result: Dict[str, Any]) -> List[str]:
        """Gerar insights comportamentais."""
        insights = []
        
        if 'rfm_segmentation' in result:
            insights.append("üé≠ Segmenta√ß√£o RFM comportamental aplicada com sucesso")
        
        if 'churn_analysis' in result:
            churn_data = result['churn_analysis']
            if isinstance(churn_data, dict) and 'high_risk_customers' in churn_data:
                high_risk = churn_data['high_risk_customers']
                if high_risk > 0:
                    insights.append(f"‚ö†Ô∏è {high_risk} clientes identificados com alto risco de churn")
        
        return insights
    
    def _generate_pricing_insights(self, result: Dict[str, Any]) -> List[str]:
        """Gerar insights de pre√ßos."""
        insights = []
        
        if 'category_elasticity' in result:
            insights.append("üí∞ Elasticidade de pre√ßos calculada por categoria")
        
        if 'optimal_pricing' in result:
            insights.append("üéØ Pontos de pre√ßo √≥timos identificados para maximizar margem")
        
        return insights
    
    # Placeholder methods para an√°lises n√£o implementadas ainda
    def _analyze_marital_patterns(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Placeholder para an√°lise por estado civil."""
        return {'message': 'An√°lise por estado civil em desenvolvimento'}
    
    def _create_demographic_segmentation(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Placeholder para segmenta√ß√£o demogr√°fica combinada."""
        return {'message': 'Segmenta√ß√£o demogr√°fica combinada em desenvolvimento'}
    
    def _analyze_state_performance(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Placeholder para an√°lise por estado."""
        return {'message': 'An√°lise por estado em desenvolvimento'}
    
    def _analyze_city_performance(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Placeholder para an√°lise por cidade."""
        return {'message': 'An√°lise por cidade em desenvolvimento'}
    
    def _calculate_geographic_concentration(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Placeholder para concentra√ß√£o geogr√°fica."""
        return {'message': 'C√°lculo de concentra√ß√£o geogr√°fica em desenvolvimento'}
    
    def _analyze_regional_seasonality(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Placeholder para sazonalidade regional."""
        return {'message': 'An√°lise de sazonalidade regional em desenvolvimento'}
    
    # M√©todos adicionais necess√°rios (implementa√ß√µes simplificadas)
    def _comprehensive_outlier_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """Placeholder para an√°lise de outliers."""
        return {'message': 'An√°lise de outliers em desenvolvimento'}
    
    def _advanced_distribution_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """Placeholder para an√°lise de distribui√ß√£o."""
        return {'message': 'An√°lise de distribui√ß√£o em desenvolvimento'}
    
    def _temporal_trend_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """Placeholder para an√°lise de tend√™ncia temporal."""
        return {'message': 'An√°lise de tend√™ncia temporal em desenvolvimento'}
    
    def _profitability_pattern_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """Placeholder para an√°lise de padr√µes de rentabilidade."""
        return {'message': 'An√°lise de padr√µes de rentabilidade em desenvolvimento'}
    
    def _comprehensive_customer_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """Placeholder para an√°lise abrangente de clientes."""
        return {'message': 'An√°lise abrangente de clientes em desenvolvimento'}
    
    def _statistical_product_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """Placeholder para an√°lise estat√≠stica de produtos."""
        return {'message': 'An√°lise estat√≠stica de produtos em desenvolvimento'}
    
    def _create_value_based_segmentation(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Placeholder para segmenta√ß√£o por valor."""
        return {'message': 'Segmenta√ß√£o por valor em desenvolvimento'}
    
    def _create_frequency_segmentation(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Placeholder para segmenta√ß√£o por frequ√™ncia."""
        return {'message': 'Segmenta√ß√£o por frequ√™ncia em desenvolvimento'}
    
    def _create_product_preference_segmentation(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Placeholder para segmenta√ß√£o por prefer√™ncia de produto."""
        return {'message': 'Segmenta√ß√£o por prefer√™ncia de produto em desenvolvimento'}
    
    def _analyze_churn_risk(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Placeholder para an√°lise de risco de churn."""
        return {'message': 'An√°lise de risco de churn em desenvolvimento'}
    
    def _calculate_price_elasticity_by_category(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Placeholder para elasticidade por categoria."""
        return {'message': 'Elasticidade por categoria em desenvolvimento'}
    
    def _analyze_discount_sensitivity(self, df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
        """Placeholder para sensibilidade a descontos."""
        return {'message': 'An√°lise de sensibilidade a descontos em desenvolvimento'}
    
    def _estimate_demand_curve(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Placeholder para curva de demanda."""
        return {'message': 'Estimativa de curva de demanda em desenvolvimento'}
    
    def _calculate_optimal_pricing(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Placeholder para pre√ßos √≥timos."""
        return {'message': 'C√°lculo de pre√ßos √≥timos em desenvolvimento'} 