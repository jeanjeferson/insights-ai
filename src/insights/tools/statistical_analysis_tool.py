from crewai.tools import BaseTool
from typing import Type, Optional, Dict, Any, List, Tuple
from pydantic import BaseModel, Field, validator
import pandas as pd
import numpy as np
from scipy import stats
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import json
import warnings
from datetime import datetime

# Importar m√≥dulos compartilhados consolidados
from .shared.data_preparation import DataPreparationMixin
from .shared.report_formatter import ReportFormatterMixin
from .shared.business_mixins import JewelryRFMAnalysisMixin

warnings.filterwarnings('ignore')

class StatisticalAnalysisToolInput(BaseModel):
    """Schema otimizado para an√°lise estat√≠stica com valida√ß√µes robustas."""
    
    analysis_type: str = Field(
        ..., 
        description="""Tipo de an√°lise estat√≠stica especializada:
        
        üî¨ AN√ÅLISES ESTAT√çSTICAS CORE:
        - 'correlation': An√°lise de correla√ß√£o multi-dimensional com testes de signific√¢ncia
        - 'clustering': Clustering avan√ßado (K-means, Hier√°rquico, DBSCAN) com valida√ß√£o
        - 'outliers': Detec√ß√£o de outliers usando m√∫ltiplos m√©todos estat√≠sticos
        - 'distribution': An√°lise de distribui√ß√µes e testes de normalidade
        - 'trend_analysis': Testes de tend√™ncia temporal e sazonalidade
        
        üë• AN√ÅLISES DEMOGR√ÅFICAS ESPECIALIZADAS:
        - 'demographic_patterns': Padr√µes demogr√°ficos avan√ßados (idade, sexo, estado civil)
        - 'generational_analysis': An√°lise geracional (Gen Z, Millennial, Gen X, Boomer)
        - 'customer_segmentation': Segmenta√ß√£o comportamental avan√ßada
        
        üó∫Ô∏è AN√ÅLISES GEOGR√ÅFICAS DETALHADAS:
        - 'geographic_performance': Performance por estado e cidade com estat√≠sticas
        - 'regional_patterns': Padr√µes sazonais e comportamentais regionais
        
        üí∞ AN√ÅLISES ESPECIALIZADAS:
        - 'price_sensitivity': An√°lise de elasticidade de pre√ßos e sensibilidade
        - 'profitability_patterns': Padr√µes de rentabilidade com an√°lise estat√≠stica
        
        üîó AN√ÅLISES INTEGRADAS:
        - 'comprehensive_customer_analysis': An√°lise completa de clientes com estat√≠sticas
        - 'product_performance_analysis': Performance de produtos com testes estat√≠sticos
        """,
        example="correlation"
    )
    
    data_csv: str = Field(
        default="data/vendas.csv", 
        description="Caminho para arquivo CSV com dados de vendas. Use 'data/vendas.csv' para dados principais.",
        example="data/vendas.csv"
    )
    
    target_column: str = Field(
        default="Total_Liquido", 
        description="Coluna alvo para an√°lise. Use 'Total_Liquido' para receita, 'Quantidade' para volume.",
        example="Total_Liquido"
    )
    
    significance_level: float = Field(
        default=0.05, 
        description="N√≠vel de signific√¢ncia para testes estat√≠sticos (0.01-0.10). Padr√£o: 0.05 (95% confian√ßa).",
        ge=0.01,
        le=0.10
    )
    
    clustering_method: str = Field(
        default="auto", 
        description="M√©todo de clustering: 'kmeans' (r√°pido), 'hierarchical' (interpret√°vel), 'dbscan' (outliers), 'auto' (otimizado).",
        pattern="^(kmeans|hierarchical|dbscan|auto)$"
    )
    
    min_correlation: float = Field(
        default=0.3, 
        description="Correla√ß√£o m√≠nima para reportar (0.1-0.9). Valores altos = apenas correla√ß√µes fortes.",
        ge=0.1,
        le=0.9
    )
    
    demographic_focus: bool = Field(
        default=True, 
        description="Incluir an√°lise demogr√°fica detalhada. Recomendado: True para insights de cliente."
    )
    
    geographic_focus: bool = Field(
        default=True, 
        description="Incluir an√°lise geogr√°fica detalhada. Recomendado: True para expans√£o regional."
    )
    
    cache_results: bool = Field(
        default=True, 
        description="Usar cache para an√°lises pesadas. Recomendado: True para datasets grandes."
    )
    
    sample_size: Optional[int] = Field(
        default=None,
        description="Tamanho da amostra para an√°lises pesadas (1000-100000). None = usar todos os dados.",
        ge=1000,
        le=100000
    )
    
    @validator('analysis_type')
    def validate_analysis_type(cls, v):
        valid_types = [
            'correlation', 'clustering', 'outliers', 'distribution', 'trend_analysis',
            'demographic_patterns', 'generational_analysis', 'customer_segmentation',
            'geographic_performance', 'regional_patterns', 'price_sensitivity',
            'profitability_patterns', 'comprehensive_customer_analysis', 'product_performance_analysis'
        ]
        if v not in valid_types:
            raise ValueError(f"analysis_type deve ser um de: {valid_types}")
        return v

class StatisticalAnalysisTool(BaseTool, 
                               DataPreparationMixin, 
                               ReportFormatterMixin,
                               JewelryRFMAnalysisMixin):
    """
    üî¨ MOTOR DE AN√ÅLISES ESTAT√çSTICAS AVAN√áADAS PARA JOALHERIAS
    
    QUANDO USAR:
    - Descobrir padr√µes ocultos nos dados de vendas
    - Realizar an√°lises estat√≠sticas rigorosas com testes de signific√¢ncia
    - Segmentar clientes baseado em comportamento estat√≠stico
    - Analisar correla√ß√µes complexas entre vari√°veis
    - Detectar outliers e anomalias estat√≠sticas
    - Realizar clustering avan√ßado de produtos/clientes
    
    CASOS DE USO ESPEC√çFICOS:
    - analysis_type='correlation': Descobrir quais fatores influenciam vendas
    - analysis_type='clustering': Segmentar clientes por padr√µes de compra
    - analysis_type='demographic_patterns': Analisar comportamento por idade/sexo
    - analysis_type='geographic_performance': Comparar performance entre regi√µes
    - analysis_type='price_sensitivity': Medir elasticidade de pre√ßos
    - analysis_type='outliers': Identificar vendas an√¥malas para investiga√ß√£o
    
    RESULTADOS ENTREGUES:
    - An√°lises estat√≠sticas rigorosas com testes de signific√¢ncia
    - Segmenta√ß√µes autom√°ticas baseadas em dados
    - Correla√ß√µes significativas com interpreta√ß√£o de neg√≥cio
    - Clusters de clientes/produtos com perfis detalhados
    - Insights demogr√°ficos e geogr√°ficos aprofundados
    - Recomenda√ß√µes baseadas em evid√™ncias estat√≠sticas
    """
    
    name: str = "Statistical Analysis Tool"
    description: str = (
        "Motor de an√°lises estat√≠sticas avan√ßadas para descobrir padr√µes ocultos em dados de joalherias. "
        "Realiza an√°lises rigorosas com testes de signific√¢ncia, clustering, correla√ß√µes e segmenta√ß√µes. "
        "Ideal para insights profundos sobre comportamento de clientes, performance de produtos e padr√µes de mercado."
    )
    args_schema: Type[BaseModel] = StatisticalAnalysisToolInput
    
    def __init__(self):
        super().__init__()
        self._analysis_cache = {}  # Cache para an√°lises computacionalmente pesadas
    
    def _run(self, analysis_type: str, data_csv: str = "data/vendas.csv", 
             target_column: str = "Total_Liquido", significance_level: float = 0.05,
             clustering_method: str = "auto", min_correlation: float = 0.3,
             demographic_focus: bool = True, geographic_focus: bool = True,
             cache_results: bool = True, sample_size: Optional[int] = None) -> str:
        try:
            print(f"üî¨ Iniciando an√°lise estat√≠stica v3.0: {analysis_type}")
            print(f"‚öôÔ∏è Configura√ß√µes: signific√¢ncia={significance_level}, correla√ß√£o_min={min_correlation}")
            
            # 1. Carregar e preparar dados usando m√≥dulo consolidado
            df = self._load_and_prepare_statistical_data(data_csv, cache_results, sample_size)
            if df is None:
                return json.dumps({
                    "error": "N√£o foi poss√≠vel carregar ou preparar os dados para an√°lise estat√≠stica",
                    "troubleshooting": {
                        "check_file_exists": f"Verifique se {data_csv} existe",
                        "check_data_quality": "Confirme que os dados t√™m qualidade suficiente para an√°lise",
                        "check_sample_size": "Verifique se h√° dados suficientes para an√°lise estat√≠stica"
                    },
                    "metadata": {
                        "tool": "Statistical Analysis",
                        "status": "error",
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                }, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ Dados preparados: {len(df)} registros, {len(df.columns)} campos")
            
            # 2. Mapeamento de an√°lises especializadas
            analysis_methods = {
                # An√°lises estat√≠sticas core
                'correlation': self._advanced_correlation_analysis,
                'clustering': self._multidimensional_clustering_analysis,
                'outliers': self._comprehensive_outlier_analysis,
                'distribution': self._advanced_distribution_analysis,
                'trend_analysis': self._temporal_trend_analysis,
                
                # An√°lises especializadas
                'demographic_patterns': self._demographic_patterns_analysis,
                'generational_analysis': self._generational_analysis,
                'customer_segmentation': self._behavioral_customer_segmentation,
                'geographic_performance': self._geographic_performance_analysis,
                'regional_patterns': self._regional_patterns_analysis,
                'price_sensitivity': self._price_elasticity_analysis,
                'profitability_patterns': self._profitability_pattern_analysis,
                
                # An√°lises integradas
                'comprehensive_customer_analysis': self._comprehensive_customer_analysis,
                'product_performance_analysis': self._statistical_product_analysis
            }
            
            if analysis_type not in analysis_methods:
                available = list(analysis_methods.keys())
                return json.dumps({
                    "error": f"An√°lise '{analysis_type}' n√£o suportada",
                    "available_analyses": available,
                    "suggestion": "Use uma das an√°lises dispon√≠veis listadas acima",
                    "metadata": {
                        "tool": "Statistical Analysis",
                        "status": "error",
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                }, ensure_ascii=False, indent=2)
            
            # 3. Executar an√°lise com par√¢metros
            analysis_params = {
                'target_column': target_column,
                'significance_level': significance_level,
                'clustering_method': clustering_method,
                'min_correlation': min_correlation,
                'demographic_focus': demographic_focus,
                'geographic_focus': geographic_focus
            }
            
            print(f"üéØ Executando an√°lise: {analysis_type}")
            result = analysis_methods[analysis_type](df, **analysis_params)
            
            # 4. Adicionar metadados
            result['metadata'] = {
                'tool': 'Statistical Analysis Tool v3.0',
                'analysis_type': analysis_type,
                'target_column': target_column,
                'total_records': len(df),
                'significance_level': significance_level,
                'date_range': {
                    'start': df['Data'].min().strftime("%Y-%m-%d") if 'Data' in df.columns else "N/A",
                    'end': df['Data'].max().strftime("%Y-%m-%d") if 'Data' in df.columns else "N/A"
                },
                'generated_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # 5. Armazenar no cache se solicitado
            if cache_results:
                cache_key = f"stat_{analysis_type}_{hash(data_csv)}_{target_column}"
                self._analysis_cache[cache_key] = result
                print(f"üíæ Resultado salvo no cache")
            
            # 6. Formatar resultado final
            print("‚úÖ An√°lise estat√≠stica conclu√≠da com sucesso!")
            return json.dumps(result, ensure_ascii=False, indent=2, default=str)
            
        except Exception as e:
            error_response = {
                "error": f"Erro na an√°lise estat√≠stica v3.0: {str(e)}",
                "analysis_type": analysis_type,
                "data_csv": data_csv,
                "troubleshooting": {
                    "check_data_format": "Verifique se os dados est√£o no formato correto",
                    "check_statistical_requirements": "Confirme que h√° dados suficientes para an√°lise estat√≠stica",
                    "try_simpler_analysis": "Tente uma an√°lise mais simples primeiro"
                },
                "metadata": {
                    "tool": "Statistical Analysis",
                    "status": "error",
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
            }
            return json.dumps(error_response, ensure_ascii=False, indent=2)
    
    def _load_and_prepare_statistical_data(self, data_csv: str, use_cache: bool = True, sample_size: Optional[int] = None) -> Optional[pd.DataFrame]:
        """Carregar e preparar dados especificamente para an√°lises estat√≠sticas."""
        cache_key = f"statistical_data_{hash(data_csv)}_{sample_size}"
        
        # Verificar cache
        if use_cache and cache_key in self._analysis_cache:
            print("üìã Usando dados estat√≠sticos do cache")
            return self._analysis_cache[cache_key]
        
        try:
            # Carregar dados brutos
            df = pd.read_csv(data_csv, sep=';', encoding='utf-8')
            print(f"üìÅ Arquivo carregado: {len(df)} registros")
            
            # Aplicar amostragem se necess√°rio
            if sample_size and len(df) > sample_size:
                df = df.sample(n=sample_size, random_state=42)
                print(f"üìä Amostra aplicada: {len(df)} registros")
            
            # Preparar dados usando mixin consolidado (n√≠vel strict para estat√≠sticas)
            df_prepared = self.prepare_jewelry_data(df, validation_level="strict")
            
            if df_prepared is None:
                print("‚ùå Falha na prepara√ß√£o dos dados estat√≠sticos")
                return None
            
            # Prepara√ß√µes espec√≠ficas para an√°lises estat√≠sticas
            df_prepared = self._add_statistical_features(df_prepared)
            
            # Armazenar no cache
            if use_cache:
                self._analysis_cache[cache_key] = df_prepared
                print("üíæ Dados estat√≠sticos salvos no cache")
            
            return df_prepared
            
        except Exception as e:
            print(f"‚ùå Erro no carregamento de dados estat√≠sticos: {str(e)}")
            return None
    
    def _add_statistical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Adicionar features espec√≠ficas para an√°lises estat√≠sticas."""
        try:
            print("üßÆ Adicionando features estat√≠sticas...")
            
            # FASE 1: CORRE√á√ÉO DO BUG - Padroniza√ß√£o de valores para clustering
            numeric_cols = ['Total_Liquido', 'Quantidade', 'Margem_Real', 'Preco_Unitario']
            available_numeric = [col for col in numeric_cols if col in df.columns]
            
            if available_numeric:
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(df[available_numeric])
                
                # CORRE√á√ÉO: Criar colunas escaladas corretamente
                for i, col in enumerate(available_numeric):
                    df[f'{col}_scaled'] = scaled_data[:, i]
                
                print(f"‚úÖ Padroniza√ß√£o aplicada: {len(available_numeric)} campos escalados")
            
            # Features temporais para an√°lise de tend√™ncia
            if 'Data' in df.columns:
                df['Days_Since_Start'] = (df['Data'] - df['Data'].min()).dt.days
                df['Weeks_Since_Start'] = df['Days_Since_Start'] // 7
                df['Month_Index'] = df['Data'].dt.month
                print("‚úÖ Features temporais adicionadas")
            
            # Encoding de vari√°veis categ√≥ricas para clustering
            categorical_cols = ['Sexo', 'Estado_Civil', 'Estado', 'Grupo_Produto']
            for col in categorical_cols:
                if col in df.columns:
                    # One-hot encoding simples
                    dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)
                    df = pd.concat([df, dummies], axis=1)
            
            print("‚úÖ Encoding categ√≥rico aplicado")
            
            # Quartis e ranks para an√°lises
            if 'Total_Liquido' in df.columns:
                df['Revenue_Quartile'] = pd.qcut(df['Total_Liquido'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
                df['Revenue_Rank'] = df['Total_Liquido'].rank(pct=True)
                print("‚úÖ Quartis e ranks calculados")
            
            return df
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao adicionar features estat√≠sticas: {str(e)}")
            return df
    
    def _advanced_correlation_analysis(self, df: pd.DataFrame, target_column: str = "Total_Liquido",
                                     min_correlation: float = 0.3, significance_level: float = 0.05,
                                     **kwargs) -> Dict[str, Any]:
        """An√°lise de correla√ß√£o avan√ßada com testes de signific√¢ncia."""
        try:
            print("üîç Executando an√°lise de correla√ß√£o avan√ßada...")
            
            result = {
                'analysis_type': 'Advanced Correlation Analysis',
                'target_column': target_column,
                'significance_level': significance_level,
                'min_correlation_threshold': min_correlation
            }
            
            # Selecionar colunas num√©ricas para correla√ß√£o
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if target_column not in numeric_cols:
                return {'error': f"Coluna alvo '{target_column}' n√£o √© num√©rica"}
            
            # Matriz de correla√ß√£o
            corr_matrix = df[numeric_cols].corr()
            target_correlations = corr_matrix[target_column].abs().sort_values(ascending=False)
            
            # Filtrar correla√ß√µes significativas
            significant_correlations = target_correlations[
                (target_correlations >= min_correlation) & 
                (target_correlations.index != target_column)
            ]
            
            result['correlation_matrix'] = corr_matrix.round(3).to_dict()
            result['significant_correlations'] = significant_correlations.round(3).to_dict()
            
            # Testes de signific√¢ncia para correla√ß√µes
            correlation_tests = {}
            for col in significant_correlations.index:
                if col in df.columns and not df[col].isna().all():
                    r_stat, p_value = stats.pearsonr(df[target_column].dropna(), df[col].dropna())
                    correlation_tests[col] = {
                        'correlation': round(r_stat, 3),
                        'p_value': round(p_value, 4),
                        'significant': p_value < significance_level,
                        'interpretation': self._interpret_correlation_strength(abs(r_stat))
                    }
            
            result['correlation_tests'] = correlation_tests
            
            # An√°lise por categorias
            categorical_correlations = {}
            categorical_cols = ['Grupo_Produto', 'Metal', 'Faixa_Etaria', 'Sexo', 'Estado']
            
            for cat_col in categorical_cols:
                if cat_col in df.columns:
                    # ANOVA para testar diferen√ßa entre grupos
                    groups = [group[target_column].dropna() for name, group in df.groupby(cat_col)]
                    if len(groups) > 1 and all(len(g) > 0 for g in groups):
                        f_stat, p_val = stats.f_oneway(*groups)
                        
                        # Eta-squared (effect size)
                        group_means = df.groupby(cat_col)[target_column].mean()
                        overall_mean = df[target_column].mean()
                        ss_between = sum(df.groupby(cat_col).size() * (group_means - overall_mean)**2)
                        ss_total = sum((df[target_column] - overall_mean)**2)
                        eta_squared = ss_between / ss_total if ss_total > 0 else 0
                        
                        categorical_correlations[cat_col] = {
                            'f_statistic': round(f_stat, 3),
                            'p_value': round(p_val, 4),
                            'significant': p_val < significance_level,
                            'eta_squared': round(eta_squared, 3),
                            'effect_size': self._interpret_effect_size(eta_squared),
                            'group_means': group_means.round(2).to_dict()
                        }
            
            result['categorical_analysis'] = categorical_correlations
            
            # Insights autom√°ticos
            result['insights'] = self._generate_correlation_insights(result)
            result['business_recommendations'] = self._generate_correlation_recommendations(result)
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise de correla√ß√£o: {str(e)}"}
    
    def _interpret_correlation_strength(self, correlation: float) -> str:
        """Interpretar for√ßa da correla√ß√£o."""
        if correlation >= 0.7:
            return "Muito forte"
        elif correlation >= 0.5:
            return "Forte"
        elif correlation >= 0.3:
            return "Moderada"
        elif correlation >= 0.1:
            return "Fraca"
        else:
            return "Muito fraca"
    
    def _interpret_effect_size(self, eta_squared: float) -> str:
        """Interpretar tamanho do efeito."""
        if eta_squared >= 0.14:
            return "Grande"
        elif eta_squared >= 0.06:
            return "M√©dio"
        elif eta_squared >= 0.01:
            return "Pequeno"
        else:
            return "Desprez√≠vel"
    
    def _generate_correlation_insights(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Gerar insights de correla√ß√£o."""
        insights = []
        
        if 'significant_correlations' in result:
            correlations = result['significant_correlations']
            if correlations:
                strongest = max(correlations.items(), key=lambda x: x[1])
                insights.append({
                    "type": "Correla√ß√£o Forte",
                    "message": f"Correla√ß√£o mais forte: {strongest[0]} ({strongest[1]:.3f})",
                    "impact": "high",
                    "recommendation": f"Investigar rela√ß√£o causal entre {strongest[0]} e vendas"
                })
                
        # Correla√ß√µes por categoria
        if 'categorical_analysis' in result:
            cat_analysis = result['categorical_analysis']
            significant_categories = [cat for cat, data in cat_analysis.items() 
                                   if data.get('significant', False)]
            if significant_categories:
                insights.append({
                    "type": "Impacto Categ√≥rico",
                    "message": f"Categorias com impacto significativo: {', '.join(significant_categories)}",
                    "impact": "medium",
                    "recommendation": "Focar estrat√©gias nestas categorias"
                })
        
        return insights
    
    def _generate_correlation_recommendations(self, result: Dict[str, Any]) -> List[str]:
        """Gerar recomenda√ß√µes baseadas em correla√ß√µes."""
        recommendations = []
        
        if 'correlation_tests' in result:
            for var, test in result['correlation_tests'].items():
                if test['significant'] and test['correlation'] > 0.5:
                    recommendations.append(f"Otimizar {var} para aumentar vendas (correla√ß√£o forte)")
        
        return recommendations[:5]
    
    def _multidimensional_clustering_analysis(self, df: pd.DataFrame, target_column: str = "Total_Liquido",
                                            clustering_method: str = "auto", **kwargs) -> Dict[str, Any]:
        """FASE 3: An√°lise de clustering multidimensional com cache otimizado."""
        
        # Verificar cache primeiro
        cache_key = f"clustering_{len(df)}_{target_column}_{clustering_method}"
        if cache_key in self._analysis_cache:
            print("üìã Usando resultado de clustering do cache")
            return self._analysis_cache[cache_key]
        
        try:
            print("üéØ Executando an√°lise de clustering multidimensional...")
            
            # Preparar dados com amostragem se necess√°rio
            if len(df) > 50000:
                print(f"üìä Dataset grande ({len(df)} registros) - usando amostra de 30.000 para clustering")
                df_sample = df.sample(n=30000, random_state=42)
            else:
                df_sample = df
            
            result = {
                'analysis_type': 'Multidimensional Clustering Analysis',
                'target_column': target_column,
                'method': clustering_method,
                'sample_size': len(df_sample),
                'original_size': len(df)
            }
            
            # Preparar dados para clustering
            feature_cols = self._select_clustering_features(df_sample, target_column)
            if len(feature_cols) < 2:
                return {'error': 'Insuficientes features num√©ricas para clustering'}
            
            X = df_sample[feature_cols].fillna(0)
            
            # Usar dados j√° escalados se dispon√≠veis, sen√£o escalar
            if any(col.endswith('_scaled') for col in feature_cols):
                X_scaled = X.values
                print("‚úÖ Usando dados pr√©-escalados")
            else:
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                print("‚öôÔ∏è Aplicando padroniza√ß√£o aos dados")
            
            # Escolher m√©todo automaticamente se necess√°rio
            if clustering_method == "auto":
                clustering_method = self._select_optimal_clustering_method(X_scaled)
                print(f"ü§ñ M√©todo autom√°tico selecionado: {clustering_method}")
            
            # Executar clustering
            if clustering_method == "kmeans":
                cluster_result = self._perform_kmeans_clustering(X_scaled, df_sample, feature_cols)
            elif clustering_method == "hierarchical":
                cluster_result = self._perform_hierarchical_clustering(X_scaled, df_sample, feature_cols)
            elif clustering_method == "dbscan":
                cluster_result = self._perform_dbscan_clustering(X_scaled, df_sample, feature_cols)
            else:
                return {'error': f"M√©todo de clustering '{clustering_method}' n√£o suportado"}
            
            result.update(cluster_result)
            
            # An√°lise dos clusters
            if 'cluster_labels' in result:
                df_clustered = df_sample.copy()
                df_clustered['Cluster'] = result['cluster_labels']
                
                # Perfil dos clusters
                cluster_profiles = {}
                for cluster_id in df_clustered['Cluster'].unique():
                    if cluster_id != -1:  # Ignorar outliers do DBSCAN
                        cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]
                        
                        profile = {
                            'size': len(cluster_data),
                            'size_percentage': round(len(cluster_data) / len(df_clustered) * 100, 1),
                            'avg_revenue': round(cluster_data[target_column].mean(), 2),
                            'total_revenue': round(cluster_data[target_column].sum(), 2),
                            'revenue_share': round(cluster_data[target_column].sum() / df_sample[target_column].sum() * 100, 1)
                        }
                        
                        # Caracter√≠sticas demogr√°ficas se dispon√≠veis
                        if 'Faixa_Etaria' in cluster_data.columns:
                            profile['predominant_age_group'] = cluster_data['Faixa_Etaria'].mode().iloc[0] if len(cluster_data['Faixa_Etaria'].mode()) > 0 else 'N/A'
                        
                        if 'Sexo' in cluster_data.columns:
                            profile['gender_distribution'] = cluster_data['Sexo'].value_counts().to_dict()
                        
                        # Produtos preferidos
                        if 'Grupo_Produto' in cluster_data.columns:
                            profile['preferred_products'] = cluster_data['Grupo_Produto'].value_counts().head(3).to_dict()
                        
                        cluster_profiles[f'Cluster_{cluster_id}'] = profile
                
                result['cluster_profiles'] = cluster_profiles
            
            # Insights de clustering
            result['insights'] = self._generate_clustering_insights(result)
            result['business_recommendations'] = self._generate_clustering_recommendations(result)
            
            # Salvar no cache
            self._analysis_cache[cache_key] = result
            print(f"üíæ Resultado salvo no cache (chave: {cache_key})")
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise de clustering: {str(e)}"}
    
    def _select_clustering_features(self, df: pd.DataFrame, target_column: str) -> List[str]:
        """Selecionar features otimizadas para clustering."""
        # Priorizar features escaladas se dispon√≠veis
        scaled_features = [col for col in df.columns if col.endswith('_scaled')]
        
        if scaled_features:
            print(f"üéØ Usando {len(scaled_features)} features escaladas para clustering")
            return scaled_features
        
        # Fallback para features num√©ricas b√°sicas
        numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
        exclude_cols = [target_column, 'Data', 'Codigo_Cliente', 'Codigo_Produto', 'Ano', 'Mes']
        selected_features = [col for col in numeric_features[:10] 
                           if col not in exclude_cols and not df[col].isna().all()]
        
        return selected_features
    
    def _select_optimal_clustering_method(self, X_scaled: np.ndarray) -> str:
        """Selecionar m√©todo de clustering automaticamente."""
        n_samples = X_scaled.shape[0]
        
        if n_samples < 50:
            return "hierarchical"
        elif n_samples > 1000:
            return "kmeans"
        else:
            return "kmeans"  # fallback seguro
    
    def _perform_kmeans_clustering(self, X_scaled: np.ndarray, df: pd.DataFrame, feature_cols: List[str]) -> Dict[str, Any]:
        """Executar clustering K-means."""
        optimal_k = min(5, len(X_scaled) // 10)  # Heur√≠stica simples
        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X_scaled)
        
        silhouette_avg = silhouette_score(X_scaled, cluster_labels) if len(set(cluster_labels)) > 1 else -1
        
        return {
            'method_used': 'K-Means',
            'optimal_k': optimal_k,
            'cluster_labels': cluster_labels,
            'silhouette_score': round(silhouette_avg, 3),
            'feature_names': feature_cols
        }
    
    def _perform_hierarchical_clustering(self, X_scaled: np.ndarray, df: pd.DataFrame, feature_cols: List[str]) -> Dict[str, Any]:
        """Executar clustering hier√°rquico."""
        linkage_matrix = linkage(X_scaled, method='ward')
        optimal_clusters = min(5, len(X_scaled) // 10)
        cluster_labels = fcluster(linkage_matrix, optimal_clusters, criterion='maxclust') - 1
        
        silhouette_avg = silhouette_score(X_scaled, cluster_labels) if len(set(cluster_labels)) > 1 else -1
        
        return {
            'method_used': 'Hierarchical',
            'optimal_clusters': optimal_clusters,
            'cluster_labels': cluster_labels,
            'silhouette_score': round(silhouette_avg, 3),
            'feature_names': feature_cols
        }
    
    def _perform_dbscan_clustering(self, X_scaled: np.ndarray, df: pd.DataFrame, feature_cols: List[str]) -> Dict[str, Any]:
        """Executar clustering DBSCAN."""
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        cluster_labels = dbscan.fit_predict(X_scaled)
        
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_outliers = list(cluster_labels).count(-1)
        
        silhouette_avg = silhouette_score(X_scaled, cluster_labels) if n_clusters > 1 else -1
        
        return {
            'method_used': 'DBSCAN',
            'cluster_labels': cluster_labels,
            'n_clusters': n_clusters,
            'n_outliers': n_outliers,
            'silhouette_score': round(silhouette_avg, 3),
            'feature_names': feature_cols
        }
    
    def _generate_clustering_insights(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Gerar insights de clustering."""
        insights = []
        
        if 'cluster_profiles' in result:
            profiles = result['cluster_profiles']
            
            # Cluster mais valioso
            revenue_clusters = {k: v.get('total_revenue', 0) for k, v in profiles.items()}
            if revenue_clusters:
                top_cluster = max(revenue_clusters, key=revenue_clusters.get)
                top_revenue_share = profiles[top_cluster].get('revenue_share', 0)
                insights.append({
                    "type": "Cluster Valioso",
                    "message": f"{top_cluster} gera {top_revenue_share}% da receita",
                    "impact": "high",
                    "recommendation": f"Focar estrat√©gias de reten√ß√£o no {top_cluster}"
                })
        
        return insights
    
    def _generate_clustering_recommendations(self, result: Dict[str, Any]) -> List[str]:
        """Gerar recomenda√ß√µes de clustering."""
        recommendations = []
        
        if 'cluster_profiles' in result:
            recommendations.append("Desenvolver estrat√©gias espec√≠ficas para cada cluster identificado")
            recommendations.append("Personalizar comunica√ß√£o baseada no perfil do cluster")
        
        return recommendations
    
    def _comprehensive_outlier_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de outliers usando m√∫ltiplos m√©todos."""
        try:
            print("üîç Executando an√°lise de outliers...")
            
            target_col = kwargs.get('target_column', 'Total_Liquido')
            if target_col not in df.columns:
                return {'error': f"Coluna {target_col} n√£o encontrada"}
            
            # M√©todo IQR
            Q1 = df[target_col].quantile(0.25)
            Q3 = df[target_col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers_iqr = df[(df[target_col] < lower_bound) | (df[target_col] > upper_bound)]
            
            # M√©todo Z-score
            z_scores = np.abs(stats.zscore(df[target_col].dropna()))
            outliers_zscore = df[z_scores > 3]
            
            result = {
                'analysis_type': 'Comprehensive Outlier Analysis',
                'target_column': target_col,
                'iqr_method': {
                    'outliers_count': len(outliers_iqr),
                    'outliers_percentage': round(len(outliers_iqr) / len(df) * 100, 2),
                    'lower_bound': round(lower_bound, 2),
                    'upper_bound': round(upper_bound, 2)
                },
                'zscore_method': {
                    'outliers_count': len(outliers_zscore),
                    'outliers_percentage': round(len(outliers_zscore) / len(df) * 100, 2)
                },
                'insights': [
                    {
                        "type": "Outliers Detectados",
                        "message": f"{len(outliers_iqr)} outliers identificados pelo m√©todo IQR",
                        "impact": "medium",
                        "recommendation": "Investigar transa√ß√µes an√¥malas para identificar oportunidades ou problemas"
                    }
                ]
            }
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise de outliers: {str(e)}"}
    
    def _advanced_distribution_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de distribui√ß√£o com testes de normalidade."""
        try:
            print("üìä Executando an√°lise de distribui√ß√£o...")
            
            target_col = kwargs.get('target_column', 'Total_Liquido')
            if target_col not in df.columns:
                return {'error': f"Coluna {target_col} n√£o encontrada"}
            
            data = df[target_col].dropna()
            
            # Teste de normalidade Shapiro-Wilk (para amostras pequenas)
            if len(data) <= 5000:
                shapiro_stat, shapiro_p = stats.shapiro(data.sample(min(5000, len(data))))
            else:
                shapiro_stat, shapiro_p = stats.shapiro(data.sample(5000))
            
            # Estat√≠sticas descritivas
            result = {
                'analysis_type': 'Advanced Distribution Analysis',
                'target_column': target_col,
                'descriptive_stats': {
                    'mean': round(data.mean(), 2),
                    'median': round(data.median(), 2),
                    'std': round(data.std(), 2),
                    'skewness': round(stats.skew(data), 3),
                    'kurtosis': round(stats.kurtosis(data), 3)
                },
                'normality_test': {
                    'shapiro_statistic': round(shapiro_stat, 4),
                    'shapiro_p_value': round(shapiro_p, 4),
                    'is_normal': shapiro_p > 0.05
                },
                'insights': [
                    {
                        "type": "Distribui√ß√£o",
                        "message": f"Dados {'seguem' if shapiro_p > 0.05 else 'n√£o seguem'} distribui√ß√£o normal",
                        "impact": "medium",
                        "recommendation": "Usar testes param√©tricos" if shapiro_p > 0.05 else "Usar testes n√£o-param√©tricos"
                    }
                ]
            }
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise de distribui√ß√£o: {str(e)}"}
    
    def _temporal_trend_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de tend√™ncia temporal."""
        try:
            print("üìà Executando an√°lise de tend√™ncia temporal...")
            
            if 'Data' not in df.columns:
                return {'error': 'Coluna Data n√£o encontrada'}
            
            target_col = kwargs.get('target_column', 'Total_Liquido')
            
            # Agregar por m√™s
            monthly_data = df.groupby(df['Data'].dt.to_period('M'))[target_col].sum()
            
            # Teste de tend√™ncia Mann-Kendall
            from scipy.stats import kendalltau
            x = range(len(monthly_data))
            tau, p_value = kendalltau(x, monthly_data.values)
            
            result = {
                'analysis_type': 'Temporal Trend Analysis',
                'target_column': target_col,
                'trend_test': {
                    'kendall_tau': round(tau, 3),
                    'p_value': round(p_value, 4),
                    'has_trend': p_value < 0.05,
                    'trend_direction': 'crescente' if tau > 0 else 'decrescente' if tau < 0 else 'est√°vel'
                },
                'monthly_summary': {
                    'periods': len(monthly_data),
                    'avg_monthly': round(monthly_data.mean(), 2),
                    'growth_rate': round(monthly_data.pct_change().mean() * 100, 2)
                },
                'insights': [
                    {
                        "type": "Tend√™ncia Temporal",
                        "message": f"Tend√™ncia {result['trend_test']['trend_direction']} detectada",
                        "impact": "high" if p_value < 0.05 else "low",
                        "recommendation": "Ajustar estrat√©gias baseado na tend√™ncia identificada"
                    }
                ]
            }
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise temporal: {str(e)}"}
    
    def _demographic_patterns_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de padr√µes demogr√°ficos."""
        try:
            print("üë• Executando an√°lise demogr√°fica...")
            
            target_col = kwargs.get('target_column', 'Total_Liquido')
            
            result = {
                'analysis_type': 'Demographic Patterns Analysis',
                'target_column': target_col
            }
            
            # An√°lise por sexo
            if 'Sexo' in df.columns:
                gender_stats = df.groupby('Sexo')[target_col].agg(['count', 'mean', 'sum']).round(2)
                result['gender_analysis'] = gender_stats.to_dict()
            
            # An√°lise por faixa et√°ria
            if 'Faixa_Etaria' in df.columns:
                age_stats = df.groupby('Faixa_Etaria')[target_col].agg(['count', 'mean', 'sum']).round(2)
                result['age_analysis'] = age_stats.to_dict()
            
            result['insights'] = [
                {
                    "type": "Demografia",
                    "message": "Padr√µes demogr√°ficos identificados",
                    "impact": "medium",
                    "recommendation": "Personalizar ofertas por segmento demogr√°fico"
                }
            ]
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise demogr√°fica: {str(e)}"}
    
    def _generational_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise geracional."""
        return {'message': 'An√°lise geracional em desenvolvimento', 'status': 'placeholder'}
    
    def _behavioral_customer_segmentation(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """Segmenta√ß√£o comportamental de clientes."""
        return {'message': 'Segmenta√ß√£o comportamental em desenvolvimento', 'status': 'placeholder'}
    
    def _geographic_performance_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de performance geogr√°fica."""
        return {'message': 'An√°lise geogr√°fica em desenvolvimento', 'status': 'placeholder'}
    
    def _regional_patterns_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de padr√µes regionais."""
        return {'message': 'An√°lise de padr√µes regionais em desenvolvimento', 'status': 'placeholder'}
    
    def _price_elasticity_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de elasticidade de pre√ßos."""
        return {'message': 'An√°lise de elasticidade em desenvolvimento', 'status': 'placeholder'}
    
    def _profitability_pattern_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de padr√µes de rentabilidade."""
        return {'message': 'An√°lise de rentabilidade em desenvolvimento', 'status': 'placeholder'}
    
    def _comprehensive_customer_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise abrangente de clientes."""
        return {'message': 'An√°lise abrangente de clientes em desenvolvimento', 'status': 'placeholder'}
    
    def _statistical_product_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise estat√≠stica de produtos."""
        return {'message': 'An√°lise estat√≠stica de produtos em desenvolvimento', 'status': 'placeholder'} 