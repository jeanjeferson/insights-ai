from crewai.tools import BaseTool
from typing import Type, Optional, Dict, Any, List, Tuple
from pydantic import BaseModel, Field, field_validator
import pandas as pd
import numpy as np
from scipy import stats
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import json
import warnings
from datetime import datetime
import time
import traceback
import os

# Importar m√≥dulos compartilhados consolidados
try:
    # Imports relativos (quando usado como m√≥dulo)
    from .shared.data_preparation import DataPreparationMixin
    from .shared.report_formatter import ReportFormatterMixin
    from .shared.business_mixins import JewelryRFMAnalysisMixin
except ImportError:
    # Imports absolutos (quando executado diretamente)
    from insights.tools.shared.data_preparation import DataPreparationMixin
    from insights.tools.shared.report_formatter import ReportFormatterMixin
    from insights.tools.shared.business_mixins import JewelryRFMAnalysisMixin

warnings.filterwarnings('ignore')

class StatisticalAnalysisToolInput(BaseModel):
    """Schema otimizado para an√°lise estat√≠stica com valida√ß√µes robustas."""
    
    analysis_type: str = Field(
        ..., 
        description="""Tipo de an√°lise estat√≠stica especializada:
        
        üî¨ AN√ÅLISES ESTAT√çSTICAS CORE:
        - 'correlation': An√°lise de correla√ß√£o multi-dimensional com testes de signific√¢ncia
        - 'clustering': Clustering avan√ßado (K-means, Hier√°rquico, DBSCAN) com valida√ß√£o
        - 'outliers': Detec√ß√£o de outliers usando m√∫ltiplos m√©todos estat√≠sticos
        - 'distribution': An√°lise de distribui√ß√µes e testes de normalidade
        - 'trend_analysis': Testes de tend√™ncia temporal e sazonalidade
        
        üë• AN√ÅLISES DEMOGR√ÅFICAS ESPECIALIZADAS:
        - 'demographic_patterns': Padr√µes demogr√°ficos avan√ßados (idade, sexo, estado civil)
        - 'generational_analysis': An√°lise geracional (Gen Z, Millennial, Gen X, Boomer)
        - 'customer_segmentation': Segmenta√ß√£o comportamental avan√ßada
        
        üó∫Ô∏è AN√ÅLISES GEOGR√ÅFICAS DETALHADAS:
        - 'geographic_performance': Performance por estado e cidade com estat√≠sticas
        - 'regional_patterns': Padr√µes sazonais e comportamentais regionais
        
        üí∞ AN√ÅLISES ESPECIALIZADAS:
        - 'price_sensitivity': An√°lise de elasticidade de pre√ßos e sensibilidade
        - 'profitability_patterns': Padr√µes de rentabilidade com an√°lise estat√≠stica
        
        üîó AN√ÅLISES INTEGRADAS:
        - 'comprehensive_customer_analysis': An√°lise completa de clientes com estat√≠sticas
        - 'product_performance_analysis': Performance de produtos com testes estat√≠sticos
        """,
        json_schema_extra={"example": "correlation"}
    )
    
    data_csv: str = Field(
        default="data/vendas.csv", 
        description="Caminho para arquivo CSV com dados de vendas. Use 'data/vendas.csv' para dados principais.",
        json_schema_extra={"example": "data/vendas.csv"}
    )
    
    target_column: str = Field(
        default="Total_Liquido", 
        description="Coluna alvo para an√°lise. Use 'Total_Liquido' para receita, 'Quantidade' para volume.",
        json_schema_extra={"example": "Total_Liquido"}
    )
    
    significance_level: float = Field(
        default=0.05, 
        description="N√≠vel de signific√¢ncia para testes estat√≠sticos (0.01-0.10). Padr√£o: 0.05 (95% confian√ßa).",
        ge=0.01,
        le=0.10
    )
    
    clustering_method: str = Field(
        default="auto", 
        description="M√©todo de clustering: 'kmeans' (r√°pido), 'hierarchical' (interpret√°vel), 'dbscan' (outliers), 'auto' (otimizado).",
        json_schema_extra={"pattern": "^(kmeans|hierarchical|dbscan|auto)$"}
    )
    
    min_correlation: float = Field(
        default=0.3, 
        description="Correla√ß√£o m√≠nima para reportar (0.1-0.9). Valores altos = apenas correla√ß√µes fortes.",
        ge=0.1,
        le=0.9
    )
    
    demographic_focus: bool = Field(
        default=True, 
        description="Incluir an√°lise demogr√°fica detalhada. Recomendado: True para insights de cliente."
    )
    
    geographic_focus: bool = Field(
        default=True, 
        description="Incluir an√°lise geogr√°fica detalhada. Recomendado: True para expans√£o regional."
    )
    
    cache_results: bool = Field(
        default=True, 
        description="Usar cache para an√°lises pesadas. Recomendado: True para datasets grandes."
    )
    
    sample_size: Optional[int] = Field(
        default=None,
        description="Tamanho da amostra para an√°lises pesadas (1000-100000). None = usar todos os dados.",
        ge=1000,
        le=100000
    )
    
    @field_validator('analysis_type')
    @classmethod
    def validate_analysis_type(cls, v):
        valid_types = [
            'correlation', 'clustering', 'outliers', 'distribution', 'trend_analysis',
            'demographic_patterns', 'generational_analysis', 'customer_segmentation',
            'geographic_performance', 'regional_patterns', 'price_sensitivity',
            'profitability_patterns', 'comprehensive_customer_analysis', 'product_performance_analysis'
        ]
        if v not in valid_types:
            raise ValueError(f"analysis_type deve ser um de: {valid_types}")
        return v

class StatisticalAnalysisTool(BaseTool, 
                               DataPreparationMixin, 
                               ReportFormatterMixin,
                               JewelryRFMAnalysisMixin):
    """
    üî¨ MOTOR DE AN√ÅLISES ESTAT√çSTICAS AVAN√áADAS PARA JOALHERIAS
    
    QUANDO USAR:
    - Descobrir padr√µes ocultos nos dados de vendas
    - Realizar an√°lises estat√≠sticas rigorosas com testes de signific√¢ncia
    - Segmentar clientes baseado em comportamento estat√≠stico
    - Analisar correla√ß√µes complexas entre vari√°veis
    - Detectar outliers e anomalias estat√≠sticas
    - Realizar clustering avan√ßado de produtos/clientes
    
    CASOS DE USO ESPEC√çFICOS:
    - analysis_type='correlation': Descobrir quais fatores influenciam vendas
    - analysis_type='clustering': Segmentar clientes por padr√µes de compra
    - analysis_type='demographic_patterns': Analisar comportamento por idade/sexo
    - analysis_type='geographic_performance': Comparar performance entre regi√µes
    - analysis_type='price_sensitivity': Medir elasticidade de pre√ßos
    - analysis_type='outliers': Identificar vendas an√¥malas para investiga√ß√£o
    
    RESULTADOS ENTREGUES:
    - An√°lises estat√≠sticas rigorosas com testes de signific√¢ncia
    - Segmenta√ß√µes autom√°ticas baseadas em dados
    - Correla√ß√µes significativas com interpreta√ß√£o de neg√≥cio
    - Clusters de clientes/produtos com perfis detalhados
    - Insights demogr√°ficos e geogr√°ficos aprofundados
    - Recomenda√ß√µes baseadas em evid√™ncias estat√≠sticas
    """
    
    name: str = "Statistical Analysis Tool"
    description: str = (
        "Motor de an√°lises estat√≠sticas avan√ßadas para descobrir padr√µes ocultos em dados de joalherias. "
        "Realiza an√°lises rigorosas com testes de signific√¢ncia, clustering, correla√ß√µes e segmenta√ß√µes. "
        "Ideal para insights profundos sobre comportamento de clientes, performance de produtos e padr√µes de mercado."
    )
    args_schema: Type[BaseModel] = StatisticalAnalysisToolInput
    
    def __init__(self):
        super().__init__()
        self._analysis_cache = {}  # Cache para an√°lises computacionalmente pesadas
    
    def _run(self, analysis_type: str, data_csv: str = "data/vendas.csv", 
             target_column: str = "Total_Liquido", significance_level: float = 0.05,
             clustering_method: str = "auto", min_correlation: float = 0.3,
             demographic_focus: bool = True, geographic_focus: bool = True,
             cache_results: bool = True, sample_size: Optional[int] = None) -> str:
        try:
            print(f"üî¨ Iniciando an√°lise estat√≠stica v3.0: {analysis_type}")
            print(f"‚öôÔ∏è Configura√ß√µes: signific√¢ncia={significance_level}, correla√ß√£o_min={min_correlation}")
            
            # 1. Carregar e preparar dados usando m√≥dulo consolidado
            df = self._load_and_prepare_statistical_data(data_csv, cache_results, sample_size)
            if df is None:
                return json.dumps({
                    "error": "N√£o foi poss√≠vel carregar ou preparar os dados para an√°lise estat√≠stica",
                    "troubleshooting": {
                        "check_file_exists": f"Verifique se {data_csv} existe",
                        "check_data_quality": "Confirme que os dados t√™m qualidade suficiente para an√°lise",
                        "check_sample_size": "Verifique se h√° dados suficientes para an√°lise estat√≠stica"
                    },
                    "metadata": {
                        "tool": "Statistical Analysis",
                        "status": "error",
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                }, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ Dados preparados: {len(df)} registros, {len(df.columns)} campos")
            
            # 2. Mapeamento de an√°lises especializadas
            analysis_methods = {
                # An√°lises estat√≠sticas core
                'correlation': self._advanced_correlation_analysis,
                'clustering': self._multidimensional_clustering_analysis,
                'outliers': self._comprehensive_outlier_analysis,
                'distribution': self._advanced_distribution_analysis,
                'trend_analysis': self._temporal_trend_analysis,
                
                # An√°lises especializadas
                'demographic_patterns': self._demographic_patterns_analysis,
                'generational_analysis': self._generational_analysis,
                'customer_segmentation': self._behavioral_customer_segmentation,
                'geographic_performance': self._geographic_performance_analysis,
                'regional_patterns': self._regional_patterns_analysis,
                'price_sensitivity': self._price_elasticity_analysis,
                'profitability_patterns': self._profitability_pattern_analysis,
                
                # An√°lises integradas
                'comprehensive_customer_analysis': self._comprehensive_customer_analysis,
                'product_performance_analysis': self._statistical_product_analysis
            }
            
            if analysis_type not in analysis_methods:
                available = list(analysis_methods.keys())
                return json.dumps({
                    "error": f"An√°lise '{analysis_type}' n√£o suportada",
                    "available_analyses": available,
                    "suggestion": "Use uma das an√°lises dispon√≠veis listadas acima",
                    "metadata": {
                        "tool": "Statistical Analysis",
                        "status": "error",
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                }, ensure_ascii=False, indent=2)
            
            # 3. Executar an√°lise com par√¢metros
            analysis_params = {
                'target_column': target_column,
                'significance_level': significance_level,
                'clustering_method': clustering_method,
                'min_correlation': min_correlation,
                'demographic_focus': demographic_focus,
                'geographic_focus': geographic_focus
            }
            
            print(f"üéØ Executando an√°lise: {analysis_type}")
            result = analysis_methods[analysis_type](df, **analysis_params)
            
            # 4. Adicionar metadados
            result['metadata'] = {
                'tool': 'Statistical Analysis Tool v3.0',
                'analysis_type': analysis_type,
                'target_column': target_column,
                'total_records': len(df),
                'significance_level': significance_level,
                'date_range': {
                    'start': df['Data'].min().strftime("%Y-%m-%d") if 'Data' in df.columns else "N/A",
                    'end': df['Data'].max().strftime("%Y-%m-%d") if 'Data' in df.columns else "N/A"
                },
                'generated_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # 5. Armazenar no cache se solicitado
            if cache_results:
                cache_key = f"stat_{analysis_type}_{hash(data_csv)}_{target_column}"
                self._analysis_cache[cache_key] = result
                print(f"üíæ Resultado salvo no cache")
            
            # 6. Formatar resultado final
            print("‚úÖ An√°lise estat√≠stica conclu√≠da com sucesso!")
            return json.dumps(result, ensure_ascii=False, indent=2, default=str)
            
        except Exception as e:
            error_response = {
                "error": f"Erro na an√°lise estat√≠stica v3.0: {str(e)}",
                "analysis_type": analysis_type,
                "data_csv": data_csv,
                "troubleshooting": {
                    "check_data_format": "Verifique se os dados est√£o no formato correto",
                    "check_statistical_requirements": "Confirme que h√° dados suficientes para an√°lise estat√≠stica",
                    "try_simpler_analysis": "Tente uma an√°lise mais simples primeiro"
                },
                "metadata": {
                    "tool": "Statistical Analysis",
                    "status": "error",
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
            }
            return json.dumps(error_response, ensure_ascii=False, indent=2)
    
    def _load_and_prepare_statistical_data(self, data_csv: str, use_cache: bool = True, sample_size: Optional[int] = None) -> Optional[pd.DataFrame]:
        """Carregar e preparar dados especificamente para an√°lises estat√≠sticas."""
        cache_key = f"statistical_data_{hash(data_csv)}_{sample_size}"
        
        # Verificar cache
        if use_cache and cache_key in self._analysis_cache:
            print("üìã Usando dados estat√≠sticos do cache")
            return self._analysis_cache[cache_key]
        
        try:
            # Carregar dados brutos
            df = pd.read_csv(data_csv, sep=';', encoding='utf-8')
            print(f"üìÅ Arquivo carregado: {len(df)} registros")
            
            # Aplicar amostragem se necess√°rio
            if sample_size and len(df) > sample_size:
                df = df.sample(n=sample_size, random_state=42)
                print(f"üìä Amostra aplicada: {len(df)} registros")
            
            # Preparar dados usando mixin consolidado (n√≠vel strict para estat√≠sticas)
            df_prepared = self.prepare_jewelry_data(df, validation_level="strict")
            
            if df_prepared is None:
                print("‚ùå Falha na prepara√ß√£o dos dados estat√≠sticos")
                return None
            
            # Prepara√ß√µes espec√≠ficas para an√°lises estat√≠sticas
            df_prepared = self._add_statistical_features(df_prepared)
            
            # Armazenar no cache
            if use_cache:
                self._analysis_cache[cache_key] = df_prepared
                print("üíæ Dados estat√≠sticos salvos no cache")
            
            return df_prepared
            
        except Exception as e:
            print(f"‚ùå Erro no carregamento de dados estat√≠sticos: {str(e)}")
            return None
    
    def _add_statistical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Adicionar features espec√≠ficas para an√°lises estat√≠sticas."""
        try:
            print("üßÆ Adicionando features estat√≠sticas...")
            
            # FASE 1: CORRE√á√ÉO DO BUG - Padroniza√ß√£o de valores para clustering
            numeric_cols = ['Total_Liquido', 'Quantidade', 'Margem_Real', 'Preco_Unitario']
            available_numeric = [col for col in numeric_cols if col in df.columns]
            
            if available_numeric:
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(df[available_numeric])
                
                # CORRE√á√ÉO: Criar colunas escaladas corretamente
                for i, col in enumerate(available_numeric):
                    df[f'{col}_scaled'] = scaled_data[:, i]
                
                print(f"‚úÖ Padroniza√ß√£o aplicada: {len(available_numeric)} campos escalados")
            
            # Features temporais para an√°lise de tend√™ncia
            if 'Data' in df.columns:
                df['Days_Since_Start'] = (df['Data'] - df['Data'].min()).dt.days
                df['Weeks_Since_Start'] = df['Days_Since_Start'] // 7
                df['Month_Index'] = df['Data'].dt.month
                print("‚úÖ Features temporais adicionadas")
            
            # Encoding de vari√°veis categ√≥ricas para clustering
            categorical_cols = ['Sexo', 'Estado_Civil', 'Estado', 'Grupo_Produto']
            for col in categorical_cols:
                if col in df.columns:
                    # One-hot encoding simples
                    dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)
                    df = pd.concat([df, dummies], axis=1)
            
            print("‚úÖ Encoding categ√≥rico aplicado")
            
            # Quartis e ranks para an√°lises
            if 'Total_Liquido' in df.columns:
                df['Revenue_Quartile'] = pd.qcut(df['Total_Liquido'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
                df['Revenue_Rank'] = df['Total_Liquido'].rank(pct=True)
                print("‚úÖ Quartis e ranks calculados")
            
            return df
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao adicionar features estat√≠sticas: {str(e)}")
            return df
    
    def _advanced_correlation_analysis(self, df: pd.DataFrame, target_column: str = "Total_Liquido",
                                     min_correlation: float = 0.3, significance_level: float = 0.05,
                                     **kwargs) -> Dict[str, Any]:
        """An√°lise de correla√ß√£o avan√ßada com testes de signific√¢ncia."""
        try:
            print("üîç Executando an√°lise de correla√ß√£o avan√ßada...")
            
            result = {
                'analysis_type': 'Advanced Correlation Analysis',
                'target_column': target_column,
                'significance_level': significance_level,
                'min_correlation_threshold': min_correlation
            }
            
            # Selecionar colunas num√©ricas para correla√ß√£o
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if target_column not in numeric_cols:
                return {'error': f"Coluna alvo '{target_column}' n√£o √© num√©rica"}
            
            # Matriz de correla√ß√£o
            corr_matrix = df[numeric_cols].corr()
            target_correlations = corr_matrix[target_column].abs().sort_values(ascending=False)
            
            # Filtrar correla√ß√µes significativas
            significant_correlations = target_correlations[
                (target_correlations >= min_correlation) & 
                (target_correlations.index != target_column)
            ]
            
            result['correlation_matrix'] = corr_matrix.round(3).to_dict()
            result['significant_correlations'] = significant_correlations.round(3).to_dict()
            
            # Testes de signific√¢ncia para correla√ß√µes
            correlation_tests = {}
            for col in significant_correlations.index:
                if col in df.columns and not df[col].isna().all():
                    r_stat, p_value = stats.pearsonr(df[target_column].dropna(), df[col].dropna())
                    correlation_tests[col] = {
                        'correlation': round(r_stat, 3),
                        'p_value': round(p_value, 4),
                        'significant': p_value < significance_level,
                        'interpretation': self._interpret_correlation_strength(abs(r_stat))
                    }
            
            result['correlation_tests'] = correlation_tests
            
            # An√°lise por categorias
            categorical_correlations = {}
            categorical_cols = ['Grupo_Produto', 'Metal', 'Faixa_Etaria', 'Sexo', 'Estado']
            
            for cat_col in categorical_cols:
                if cat_col in df.columns:
                    # ANOVA para testar diferen√ßa entre grupos
                    groups = [group[target_column].dropna() for name, group in df.groupby(cat_col)]
                    if len(groups) > 1 and all(len(g) > 0 for g in groups):
                        f_stat, p_val = stats.f_oneway(*groups)
                        
                        # Eta-squared (effect size)
                        group_means = df.groupby(cat_col)[target_column].mean()
                        overall_mean = df[target_column].mean()
                        ss_between = sum(df.groupby(cat_col).size() * (group_means - overall_mean)**2)
                        ss_total = sum((df[target_column] - overall_mean)**2)
                        eta_squared = ss_between / ss_total if ss_total > 0 else 0
                        
                        categorical_correlations[cat_col] = {
                            'f_statistic': round(f_stat, 3),
                            'p_value': round(p_val, 4),
                            'significant': p_val < significance_level,
                            'eta_squared': round(eta_squared, 3),
                            'effect_size': self._interpret_effect_size(eta_squared),
                            'group_means': group_means.round(2).to_dict()
                        }
            
            result['categorical_analysis'] = categorical_correlations
            
            # Insights autom√°ticos
            result['insights'] = self._generate_correlation_insights(result)
            result['business_recommendations'] = self._generate_correlation_recommendations(result)
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise de correla√ß√£o: {str(e)}"}
    
    def _interpret_correlation_strength(self, correlation: float) -> str:
        """Interpretar for√ßa da correla√ß√£o."""
        if correlation >= 0.7:
            return "Muito forte"
        elif correlation >= 0.5:
            return "Forte"
        elif correlation >= 0.3:
            return "Moderada"
        elif correlation >= 0.1:
            return "Fraca"
        else:
            return "Muito fraca"
    
    def _interpret_effect_size(self, eta_squared: float) -> str:
        """Interpretar tamanho do efeito."""
        if eta_squared >= 0.14:
            return "Grande"
        elif eta_squared >= 0.06:
            return "M√©dio"
        elif eta_squared >= 0.01:
            return "Pequeno"
        else:
            return "Desprez√≠vel"
    
    def _generate_correlation_insights(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Gerar insights de correla√ß√£o."""
        insights = []
        
        if 'significant_correlations' in result:
            correlations = result['significant_correlations']
            if correlations:
                strongest = max(correlations.items(), key=lambda x: x[1])
                insights.append({
                    "type": "Correla√ß√£o Forte",
                    "message": f"Correla√ß√£o mais forte: {strongest[0]} ({strongest[1]:.3f})",
                    "impact": "high",
                    "recommendation": f"Investigar rela√ß√£o causal entre {strongest[0]} e vendas"
                })
                
        # Correla√ß√µes por categoria
        if 'categorical_analysis' in result:
            cat_analysis = result['categorical_analysis']
            significant_categories = [cat for cat, data in cat_analysis.items() 
                                   if data.get('significant', False)]
            if significant_categories:
                insights.append({
                    "type": "Impacto Categ√≥rico",
                    "message": f"Categorias com impacto significativo: {', '.join(significant_categories)}",
                    "impact": "medium",
                    "recommendation": "Focar estrat√©gias nestas categorias"
                })
        
        return insights
    
    def _generate_correlation_recommendations(self, result: Dict[str, Any]) -> List[str]:
        """Gerar recomenda√ß√µes baseadas em correla√ß√µes."""
        recommendations = []
        
        if 'correlation_tests' in result:
            for var, test in result['correlation_tests'].items():
                if test['significant'] and test['correlation'] > 0.5:
                    recommendations.append(f"Otimizar {var} para aumentar vendas (correla√ß√£o forte)")
        
        return recommendations[:5]
    
    def _multidimensional_clustering_analysis(self, df: pd.DataFrame, target_column: str = "Total_Liquido",
                                            clustering_method: str = "auto", **kwargs) -> Dict[str, Any]:
        """FASE 3: An√°lise de clustering multidimensional com cache otimizado."""
        
        # Verificar cache primeiro
        cache_key = f"clustering_{len(df)}_{target_column}_{clustering_method}"
        if cache_key in self._analysis_cache:
            print("üìã Usando resultado de clustering do cache")
            return self._analysis_cache[cache_key]
        
        try:
            print("üéØ Executando an√°lise de clustering multidimensional...")
            
            # Preparar dados com amostragem se necess√°rio
            if len(df) > 50000:
                print(f"üìä Dataset grande ({len(df)} registros) - usando amostra de 30.000 para clustering")
                df_sample = df.sample(n=30000, random_state=42)
            else:
                df_sample = df
            
            result = {
                'analysis_type': 'Multidimensional Clustering Analysis',
                'target_column': target_column,
                'method': clustering_method,
                'sample_size': len(df_sample),
                'original_size': len(df)
            }
            
            # Preparar dados para clustering
            feature_cols = self._select_clustering_features(df_sample, target_column)
            if len(feature_cols) < 2:
                return {'error': 'Insuficientes features num√©ricas para clustering'}
            
            X = df_sample[feature_cols].fillna(0)
            
            # Usar dados j√° escalados se dispon√≠veis, sen√£o escalar
            if any(col.endswith('_scaled') for col in feature_cols):
                X_scaled = X.values
                print("‚úÖ Usando dados pr√©-escalados")
            else:
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                print("‚öôÔ∏è Aplicando padroniza√ß√£o aos dados")
            
            # Escolher m√©todo automaticamente se necess√°rio
            if clustering_method == "auto":
                clustering_method = self._select_optimal_clustering_method(X_scaled)
                print(f"ü§ñ M√©todo autom√°tico selecionado: {clustering_method}")
            
            # Executar clustering
            if clustering_method == "kmeans":
                cluster_result = self._perform_kmeans_clustering(X_scaled, df_sample, feature_cols)
            elif clustering_method == "hierarchical":
                cluster_result = self._perform_hierarchical_clustering(X_scaled, df_sample, feature_cols)
            elif clustering_method == "dbscan":
                cluster_result = self._perform_dbscan_clustering(X_scaled, df_sample, feature_cols)
            else:
                return {'error': f"M√©todo de clustering '{clustering_method}' n√£o suportado"}
            
            result.update(cluster_result)
            
            # An√°lise dos clusters
            if 'cluster_labels' in result:
                df_clustered = df_sample.copy()
                df_clustered['Cluster'] = result['cluster_labels']
                
                # Perfil dos clusters
                cluster_profiles = {}
                for cluster_id in df_clustered['Cluster'].unique():
                    if cluster_id != -1:  # Ignorar outliers do DBSCAN
                        cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]
                        
                        profile = {
                            'size': len(cluster_data),
                            'size_percentage': round(len(cluster_data) / len(df_clustered) * 100, 1),
                            'avg_revenue': round(cluster_data[target_column].mean(), 2),
                            'total_revenue': round(cluster_data[target_column].sum(), 2),
                            'revenue_share': round(cluster_data[target_column].sum() / df_sample[target_column].sum() * 100, 1)
                        }
                        
                        # Caracter√≠sticas demogr√°ficas se dispon√≠veis
                        if 'Faixa_Etaria' in cluster_data.columns:
                            profile['predominant_age_group'] = cluster_data['Faixa_Etaria'].mode().iloc[0] if len(cluster_data['Faixa_Etaria'].mode()) > 0 else 'N/A'
                        
                        if 'Sexo' in cluster_data.columns:
                            profile['gender_distribution'] = cluster_data['Sexo'].value_counts().to_dict()
                        
                        # Produtos preferidos
                        if 'Grupo_Produto' in cluster_data.columns:
                            profile['preferred_products'] = cluster_data['Grupo_Produto'].value_counts().head(3).to_dict()
                        
                        cluster_profiles[f'Cluster_{cluster_id}'] = profile
                
                result['cluster_profiles'] = cluster_profiles
            
            # Insights de clustering
            result['insights'] = self._generate_clustering_insights(result)
            result['business_recommendations'] = self._generate_clustering_recommendations(result)
            
            # Salvar no cache
            self._analysis_cache[cache_key] = result
            print(f"üíæ Resultado salvo no cache (chave: {cache_key})")
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise de clustering: {str(e)}"}
    
    def _select_clustering_features(self, df: pd.DataFrame, target_column: str) -> List[str]:
        """Selecionar features otimizadas para clustering."""
        # Priorizar features escaladas se dispon√≠veis
        scaled_features = [col for col in df.columns if col.endswith('_scaled')]
        
        if scaled_features:
            print(f"üéØ Usando {len(scaled_features)} features escaladas para clustering")
            return scaled_features
        
        # Fallback para features num√©ricas b√°sicas
        numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
        exclude_cols = [target_column, 'Data', 'Codigo_Cliente', 'Codigo_Produto', 'Ano', 'Mes']
        selected_features = [col for col in numeric_features[:10] 
                           if col not in exclude_cols and not df[col].isna().all()]
        
        return selected_features
    
    def _select_optimal_clustering_method(self, X_scaled: np.ndarray) -> str:
        """Selecionar m√©todo de clustering automaticamente."""
        n_samples = X_scaled.shape[0]
        
        if n_samples < 50:
            return "hierarchical"
        elif n_samples > 1000:
            return "kmeans"
        else:
            return "kmeans"  # fallback seguro
    
    def _perform_kmeans_clustering(self, X_scaled: np.ndarray, df: pd.DataFrame, feature_cols: List[str]) -> Dict[str, Any]:
        """Executar clustering K-means."""
        optimal_k = min(5, len(X_scaled) // 10)  # Heur√≠stica simples
        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X_scaled)
        
        silhouette_avg = silhouette_score(X_scaled, cluster_labels) if len(set(cluster_labels)) > 1 else -1
        
        return {
            'method_used': 'K-Means',
            'optimal_k': optimal_k,
            'cluster_labels': cluster_labels,
            'silhouette_score': round(silhouette_avg, 3),
            'feature_names': feature_cols
        }
    
    def _perform_hierarchical_clustering(self, X_scaled: np.ndarray, df: pd.DataFrame, feature_cols: List[str]) -> Dict[str, Any]:
        """Executar clustering hier√°rquico."""
        linkage_matrix = linkage(X_scaled, method='ward')
        optimal_clusters = min(5, len(X_scaled) // 10)
        cluster_labels = fcluster(linkage_matrix, optimal_clusters, criterion='maxclust') - 1
        
        silhouette_avg = silhouette_score(X_scaled, cluster_labels) if len(set(cluster_labels)) > 1 else -1
        
        return {
            'method_used': 'Hierarchical',
            'optimal_clusters': optimal_clusters,
            'cluster_labels': cluster_labels,
            'silhouette_score': round(silhouette_avg, 3),
            'feature_names': feature_cols
        }
    
    def _perform_dbscan_clustering(self, X_scaled: np.ndarray, df: pd.DataFrame, feature_cols: List[str]) -> Dict[str, Any]:
        """Executar clustering DBSCAN."""
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        cluster_labels = dbscan.fit_predict(X_scaled)
        
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_outliers = list(cluster_labels).count(-1)
        
        silhouette_avg = silhouette_score(X_scaled, cluster_labels) if n_clusters > 1 else -1
        
        return {
            'method_used': 'DBSCAN',
            'cluster_labels': cluster_labels,
            'n_clusters': n_clusters,
            'n_outliers': n_outliers,
            'silhouette_score': round(silhouette_avg, 3),
            'feature_names': feature_cols
        }
    
    def _generate_clustering_insights(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Gerar insights de clustering."""
        insights = []
        
        if 'cluster_profiles' in result:
            profiles = result['cluster_profiles']
            
            # Cluster mais valioso
            revenue_clusters = {k: v.get('total_revenue', 0) for k, v in profiles.items()}
            if revenue_clusters:
                top_cluster = max(revenue_clusters, key=revenue_clusters.get)
                top_revenue_share = profiles[top_cluster].get('revenue_share', 0)
                insights.append({
                    "type": "Cluster Valioso",
                    "message": f"{top_cluster} gera {top_revenue_share}% da receita",
                    "impact": "high",
                    "recommendation": f"Focar estrat√©gias de reten√ß√£o no {top_cluster}"
                })
        
        return insights
    
    def _generate_clustering_recommendations(self, result: Dict[str, Any]) -> List[str]:
        """Gerar recomenda√ß√µes de clustering."""
        recommendations = []
        
        if 'cluster_profiles' in result:
            recommendations.append("Desenvolver estrat√©gias espec√≠ficas para cada cluster identificado")
            recommendations.append("Personalizar comunica√ß√£o baseada no perfil do cluster")
        
        return recommendations
    
    def _comprehensive_outlier_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de outliers usando m√∫ltiplos m√©todos."""
        try:
            print("üîç Executando an√°lise de outliers...")
            
            target_col = kwargs.get('target_column', 'Total_Liquido')
            if target_col not in df.columns:
                return {'error': f"Coluna {target_col} n√£o encontrada"}
            
            # M√©todo IQR
            Q1 = df[target_col].quantile(0.25)
            Q3 = df[target_col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers_iqr = df[(df[target_col] < lower_bound) | (df[target_col] > upper_bound)]
            
            # M√©todo Z-score
            z_scores = np.abs(stats.zscore(df[target_col].dropna()))
            outliers_zscore = df[z_scores > 3]
            
            result = {
                'analysis_type': 'Comprehensive Outlier Analysis',
                'target_column': target_col,
                'iqr_method': {
                    'outliers_count': len(outliers_iqr),
                    'outliers_percentage': round(len(outliers_iqr) / len(df) * 100, 2),
                    'lower_bound': round(lower_bound, 2),
                    'upper_bound': round(upper_bound, 2)
                },
                'zscore_method': {
                    'outliers_count': len(outliers_zscore),
                    'outliers_percentage': round(len(outliers_zscore) / len(df) * 100, 2)
                },
                'insights': [
                    {
                        "type": "Outliers Detectados",
                        "message": f"{len(outliers_iqr)} outliers identificados pelo m√©todo IQR",
                        "impact": "medium",
                        "recommendation": "Investigar transa√ß√µes an√¥malas para identificar oportunidades ou problemas"
                    }
                ]
            }
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise de outliers: {str(e)}"}
    
    def _advanced_distribution_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de distribui√ß√£o com testes de normalidade."""
        try:
            print("üìä Executando an√°lise de distribui√ß√£o...")
            
            target_col = kwargs.get('target_column', 'Total_Liquido')
            if target_col not in df.columns:
                return {'error': f"Coluna {target_col} n√£o encontrada"}
            
            data = df[target_col].dropna()
            
            # Teste de normalidade Shapiro-Wilk (para amostras pequenas)
            if len(data) <= 5000:
                shapiro_stat, shapiro_p = stats.shapiro(data.sample(min(5000, len(data))))
            else:
                shapiro_stat, shapiro_p = stats.shapiro(data.sample(5000))
            
            # Estat√≠sticas descritivas
            result = {
                'analysis_type': 'Advanced Distribution Analysis',
                'target_column': target_col,
                'descriptive_stats': {
                    'mean': round(data.mean(), 2),
                    'median': round(data.median(), 2),
                    'std': round(data.std(), 2),
                    'skewness': round(stats.skew(data), 3),
                    'kurtosis': round(stats.kurtosis(data), 3)
                },
                'normality_test': {
                    'shapiro_statistic': round(shapiro_stat, 4),
                    'shapiro_p_value': round(shapiro_p, 4),
                    'is_normal': shapiro_p > 0.05
                },
                'insights': [
                    {
                        "type": "Distribui√ß√£o",
                        "message": f"Dados {'seguem' if shapiro_p > 0.05 else 'n√£o seguem'} distribui√ß√£o normal",
                        "impact": "medium",
                        "recommendation": "Usar testes param√©tricos" if shapiro_p > 0.05 else "Usar testes n√£o-param√©tricos"
                    }
                ]
            }
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise de distribui√ß√£o: {str(e)}"}
    
    def _temporal_trend_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de tend√™ncia temporal."""
        try:
            print("üìà Executando an√°lise de tend√™ncia temporal...")
            
            # CORRE√á√ÉO: Inicializar result
            result = {
                'analysis_type': 'Temporal Trend Analysis',
                'target_column': kwargs.get('target_column', 'Total_Liquido')
            }
            
            if 'Data' not in df.columns:
                return {'error': 'Coluna Data n√£o encontrada'}
            
            target_col = kwargs.get('target_column', 'Total_Liquido')
            
            # Agregar por m√™s
            monthly_data = df.groupby(df['Data'].dt.to_period('M'))[target_col].sum()
            
            # Teste de tend√™ncia Mann-Kendall
            from scipy.stats import kendalltau
            x = range(len(monthly_data))
            tau, p_value = kendalltau(x, monthly_data.values)
            
            result['trend_test'] = {
                'kendall_tau': round(tau, 3),
                'p_value': round(p_value, 4),
                'has_trend': p_value < 0.05,
                'trend_direction': 'crescente' if tau > 0 else 'decrescente' if tau < 0 else 'est√°vel'
            }
            
            result['monthly_summary'] = {
                'periods': len(monthly_data),
                'avg_monthly': round(monthly_data.mean(), 2),
                'growth_rate': round(monthly_data.pct_change().mean() * 100, 2)
            }
            
            result['insights'] = [
                {
                    "type": "Tend√™ncia Temporal",
                    "message": f"Tend√™ncia {result['trend_test']['trend_direction']} detectada",
                    "impact": "high" if p_value < 0.05 else "low",
                    "recommendation": "Ajustar estrat√©gias baseado na tend√™ncia identificada"
                }
            ]
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise temporal: {str(e)}"}
    
    def _demographic_patterns_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de padr√µes demogr√°ficos."""
        try:
            print("üë• Executando an√°lise demogr√°fica...")
            
            target_col = kwargs.get('target_column', 'Total_Liquido')
            
            result = {
                'analysis_type': 'Demographic Patterns Analysis',
                'target_column': target_col
            }
            
            # An√°lise por sexo
            if 'Sexo' in df.columns:
                gender_stats = df.groupby('Sexo')[target_col].agg(['count', 'mean', 'sum']).round(2)
                result['gender_analysis'] = gender_stats.to_dict()
            
            # An√°lise por faixa et√°ria
            if 'Faixa_Etaria' in df.columns:
                age_stats = df.groupby('Faixa_Etaria')[target_col].agg(['count', 'mean', 'sum']).round(2)
                result['age_analysis'] = age_stats.to_dict()
            
            result['insights'] = [
                {
                    "type": "Demografia",
                    "message": "Padr√µes demogr√°ficos identificados",
                    "impact": "medium",
                    "recommendation": "Personalizar ofertas por segmento demogr√°fico"
                }
            ]
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise demogr√°fica: {str(e)}"}
    
    def _generational_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise geracional IMPLEMENTADA."""
        try:
            print("üë• Executando an√°lise geracional...")
            
            result = {
                'analysis_type': 'Generational Analysis',
                'target_column': kwargs.get('target_column', 'Total_Liquido')
            }
            
            if 'Geracao' in df.columns:
                gen_stats = df.groupby('Geracao')[kwargs.get('target_column', 'Total_Liquido')].agg(['count', 'mean', 'sum']).round(2)
                result['generational_analysis'] = gen_stats.to_dict()
                
                # Gera√ß√£o mais valiosa
                top_generation = gen_stats['sum'].idxmax()
                result['insights'] = [
                    {
                        "type": "Gera√ß√£o Dominante",
                        "message": f"Gera√ß√£o {top_generation} √© a mais valiosa em receita",
                        "impact": "medium",
                        "recommendation": f"Focar estrat√©gias na gera√ß√£o {top_generation}"
                    }
                ]
            else:
                result['insights'] = [
                    {
                        "type": "Dados Insuficientes",
                        "message": "Campo 'Geracao' n√£o encontrado para an√°lise geracional",
                        "impact": "low",
                        "recommendation": "Implementar classifica√ß√£o geracional baseada em idade"
                    }
                ]
            
            return result
            
        except Exception as e:
            return {'error': f"Erro na an√°lise geracional: {str(e)}"}
    
    def _behavioral_customer_segmentation(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """Segmenta√ß√£o comportamental de clientes."""
        return {'message': 'Segmenta√ß√£o comportamental em desenvolvimento', 'status': 'placeholder'}
    
    def _geographic_performance_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de performance geogr√°fica."""
        return {'message': 'An√°lise geogr√°fica em desenvolvimento', 'status': 'placeholder'}
    
    def _regional_patterns_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de padr√µes regionais."""
        return {'message': 'An√°lise de padr√µes regionais em desenvolvimento', 'status': 'placeholder'}
    
    def _price_elasticity_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de elasticidade de pre√ßos."""
        return {'message': 'An√°lise de elasticidade em desenvolvimento', 'status': 'placeholder'}
    
    def _profitability_pattern_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise de padr√µes de rentabilidade."""
        return {'message': 'An√°lise de rentabilidade em desenvolvimento', 'status': 'placeholder'}
    
    def _comprehensive_customer_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise abrangente de clientes."""
        return {'message': 'An√°lise abrangente de clientes em desenvolvimento', 'status': 'placeholder'}
    
    def _statistical_product_analysis(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """An√°lise estat√≠stica de produtos."""
        return {'message': 'An√°lise estat√≠stica de produtos em desenvolvimento', 'status': 'placeholder'}

    def generate_statistical_visual_report(self, test_data: dict) -> str:
        """Gera relat√≥rio visual completo dos testes estat√≠sticos em formato markdown."""
        
        # Coletar dados com fallbacks
        metadata = test_data.get('metadata', {})
        data_metrics = test_data.get('data_metrics', {})
        results = test_data.get('results', {})
        component_tests = test_data.get('component_tests', {})
        
        report = [
            "# üî¨ Teste Completo de An√°lises Estat√≠sticas - Relat√≥rio Executivo",
            f"**Data do Teste:** {metadata.get('test_timestamp', 'N/A')}",
            f"**Fonte de Dados:** `{metadata.get('data_source', 'desconhecida')}`",
            f"**Registros Analisados:** {data_metrics.get('total_records', 0):,}",
            f"**Per√≠odo de An√°lise:** {data_metrics.get('date_range', {}).get('start', 'N/A')} at√© {data_metrics.get('date_range', {}).get('end', 'N/A')}",
            "\n## üìà Performance de Execu√ß√£o",
            f"```\n{json.dumps(test_data.get('performance_metrics', {}), indent=2)}\n```",
            "\n## üéØ Resumo dos Testes Executados"
        ]
        
        # Contabilizar sucessos e falhas
        successful_tests = len([r for r in results.values() if 'error' not in r])
        failed_tests = len([r for r in results.values() if 'error' in r])
        total_tests = len(results)
        
        report.extend([
            f"- **Total de An√°lises:** {total_tests}",
            f"- **Sucessos:** {successful_tests} ‚úÖ",
            f"- **Falhas:** {failed_tests} ‚ùå",
            f"- **Taxa de Sucesso:** {(successful_tests/total_tests*100):.1f}%" if total_tests > 0 else "- **Taxa de Sucesso:** N/A"
        ])
        
        # Insights Estat√≠sticos Principais
        report.append("\n## üîç Principais Descobertas Estat√≠sticas")
        
        # Correla√ß√µes significativas
        if 'correlation' in results and 'error' not in results['correlation']:
            corr_data = results['correlation']
            if 'significant_correlations' in corr_data:
                sig_corr = corr_data['significant_correlations']
                if sig_corr:
                    strongest = max(sig_corr.items(), key=lambda x: abs(x[1]))
                    report.append(f"- **Correla√ß√£o Mais Forte:** {strongest[0]} ({strongest[1]:.3f})")
        
        # Clusters identificados
        if 'clustering' in results and 'error' not in results['clustering']:
            cluster_data = results['clustering']
            if 'cluster_profiles' in cluster_data:
                n_clusters = len(cluster_data['cluster_profiles'])
                report.append(f"- **Clusters Identificados:** {n_clusters} segmentos distintos")
        
        # Outliers detectados
        if 'outliers' in results and 'error' not in results['outliers']:
            outlier_data = results['outliers']
            if 'iqr_method' in outlier_data:
                outlier_count = outlier_data['iqr_method'].get('outliers_count', 0)
                report.append(f"- **Outliers Detectados:** {outlier_count} transa√ß√µes an√¥malas")
        
        # Testes de Normalidade
        if 'distribution' in results and 'error' not in results['distribution']:
            dist_data = results['distribution']
            if 'normality_test' in dist_data:
                is_normal = dist_data['normality_test'].get('is_normal', False)
                report.append(f"- **Distribui√ß√£o dos Dados:** {'Normal' if is_normal else 'N√£o-Normal'}")
        
        # An√°lise Temporal
        if 'trend_analysis' in results and 'error' not in results['trend_analysis']:
            trend_data = results['trend_analysis']
            if 'trend_test' in trend_data:
                trend_dir = trend_data['trend_test'].get('trend_direction', 'N/A')
                report.append(f"- **Tend√™ncia Temporal:** {trend_dir.title()}")
        
        # Detalhamento por Tipo de An√°lise
        report.append("\n## üìä Detalhamento das An√°lises")
        
        analysis_categories = {
            'An√°lises Estat√≠sticas Core': ['correlation', 'clustering', 'outliers', 'distribution', 'trend_analysis'],
            'An√°lises Demogr√°ficas': ['demographic_patterns', 'generational_analysis', 'customer_segmentation'],
            'An√°lises Geogr√°ficas': ['geographic_performance', 'regional_patterns'],
            'An√°lises Especializadas': ['price_sensitivity', 'profitability_patterns'],
            'An√°lises Integradas': ['comprehensive_customer_analysis', 'product_performance_analysis']
        }
        
        for category, analyses in analysis_categories.items():
            report.append(f"\n### {category}")
            for analysis in analyses:
                if analysis in results:
                    if 'error' in results[analysis]:
                        report.append(f"- ‚ùå **{analysis}**: {results[analysis]['error']}")
                    else:
                        # Resumir insights principais de cada an√°lise
                        insights = results[analysis].get('insights', [])
                        if insights:
                            report.append(f"- ‚úÖ **{analysis}**: {len(insights)} insights gerados")
                            for insight in insights[:2]:  # Top 2 insights
                                report.append(f"  - {insight.get('message', 'N/A')}")
                        else:
                            report.append(f"- ‚úÖ **{analysis}**: Conclu√≠do")
                else:
                    report.append(f"- ‚è≠Ô∏è **{analysis}**: N√£o testado")
        
        # Recomenda√ß√µes Baseadas em Evid√™ncias Estat√≠sticas
        report.append("\n## üí° Recomenda√ß√µes Baseadas em Evid√™ncias")
        
        all_insights = []
        for result in results.values():
            if 'insights' in result and isinstance(result['insights'], list):
                all_insights.extend(result['insights'])
        
        # Agrupar recomenda√ß√µes por impacto
        high_impact = [i for i in all_insights if i.get('impact') == 'high']
        medium_impact = [i for i in all_insights if i.get('impact') == 'medium']
        
        if high_impact:
            report.append("\n### üî• Alta Prioridade")
            for insight in high_impact[:3]:
                report.append(f"- {insight.get('recommendation', insight.get('message', 'N/A'))}")
        
        if medium_impact:
            report.append("\n### üìà M√©dia Prioridade")
            for insight in medium_impact[:3]:
                report.append(f"- {insight.get('recommendation', insight.get('message', 'N/A'))}")
        
        # Qualidade dos Dados e Limita√ß√µes
        report.append("\n## ‚ö†Ô∏è Limita√ß√µes e Considera√ß√µes")
        
        data_quality = data_metrics.get('data_quality_check', {})
        if data_quality:
            report.append("### Qualidade dos Dados:")
            for check, value in data_quality.items():
                if value > 0:
                    report.append(f"- **{check}**: {value} ocorr√™ncias")
        
        # Erros encontrados
        errors = test_data.get('errors', [])
        if errors:
            report.append(f"\n### Erros Detectados ({len(errors)}):")
            for error in errors[-3:]:  # √öltimos 3 erros
                report.append(f"- **{error['context']}**: {error['error_message']}")
        
        return "\n".join(report)

    def run_full_statistical_test(self) -> str:
        """Executa teste completo e retorna relat√≥rio formatado"""
        test_result = self.test_all_statistical_analyses()
        parsed = json.loads(test_result)
        return self.generate_statistical_visual_report(parsed)

    def test_all_statistical_analyses(self, sample_data: str = "data/vendas.csv") -> str:
        """
        Executa teste completo de todas as an√°lises estat√≠sticas da classe
        """
        
        # Corrigir caminho do arquivo
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.abspath(os.path.join(current_dir, "..", "..", ".."))
        data_file_path = os.path.join(project_root, sample_data)
        
        # Verificar se arquivo existe
        if not os.path.exists(data_file_path):
            return json.dumps({
                "error": f"Arquivo n√£o encontrado: {data_file_path}",
                "current_dir": current_dir,
                "project_root": project_root,
                "expected_path": data_file_path
            }, indent=2)

        test_report = {
            "metadata": {
                "test_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "test_version": "Statistical Analysis Test Suite v1.0",
                "data_source": data_file_path,
                "tool_version": "Statistical Analysis Tool v3.0",
                "status": "in_progress"
            },
            "data_metrics": {
                "total_records": 0,
                "columns": [],
                "date_range": {},
                "data_quality_check": {}
            },
            "results": {},
            "component_tests": {},
            "performance_metrics": {},
            "errors": []
        }

        try:
            # 1. Fase de Carregamento de Dados (UMA VEZ S√ì)
            test_report["metadata"]["current_stage"] = "data_loading"
            print("\n=== ETAPA 1: CARREGAMENTO DE DADOS ESTAT√çSTICOS ===")
            print(f"üìÅ Tentando carregar: {data_file_path}")
            
            # Carregar dados UMA VEZ e reutilizar
            df = self._load_and_prepare_statistical_data(data_file_path, use_cache=True)
            
            if df is None:
                raise Exception("Falha no carregamento dos dados estat√≠sticos")
            
            print(f"‚úÖ Dados carregados GLOBALMENTE: {len(df)} registros")
            
            # Coletar m√©tricas b√°sicas dos dados
            test_report["data_metrics"] = {
                "total_records": int(len(df)),
                "columns": list(df.columns),
                "date_range": {
                    "start": str(df['Data'].min()) if 'Data' in df.columns else "N/A",
                    "end": str(df['Data'].max()) if 'Data' in df.columns else "N/A"
                },
                "data_quality_check": self._convert_to_native_types(self._perform_statistical_data_quality_check(df))
            }

            # 2. Teste de Todas as An√°lises Estat√≠sticas (REUTILIZANDO DADOS)
            test_report["metadata"]["current_stage"] = "statistical_testing"
            print("\n=== ETAPA 2: TESTE DE AN√ÅLISES ESTAT√çSTICAS ===")
            
            # Definir todas as an√°lises dispon√≠veis
            statistical_analyses = [
                'correlation',
                'clustering', 
                'outliers',
                'distribution',
                'trend_analysis',
                'demographic_patterns',
                'generational_analysis',
                'customer_segmentation',
                'geographic_performance',
                'regional_patterns',
                'price_sensitivity',
                'profitability_patterns',
                'comprehensive_customer_analysis',
                'product_performance_analysis'
            ]
            
            for analysis_type in statistical_analyses:
                try:
                    print(f"\nüî¨ TESTANDO AN√ÅLISE: {analysis_type.upper()}")
                    start_time = time.time()
                    
                    # CORRE√á√ÉO: Usar dados j√° carregados ao inv√©s de recarregar
                    result = self._run_analysis_with_prepared_data(
                        df=df,  # USAR DADOS PREPARADOS
                        analysis_type=analysis_type,
                        target_column="Total_Liquido",
                        significance_level=0.05,
                        clustering_method="auto",
                        min_correlation=0.3,
                        demographic_focus=True,
                        geographic_focus=True
                    )
                    
                    # An√°lise de resultados
                    parsed_result = json.loads(result)
                    test_report["results"][analysis_type] = parsed_result
                    
                    execution_time = time.time() - start_time
                    
                    # Verifica√ß√£o b√°sica de integridade
                    if 'error' in parsed_result:
                        print(f"‚ùå {analysis_type.upper()} - Erro: {parsed_result['error']}")
                    else:
                        insights_count = len(parsed_result.get('insights', []))
                        print(f"‚úÖ {analysis_type.upper()} - {insights_count} insights gerados ({execution_time:.2f}s)")

                except Exception as e:
                    error_id = f"ERR-{analysis_type.upper()}-{datetime.now().strftime('%H%M%S')}"
                    self._log_statistical_test_error(test_report, e, analysis_type)
                    print(f"‚õî Erro em {analysis_type.upper()} - {error_id}: {str(e)}")

            # 3. Teste de Componentes de Cache e Otimiza√ß√£o
            test_report["metadata"]["current_stage"] = "optimization_testing"
            print("\n=== ETAPA 3: TESTE DE OTIMIZA√á√ïES ===")
            
            try:
                print("üîß Testando cache de an√°lises...")
                # Teste com cache
                start_time = time.time()
                result_with_cache = self._run(
                    analysis_type="correlation",
                    data_csv=data_file_path,
                    cache_results=True
                )
                cache_time = time.time() - start_time
                
                # Teste sem cache
                start_time = time.time()
                result_without_cache = self._run(
                    analysis_type="correlation", 
                    data_csv=data_file_path,
                    cache_results=False
                )
                no_cache_time = time.time() - start_time
                
                test_report["component_tests"]["cache_performance"] = {
                    "cache_enabled_time": round(cache_time, 3),
                    "cache_disabled_time": round(no_cache_time, 3),
                    "cache_efficiency": round((no_cache_time - cache_time) / no_cache_time * 100, 1) if no_cache_time > 0 else 0
                }
                print("‚úÖ Cache performance - OK")
                
            except Exception as e:
                self._log_statistical_test_error(test_report, e, "cache_test")
                print(f"‚ùå Cache test - Falha: {str(e)}")

            try:
                print("üîß Testando amostragem para datasets grandes...")
                # Simular dataset grande com amostragem
                result_sampled = self._run(
                    analysis_type="clustering",
                    data_csv=data_file_path,
                    sample_size=1000  # For√ßar amostragem
                )
                
                parsed_sampled = json.loads(result_sampled)
                test_report["component_tests"]["sampling"] = {
                    "sample_size_used": parsed_sampled.get('metadata', {}).get('total_records', 0),
                    "sampling_successful": 'error' not in parsed_sampled
                }
                print("‚úÖ Sampling test - OK")
                
            except Exception as e:
                self._log_statistical_test_error(test_report, e, "sampling_test")
                print(f"‚ùå Sampling test - Falha: {str(e)}")

            # 4. Teste de Performance com An√°lise Complexa
            test_report["metadata"]["current_stage"] = "performance_testing"
            print("\n=== ETAPA 4: TESTE DE PERFORMANCE ===")
            try:
                start_time = time.time()
                
                # CORRE√á√ÉO: Remover par√¢metro inexistente
                complex_test = self._run(
                    analysis_type="clustering",
                    data_csv=data_file_path,
                    clustering_method="auto",
                    demographic_focus=True,
                    geographic_focus=True,
                    cache_results=True  # SUBSTITUIR include_statistical_insights
                )
                
                test_report["performance_metrics"] = {
                    "complex_analysis_time_seconds": round(time.time() - start_time, 2),
                    "result_size_kb": round(len(complex_test)/1024, 2),
                    "memory_usage_mb": round(self._get_statistical_memory_usage(), 2),
                    "cache_size": len(self._analysis_cache)
                }
                print("‚úÖ Performance test conclu√≠do")
                
            except Exception as e:
                self._log_statistical_test_error(test_report, e, "performance_test")
                print(f"‚ùå Performance test falhou: {str(e)}")

            # 5. An√°lise Final
            test_report["metadata"]["status"] = "completed" if not test_report["errors"] else "completed_with_errors"
            print(f"\n‚úÖ‚úÖ‚úÖ TESTE ESTAT√çSTICO COMPLETO - {len(test_report['errors'])} erros ‚úÖ‚úÖ‚úÖ")
            
            return json.dumps(test_report, ensure_ascii=False, indent=2, default=str)

        except Exception as e:
            test_report["metadata"]["status"] = "failed"
            self._log_statistical_test_error(test_report, e, "global")
            print(f"‚ùå TESTE ESTAT√çSTICO FALHOU: {str(e)}")
            return json.dumps(test_report, ensure_ascii=False, indent=2, default=str)

    def _get_analysis_specific_params(self, analysis_type: str) -> dict:
        """Retorna par√¢metros espec√≠ficos para cada tipo de an√°lise"""
        params_map = {
            'correlation': {'min_correlation': 0.3, 'significance_level': 0.05},
            'clustering': {'clustering_method': 'auto'},
            'outliers': {'target_column': 'Total_Liquido'},
            'distribution': {'target_column': 'Total_Liquido'},
            'trend_analysis': {'target_column': 'Total_Liquido'},
            'demographic_patterns': {'demographic_focus': True},
            'geographic_performance': {'geographic_focus': True},
            'price_sensitivity': {'target_column': 'Total_Liquido'},
        }
        return params_map.get(analysis_type, {})

    def _log_statistical_test_error(self, report: dict, error: Exception, context: str) -> None:
        """Registra erros de teste estat√≠stico de forma estruturada"""
        error_entry = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "context": context,
            "error_type": type(error).__name__,
            "error_message": str(error),
            "traceback": traceback.format_exc()
        }
        report["errors"].append(error_entry)

    def _convert_to_native_types(self, obj):
        """Converte tipos numpy/pandas para tipos nativos Python."""
        if isinstance(obj, dict):
            return {k: self._convert_to_native_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_native_types(v) for v in obj]
        elif hasattr(obj, 'item'):  # numpy types
            return obj.item()
        elif hasattr(obj, 'tolist'):  # numpy arrays
            return obj.tolist()
        else:
            return obj

    def _perform_statistical_data_quality_check(self, df: pd.DataFrame) -> dict:
        """Executa verifica√ß√µes de qualidade espec√≠ficas para an√°lises estat√≠sticas"""
        checks = {
            "missing_values_total": int(df.isnull().sum().sum()),
            "duplicate_records": int(df.duplicated().sum()),
            "numerical_columns": len(df.select_dtypes(include=[np.number]).columns),
            "categorical_columns": len(df.select_dtypes(include=['object']).columns),
            "zero_variance_columns": int((df.var(numeric_only=True) == 0).sum()),
            "outliers_iqr_total": self._count_total_outliers_iqr(df)
        }
        return checks

    def _count_total_outliers_iqr(self, df: pd.DataFrame) -> int:
        """Conta outliers totais usando m√©todo IQR para todas as colunas num√©ricas"""
        try:
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            total_outliers = 0
            
            for col in numeric_cols:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
                total_outliers += outliers
                
            return int(total_outliers)
        except:
            return 0

    def _get_statistical_memory_usage(self) -> float:
        """Obt√©m uso de mem√≥ria espec√≠fico para an√°lises estat√≠sticas"""
        try:
            import psutil
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024 / 1024  # Em MB
        except:
            return 0.0

    def _run_analysis_with_prepared_data(self, df: pd.DataFrame, analysis_type: str, **kwargs) -> str:
        """Executa an√°lise usando dados j√° preparados (otimiza√ß√£o para testes)."""
        try:
            print(f"üöÄ Executando {analysis_type} com dados pr√©-carregados...")
            
            # Mapear an√°lises
            analysis_methods = {
                'correlation': self._advanced_correlation_analysis,
                'clustering': self._multidimensional_clustering_analysis,
                'outliers': self._comprehensive_outlier_analysis,
                'distribution': self._advanced_distribution_analysis,
                'trend_analysis': self._temporal_trend_analysis,
                'demographic_patterns': self._demographic_patterns_analysis,
                'generational_analysis': self._generational_analysis,
                'customer_segmentation': self._behavioral_customer_segmentation,
                'geographic_performance': self._geographic_performance_analysis,
                'regional_patterns': self._regional_patterns_analysis,
                'price_sensitivity': self._price_elasticity_analysis,
                'profitability_patterns': self._profitability_pattern_analysis,
                'comprehensive_customer_analysis': self._comprehensive_customer_analysis,
                'product_performance_analysis': self._statistical_product_analysis
            }
            
            if analysis_type not in analysis_methods:
                return json.dumps({'error': f"An√°lise '{analysis_type}' n√£o suportada"})
            
            # Executar an√°lise diretamente
            result = analysis_methods[analysis_type](df, **kwargs)
            
            # Adicionar metadados
            result['metadata'] = {
                'tool': 'Statistical Analysis Tool v3.0',
                'analysis_type': analysis_type,
                'total_records': len(df),
                'generated_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            return json.dumps(result, ensure_ascii=False, indent=2, default=str)
            
        except Exception as e:
            return json.dumps({'error': f"Erro na an√°lise {analysis_type}: {str(e)}"})

# Exemplo de uso
if __name__ == "__main__":
    analyzer = StatisticalAnalysisTool()
    
    print("üî¨ Iniciando Teste Completo de An√°lises Estat√≠sticas...")
    report = analyzer.run_full_statistical_test()
    
    # Salvar relat√≥rio
    os.makedirs("test_results", exist_ok=True)
    with open("test_results/statistical_analysis_test_report.md", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("‚úÖ Relat√≥rio estat√≠stico gerado em test_results/statistical_analysis_test_report.md")
    print("\n" + "="*80)
    print(report[:1500])  # Exibir parte do relat√≥rio no console 