#!/usr/bin/env python
"""
âš¡ MAIN OTIMIZADO - INSIGHTS-AI (UNIFICADO)
==========================================

Script principal unificado para executar o Insights-AI com otimizaÃ§Ãµes de performance
e funcionalidades selecionadas da versÃ£o enhanced v2:

FUNCIONALIDADES PRINCIPAIS:
- Logging estruturado menos verbose
- Lazy loading de ferramentas
- Cache inteligente de validaÃ§Ãµes
- MÃ©tricas de performance em tempo real
- ConfiguraÃ§Ã£o automÃ¡tica por ambiente

FUNCIONALIDADES ADICIONAIS (do v2):
- Log file management bÃ¡sico
- RelatÃ³rios de ferramentas opcionais
- ValidaÃ§Ã£o de arquivos gerados
- ConsolidaÃ§Ã£o bÃ¡sica de logs repetitivos
- PerÃ­odo padrÃ£o otimizado (90 dias)

Uso:
    python main_optimized.py                    # Ãšltimos 90 dias
    python main_optimized.py --days 60          # Ãšltimos 60 dias
    python main_optimized.py --start 2024-01-01 --end 2024-12-31
    python main_optimized.py --debug            # Modo debug
    python main_optimized.py --minimal          # Logs mÃ­nimos
    python main_optimized.py --tools-report     # RelatÃ³rio de ferramentas
    python main_optimized.py --validate-files   # Validar arquivos gerados
"""

import os
import sys
import argparse
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict

# Adicionar src ao Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

# =============== SISTEMA DE LOG CONSOLIDATION BÃSICO ===============

class BasicLogConsolidator:
    """Consolidador bÃ¡sico de logs repetitivos (versÃ£o simplificada do v2)"""
    
    def __init__(self):
        self.message_counts = defaultdict(int)
        self.last_log_time = {}
        self.consolidation_threshold = 5  # Consolidar apÃ³s 5 repetiÃ§Ãµes
        self.time_window = 30  # 30 segundos
    
    def should_log(self, message: str) -> tuple[bool, str]:
        """Determinar se deve fazer log ou consolidar"""
        current_time = time.time()
        
        # Normalizar mensagem para padrÃ£o
        pattern = self._normalize_message(message)
        
        self.message_counts[pattern] += 1
        count = self.message_counts[pattern]
        
        # Se Ã© primeira vez ou passou do threshold
        if count == 1:
            self.last_log_time[pattern] = current_time
            return True, message
        elif count == self.consolidation_threshold:
            return True, f"[CONSOLIDADO] {message} (repetido {count}x)"
        elif count > self.consolidation_threshold:
            # Log apenas a cada 10 repetiÃ§Ãµes apÃ³s o threshold
            if count % 10 == 0:
                return True, f"[CONSOLIDADO] {message} (repetido {count}x)"
            return False, ""
        else:
            return True, message
    
    def _normalize_message(self, message: str) -> str:
        """Normalizar mensagem removendo partes dinÃ¢micas"""
        import re
        # Remover nÃºmeros, timestamps, IDs dinÃ¢micos
        normalized = re.sub(r'\d+', 'N', message)
        normalized = re.sub(r'\d{2}:\d{2}:\d{2}', 'HH:MM:SS', normalized)
        return normalized
    
    def get_stats(self) -> dict:
        """Obter estatÃ­sticas de consolidaÃ§Ã£o"""
        total_messages = sum(self.message_counts.values())
        unique_patterns = len(self.message_counts)
        suppressed = total_messages - unique_patterns
        return {
            'total_messages': total_messages,
            'unique_patterns': unique_patterns,
            'suppressed_messages': suppressed
        }

# =============== ENHANCED LOGGER SIMPLES ===============

class SimpleEnhancedLogger:
    """Logger simples com funcionalidades bÃ¡sicas do enhanced v2"""
    
    def __init__(self):
        self.consolidator = BasicLogConsolidator()
        self.log_file_path = None
        self.enable_consolidation = True
        self.performance_metrics = {
            'operations': {},
            'start_time': time.time()
        }
        self._setup_file_logging()
    
    def _setup_file_logging(self):
        """Configurar logging em arquivo (versÃ£o simplificada)"""
        try:
            log_dir = Path("logs/optimized")
            log_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d")
            self.log_file_path = log_dir / f"insights_optimized_{timestamp}.log"
            
            # Configurar handler de arquivo
            file_handler = logging.FileHandler(self.log_file_path, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            
            formatter = logging.Formatter(
                '%(asctime)s [%(levelname)s] %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            file_handler.setFormatter(formatter)
            
            # Adicionar ao logger principal
            logger = logging.getLogger()
            logger.addHandler(file_handler)
            logger.setLevel(logging.INFO)
            
        except Exception as e:
            print(f"âš ï¸ NÃ£o foi possÃ­vel configurar log em arquivo: {e}")
    
    def _log_with_consolidation(self, level: str, message: str):
        """Log com consolidaÃ§Ã£o bÃ¡sica"""
        if self.enable_consolidation:
            should_log, final_message = self.consolidator.should_log(message)
            if should_log and final_message:
                if level.upper() == 'INFO':
                    print(final_message)
                elif level.upper() == 'WARNING':
                    print(f"âš ï¸ {final_message}")
                elif level.upper() == 'ERROR':
                    print(f"âŒ {final_message}")
                elif level.upper() == 'DEBUG':
                    if os.getenv('INSIGHTS_DEBUG', 'false').lower() == 'true':
                        print(f"ğŸ› {final_message}")
                
                # Log em arquivo sempre
                if self.log_file_path:
                    logging.getLogger().log(getattr(logging, level.upper()), final_message)
        else:
            # Log direto sem consolidaÃ§Ã£o
            print(message)
            if self.log_file_path:
                logging.getLogger().log(getattr(logging, level.upper()), message)
    
    def info(self, message: str):
        self._log_with_consolidation('INFO', message)
    
    def warning(self, message: str):
        self._log_with_consolidation('WARNING', message)
    
    def error(self, message: str):
        self._log_with_consolidation('ERROR', message)
    
    def debug(self, message: str):
        self._log_with_consolidation('DEBUG', message)
    
    def log_milestone(self, title: str, data: dict):
        """Log milestone simplificado"""
        self.info(f"ğŸ¯ {title}")
        for key, value in data.items():
            self.info(f"   ğŸ“Š {key}: {value}")
    
    def track_operation(self, operation: str, duration: float):
        """Track operation performance"""
        if operation not in self.performance_metrics['operations']:
            self.performance_metrics['operations'][operation] = []
        self.performance_metrics['operations'][operation].append(duration)
    
    def get_performance_summary(self) -> dict:
        """Obter resumo de performance"""
        total_time = time.time() - self.performance_metrics['start_time']
        consolidation_stats = self.consolidator.get_stats()
        
        return {
            'total_execution_time': total_time,
            'operations_tracked': len(self.performance_metrics['operations']),
            'log_file': str(self.log_file_path) if self.log_file_path else None,
            'consolidation': consolidation_stats
        }

# InstÃ¢ncia global do logger
enhanced_logger = SimpleEnhancedLogger()

# =============== CONFIGURAÃ‡ÃƒO DE ENVIRONMENT ===============

def setup_environment(args):
    """Configurar ambiente baseado nos argumentos"""
    
    # Configurar nÃ­vel de logging
    if args.debug:
        os.environ['INSIGHTS_DEBUG'] = 'true'
        enhanced_logger.info("ğŸ› Modo DEBUG ativado")
    elif args.minimal:
        os.environ['INSIGHTS_LOG_LEVEL'] = 'MINIMAL'
        enhanced_logger.info("ğŸ”‡ Modo MINIMAL ativado")
    elif args.verbose:
        os.environ['INSIGHTS_LOG_LEVEL'] = 'VERBOSE'
        enhanced_logger.info("ğŸ“¢ Modo VERBOSE ativado")
    
    # Configurar environment
    if args.production:
        os.environ['ENVIRONMENT'] = 'production'
        enhanced_logger.info("ğŸ­ Modo PRODUÃ‡ÃƒO ativado")
    
    # Configurar cache
    if args.no_cache:
        os.environ['INSIGHTS_DISABLE_CACHE'] = 'true'
        enhanced_logger.info("ğŸš« Cache DESABILITADO")
    
    # ConfiguraÃ§Ãµes do v2
    if args.no_consolidation:
        enhanced_logger.enable_consolidation = False
        enhanced_logger.info("ğŸ“ ConsolidaÃ§Ã£o de logs DESABILITADA")
    
    if args.tools_report:
        enhanced_logger.info("ğŸ”§ RelatÃ³rio de ferramentas ATIVADO")

# =============== VALIDAÃ‡ÃƒO DE DATAS ===============

def validate_dates(data_inicio: str, data_fim: str):
    """Validar formato e lÃ³gica das datas"""
    
    try:
        start_date = datetime.strptime(data_inicio, '%Y-%m-%d')
        end_date = datetime.strptime(data_fim, '%Y-%m-%d')
        
        if start_date > end_date:
            raise ValueError("Data inÃ­cio nÃ£o pode ser posterior Ã  data fim")
        
        if end_date > datetime.now():
            print("âš ï¸ Data fim estÃ¡ no futuro - usando dados disponÃ­veis")
        
        days_diff = (end_date - start_date).days
        if days_diff > 365:
            print(f"âš ï¸ PerÃ­odo muito longo ({days_diff} dias) - pode impactar performance")
        
        return True
        
    except ValueError as e:
        print(f"âŒ Erro na data: {e}")
        return False

# =============== CÃLCULO DE DATAS ===============

def calculate_dates(args):
    """Calcular datas baseado nos argumentos"""
    
    if args.start and args.end:
        # Datas especÃ­ficas
        return args.start, args.end
    
    elif args.days:
        # Ãšltimos N dias
        end_date = datetime.now()
        start_date = end_date - timedelta(days=args.days)
        return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')
    
    else:
        # PadrÃ£o: 90 dias (otimizado do v2)
        end_date = datetime.now()
        start_date = end_date - timedelta(days=90)
        return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')

# =============== EXIBIÃ‡ÃƒO DE INFORMAÃ‡Ã•ES ===============

def show_execution_info(data_inicio: str, data_fim: str, args):
    """Exibir informaÃ§Ãµes da execuÃ§Ã£o"""
    
    enhanced_logger.info("="*60)
    enhanced_logger.info("âš¡ INSIGHTS-AI OTIMIZADO (UNIFICADO)")
    enhanced_logger.info("="*60)
    
    # PerÃ­odo
    start_date = datetime.strptime(data_inicio, '%Y-%m-%d')
    end_date = datetime.strptime(data_fim, '%Y-%m-%d')
    days = (end_date - start_date).days
    
    # ConfiguraÃ§Ãµes
    config_info = []
    if args.debug:
        config_info.append("DEBUG")
    elif args.minimal:
        config_info.append("MINIMAL") 
    elif args.verbose:
        config_info.append("VERBOSE")
    else:
        config_info.append("NORMAL")
        
    if args.production:
        config_info.append("PRODUÃ‡ÃƒO")
    else:
        config_info.append("DESENVOLVIMENTO")
        
    if args.no_cache:
        config_info.append("SEM CACHE")
    else:
        config_info.append("COM CACHE")
    
    # Features do v2
    v2_features = []
    if not args.no_consolidation:
        v2_features.append("CONSOLIDAÃ‡ÃƒO")
    if args.tools_report:
        v2_features.append("TOOLS_REPORT")
    if args.validate_files:
        v2_features.append("VALIDAÃ‡ÃƒO_ARQUIVOS")
    
    # Log milestone
    enhanced_logger.log_milestone("CONFIGURAÃ‡ÃƒO DO SISTEMA", {
        "PerÃ­odo": f"{data_inicio} atÃ© {data_fim} ({days} dias)",
        "InÃ­cio": datetime.now().strftime('%H:%M:%S'),
        "ConfiguraÃ§Ã£o": ' | '.join(config_info),
        "Features_v2": ' + '.join(v2_features) if v2_features else "PadrÃ£o",
        "Log_File": str(enhanced_logger.log_file_path) if enhanced_logger.log_file_path else "N/A"
    })

def show_performance_metrics():
    """Exibir mÃ©tricas de performance"""
    
    try:
        from insights.crew_optimized import get_performance_metrics
        
        metrics = get_performance_metrics()
        
        enhanced_logger.info("ğŸ“Š MÃ‰TRICAS DE PERFORMANCE:")
        enhanced_logger.info("-" * 30)
        enhanced_logger.info(f"   â€¢ NÃ­vel de log:    {metrics.get('log_level', 'N/A')}")
        enhanced_logger.info(f"   â€¢ Cache:           {'âœ…' if metrics.get('cache_enabled') else 'âŒ'}")
        enhanced_logger.info(f"   â€¢ Lazy loading:    {'âœ…' if metrics.get('lazy_loading') else 'âŒ'}")
        enhanced_logger.info(f"   â€¢ Cache entries:   {metrics.get('cache_size', 0)}")
        
        if metrics.get('cache_hits', 0) > 0 or metrics.get('cache_misses', 0) > 0:
            total_requests = metrics.get('cache_hits', 0) + metrics.get('cache_misses', 0)
            hit_rate = (metrics.get('cache_hits', 0) / total_requests) * 100 if total_requests > 0 else 0
            enhanced_logger.info(f"   â€¢ Cache hit rate:  {hit_rate:.1f}%")
        
        # Adicionar mÃ©tricas do enhanced logger
        enhanced_metrics = enhanced_logger.get_performance_summary()
        enhanced_logger.info(f"   â€¢ Ops rastreadas:  {enhanced_metrics['operations_tracked']}")
        enhanced_logger.info(f"   â€¢ Logs suprimidos: {enhanced_metrics['consolidation']['suppressed_messages']}")
            
    except Exception as e:
        enhanced_logger.warning(f"Erro ao obter mÃ©tricas: {e}")

# =============== FUNCIONALIDADES DO V2 SIMPLIFICADAS ===============

def run_tools_report():
    """Executar relatÃ³rio de ferramentas (simplificado do v2)"""
    try:
        enhanced_logger.info("ğŸ”§ Gerando relatÃ³rio de ferramentas...")
        
        # RelatÃ³rio bÃ¡sico das ferramentas do sistema (sem dependÃªncia do enhanced_tool_integration)
        try:
            from insights.config.tools_config_v3 import get_tools_statistics
            stats = get_tools_statistics()
            
            enhanced_logger.log_milestone("RELATÃ“RIO DE FERRAMENTAS", {
                "Total_ferramentas": stats.get('total_tools', 0),
                "Categorias": stats.get('categories', 0),
                "IntegraÃ§Ãµes": stats.get('integrations', 0)
            })
        except ImportError:
            enhanced_logger.warning("Sistema de estatÃ­sticas de ferramentas nÃ£o disponÃ­vel")
        
        # RelatÃ³rio bÃ¡sico adicional
        enhanced_logger.info("ğŸ“Š Ferramentas do sistema:")
        enhanced_logger.info("   â€¢ SQL Query Tool: ExtraÃ§Ã£o de dados")
        enhanced_logger.info("   â€¢ File Read Tool: Leitura de arquivos")
        enhanced_logger.info("   â€¢ Statistical Analysis Tool: AnÃ¡lises estatÃ­sticas")
        enhanced_logger.info("   â€¢ Business Intelligence Tool: Dashboards")
        enhanced_logger.info("   â€¢ File Generation Tool: GeraÃ§Ã£o de relatÃ³rios")
            
    except Exception as e:
        enhanced_logger.error(f"Erro no relatÃ³rio de ferramentas: {e}")

def validate_generated_files(data_inicio: str, data_fim: str):
    """Validar arquivos gerados (simplificado do v2)"""
    try:
        enhanced_logger.info("ğŸ“ Validando arquivos gerados...")
        
        # Arquivos esperados bÃ¡sicos
        expected_files = [
            "data/vendas.csv",
            "output/insights_*.txt"
        ]
        
        found_files = []
        missing_files = []
        
        for file_pattern in expected_files:
            if "*" in file_pattern:
                # Pattern matching bÃ¡sico
                base_dir = Path(file_pattern).parent
                if base_dir.exists():
                    pattern = Path(file_pattern).name.replace("*", "")
                    matching_files = list(base_dir.glob(f"*{pattern}*"))
                    if matching_files:
                        found_files.extend(matching_files)
                    else:
                        missing_files.append(file_pattern)
                else:
                    missing_files.append(file_pattern)
            else:
                file_path = Path(file_pattern)
                if file_path.exists():
                    found_files.append(file_path)
                else:
                    missing_files.append(file_pattern)
        
        # Log resultado
        enhanced_logger.log_milestone("VALIDAÃ‡ÃƒO DE ARQUIVOS", {
            "Arquivos_encontrados": len(found_files),
            "Arquivos_esperados": len(expected_files),
            "Taxa_completude": f"{(len(found_files)/len(expected_files)*100):.1f}%" if expected_files else "N/A"
        })
        
        if missing_files:
            enhanced_logger.warning(f"Arquivos nÃ£o encontrados: {missing_files}")
        
        return len(found_files) >= len(expected_files) * 0.8  # 80% de completude
        
    except Exception as e:
        enhanced_logger.error(f"Erro na validaÃ§Ã£o de arquivos: {e}")
        return False

# =============== EXECUÃ‡ÃƒO PRINCIPAL ===============

def run_insights_optimized(data_inicio: str, data_fim: str, args):
    """Executar o Insights-AI otimizado com funcionalidades v2"""
    
    execution_start = time.time()
    
    try:
        enhanced_logger.info("ğŸš€ Carregando sistema otimizado...")
        
        # Importar funÃ§Ã£o otimizada
        from insights.crew_optimized import run_optimized_crew
        
        # Executar anÃ¡lise
        enhanced_logger.info("âš¡ Executando anÃ¡lise de insights...")
        result = run_optimized_crew(data_inicio, data_fim)
        
        # Calcular tempo total
        total_time = time.time() - execution_start
        enhanced_logger.track_operation("main_execution", total_time)
        
        # RelatÃ³rio de ferramentas se solicitado
        if args.tools_report:
            run_tools_report()
        
        # ValidaÃ§Ã£o de arquivos se solicitada
        if args.validate_files:
            validation_ok = validate_generated_files(data_inicio, data_fim)
            if not validation_ok:
                enhanced_logger.warning("âš ï¸ ValidaÃ§Ã£o de arquivos incompleta")
        
        # Log resultado final
        enhanced_logger.log_milestone("EXECUÃ‡ÃƒO CONCLUÃDA", {
            "Tempo_total": f"{total_time:.2f}s",
            "Resultado_chars": len(str(result)) if result else 0,
            "Status": "Sucesso"
        })
        
        # MÃ©tricas de performance
        show_performance_metrics()
        
        # Salvar resultado se necessÃ¡rio
        if hasattr(result, 'raw') and len(str(result.raw)) > 100:
            save_file = f"output/insights_optimized_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            Path("output").mkdir(exist_ok=True)
            
            with open(save_file, 'w', encoding='utf-8') as f:
                f.write(str(result.raw))
            
            enhanced_logger.info(f"ğŸ’¾ Resultado salvo em: {save_file}")
        
        return result
        
    except KeyboardInterrupt:
        execution_time = time.time() - execution_start
        enhanced_logger.warning(f"ExecuÃ§Ã£o interrompida pelo usuÃ¡rio apÃ³s {execution_time:.2f}s")
        return None
        
    except Exception as e:
        execution_time = time.time() - execution_start
        enhanced_logger.error(f"Erro apÃ³s {execution_time:.2f}s: {e}")
        
        # Exibir stack trace em modo debug
        if os.getenv('INSIGHTS_DEBUG', 'false').lower() == 'true':
            import traceback
            traceback.print_exc()
        
        raise

# =============== INTERFACE DE LINHA DE COMANDO ===============

def create_parser():
    """Criar parser de argumentos de linha de comando"""
    
    parser = argparse.ArgumentParser(
        description="Insights-AI Otimizado (Unificado) - AnÃ¡lise de dados com performance avanÃ§ada",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Funcionalidades integradas das versÃµes optimized + enhanced v2:
  â€¢ Sistema de logging otimizado com consolidaÃ§Ã£o bÃ¡sica
  â€¢ Cache inteligente e lazy loading de ferramentas  
  â€¢ Log file management automÃ¡tico
  â€¢ RelatÃ³rios de performance e ferramentas
  â€¢ ValidaÃ§Ã£o de arquivos gerados
  â€¢ PerÃ­odo padrÃ£o otimizado (90 dias)

Exemplos de uso:
  %(prog)s                           # Ãšltimos 90 dias
  %(prog)s --days 60                 # Ãšltimos 60 dias  
  %(prog)s --start 2024-01-01 --end 2024-12-31
  %(prog)s --debug                   # Modo debug
  %(prog)s --minimal                 # Logs mÃ­nimos
  %(prog)s --production              # Modo produÃ§Ã£o
  %(prog)s --tools-report            # RelatÃ³rio de ferramentas
  %(prog)s --validate-files          # Validar arquivos gerados
  %(prog)s --no-consolidation        # Desabilitar consolidaÃ§Ã£o
        """
    )
    
    # Grupo de datas
    date_group = parser.add_argument_group('PerÃ­odo de anÃ¡lise')
    date_group.add_argument(
        '--start', 
        type=str, 
        help='Data inÃ­cio (YYYY-MM-DD)'
    )
    date_group.add_argument(
        '--end', 
        type=str, 
        help='Data fim (YYYY-MM-DD)'
    )
    date_group.add_argument(
        '--days', 
        type=int, 
        help='Ãšltimos N dias (padrÃ£o: 90)'
    )
    
    # Grupo de logging
    log_group = parser.add_argument_group('ConfiguraÃ§Ã£o de logs')
    log_group.add_argument(
        '--debug', 
        action='store_true', 
        help='Modo debug (logs detalhados)'
    )
    log_group.add_argument(
        '--verbose', 
        action='store_true', 
        help='Modo verbose (mais logs)'
    )
    log_group.add_argument(
        '--minimal', 
        action='store_true', 
        help='Modo minimal (poucos logs)'
    )
    log_group.add_argument(
        '--no-consolidation', 
        action='store_true', 
        help='Desabilitar consolidaÃ§Ã£o de logs'
    )
    
    # Grupo de performance
    perf_group = parser.add_argument_group('ConfiguraÃ§Ã£o de performance')
    perf_group.add_argument(
        '--production', 
        action='store_true', 
        help='Modo produÃ§Ã£o'
    )
    perf_group.add_argument(
        '--no-cache', 
        action='store_true', 
        help='Desabilitar cache'
    )
    
    # Funcionalidades do v2
    v2_group = parser.add_argument_group('Funcionalidades avanÃ§adas (do v2)')
    v2_group.add_argument(
        '--tools-report', 
        action='store_true', 
        help='Gerar relatÃ³rio detalhado de ferramentas'
    )
    v2_group.add_argument(
        '--validate-files', 
        action='store_true', 
        help='Validar se arquivos obrigatÃ³rios foram gerados'
    )
    
    # OpÃ§Ãµes gerais
    parser.add_argument(
        '--version', 
        action='version', 
        version='Insights-AI Otimizado v2.0 (Unificado)'
    )
    
    return parser

# =============== FUNÃ‡ÃƒO PRINCIPAL ===============

def main():
    """FunÃ§Ã£o principal"""
    
    # Parser de argumentos
    parser = create_parser()
    args = parser.parse_args()
    
    # Validar argumentos
    if (args.start and not args.end) or (args.end and not args.start):
        parser.error("--start e --end devem ser usados juntos")
    
    if args.debug and args.minimal:
        parser.error("--debug e --minimal sÃ£o mutuamente exclusivos")
    
    # Configurar ambiente
    setup_environment(args)
    
    # Calcular datas
    data_inicio, data_fim = calculate_dates(args)
    
    # Validar datas
    if not validate_dates(data_inicio, data_fim):
        sys.exit(1)
    
    # Exibir informaÃ§Ãµes
    show_execution_info(data_inicio, data_fim, args)
    
    try:
        # Executar anÃ¡lise
        result = run_insights_optimized(data_inicio, data_fim, args)
        
        if result:
            enhanced_logger.info("ğŸ‰ Insights-AI executado com sucesso!")
            enhanced_logger.info(f"ğŸ•’ Finalizado: {datetime.now().strftime('%H:%M:%S')}")
            
            # EstatÃ­sticas finais
            performance_summary = enhanced_logger.get_performance_summary()
            if performance_summary['consolidation']['suppressed_messages'] > 0:
                enhanced_logger.info(f"ğŸ“¦ Logs consolidados: {performance_summary['consolidation']['suppressed_messages']} mensagens suprimidas")
            if performance_summary['log_file']:
                enhanced_logger.info(f"ğŸ“ Log salvo em: {performance_summary['log_file']}")
        
    except Exception as e:
        enhanced_logger.error(f"Falha na execuÃ§Ã£o: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 