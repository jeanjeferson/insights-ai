#!/usr/bin/env python
"""
‚ö° MAIN OTIMIZADO - INSIGHTS-AI (UNIFICADO)
==========================================

Script principal unificado para executar o Insights-AI com otimiza√ß√µes de performance
e funcionalidades selecionadas da vers√£o enhanced v2:

FUNCIONALIDADES PRINCIPAIS:
- Logging estruturado menos verbose
- Lazy loading de ferramentas
- Cache inteligente de valida√ß√µes
- M√©tricas de performance em tempo real
- Configura√ß√£o autom√°tica por ambiente

FUNCIONALIDADES ADICIONAIS (do v2):
- Log file management b√°sico
- Relat√≥rios de ferramentas opcionais
- Valida√ß√£o de arquivos gerados
- Consolida√ß√£o b√°sica de logs repetitivos
- Per√≠odo padr√£o otimizado (90 dias)

Uso:
    python main_optimized.py                    # √öltimos 90 dias
    python main_optimized.py --days 60          # √öltimos 60 dias
    python main_optimized.py --start 2024-01-01 --end 2024-12-31
    python main_optimized.py --debug            # Modo debug
    python main_optimized.py --minimal          # Logs m√≠nimos
    python main_optimized.py --tools-report     # Relat√≥rio de ferramentas
    python main_optimized.py --validate-files   # Validar arquivos gerados
"""

import os
import sys
import argparse
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict

# Adicionar src ao Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

# =============== SISTEMA DE LOG CONSOLIDATION B√ÅSICO ===============

class BasicLogConsolidator:
    """Consolidador b√°sico de logs repetitivos (vers√£o simplificada do v2)"""
    
    def __init__(self):
        self.message_counts = defaultdict(int)
        self.last_log_time = {}
        self.consolidation_threshold = 5  # Consolidar ap√≥s 5 repeti√ß√µes
        self.time_window = 30  # 30 segundos
    
    def should_log(self, message: str) -> tuple[bool, str]:
        """Determinar se deve fazer log ou consolidar"""
        current_time = time.time()
        
        # Normalizar mensagem para padr√£o
        pattern = self._normalize_message(message)
        
        self.message_counts[pattern] += 1
        count = self.message_counts[pattern]
        
        # Se √© primeira vez ou passou do threshold
        if count == 1:
            self.last_log_time[pattern] = current_time
            return True, message
        elif count == self.consolidation_threshold:
            return True, f"[CONSOLIDADO] {message} (repetido {count}x)"
        elif count > self.consolidation_threshold:
            # Log apenas a cada 10 repeti√ß√µes ap√≥s o threshold
            if count % 10 == 0:
                return True, f"[CONSOLIDADO] {message} (repetido {count}x)"
            return False, ""
        else:
            return True, message
    
    def _normalize_message(self, message: str) -> str:
        """Normalizar mensagem removendo partes din√¢micas"""
        import re
        # Remover n√∫meros, timestamps, IDs din√¢micos
        normalized = re.sub(r'\d+', 'N', message)
        normalized = re.sub(r'\d{2}:\d{2}:\d{2}', 'HH:MM:SS', normalized)
        return normalized
    
    def get_stats(self) -> dict:
        """Obter estat√≠sticas de consolida√ß√£o"""
        total_messages = sum(self.message_counts.values())
        unique_patterns = len(self.message_counts)
        suppressed = total_messages - unique_patterns
        return {
            'total_messages': total_messages,
            'unique_patterns': unique_patterns,
            'suppressed_messages': suppressed
        }

# =============== ENHANCED LOGGER SIMPLES ===============

class SimpleEnhancedLogger:
    """Logger simples com funcionalidades b√°sicas do enhanced v2"""
    
    def __init__(self):
        self.consolidator = BasicLogConsolidator()
        self.log_file_path = None
        self.enable_consolidation = True
        self.performance_metrics = {
            'operations': {},
            'start_time': time.time()
        }
        self._setup_file_logging()
    
    def _setup_file_logging(self):
        """Configurar logging em arquivo (vers√£o simplificada)"""
        try:
            log_dir = Path("logs/optimized")
            log_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d")
            self.log_file_path = log_dir / f"insights_optimized_{timestamp}.log"
            
            # Configurar handler de arquivo
            file_handler = logging.FileHandler(self.log_file_path, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            
            formatter = logging.Formatter(
                '%(asctime)s [%(levelname)s] %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            file_handler.setFormatter(formatter)
            
            # Adicionar ao logger principal
            logger = logging.getLogger()
            logger.addHandler(file_handler)
            logger.setLevel(logging.INFO)
            
        except Exception as e:
            print(f"‚ö†Ô∏è N√£o foi poss√≠vel configurar log em arquivo: {e}")
    
    def _log_with_consolidation(self, level: str, message: str):
        """Log com consolida√ß√£o b√°sica"""
        if self.enable_consolidation:
            should_log, final_message = self.consolidator.should_log(message)
            if should_log and final_message:
                if level.upper() == 'INFO':
                    print(final_message)
                elif level.upper() == 'WARNING':
                    print(f"‚ö†Ô∏è {final_message}")
                elif level.upper() == 'ERROR':
                    print(f"‚ùå {final_message}")
                elif level.upper() == 'DEBUG':
                    if os.getenv('INSIGHTS_DEBUG', 'false').lower() == 'true':
                        print(f"üêõ {final_message}")
                
                # Log em arquivo sempre
                if self.log_file_path:
                    logging.getLogger().log(getattr(logging, level.upper()), final_message)
        else:
            # Log direto sem consolida√ß√£o
            print(message)
            if self.log_file_path:
                logging.getLogger().log(getattr(logging, level.upper()), message)
    
    def info(self, message: str):
        self._log_with_consolidation('INFO', message)
    
    def warning(self, message: str):
        self._log_with_consolidation('WARNING', message)
    
    def error(self, message: str):
        self._log_with_consolidation('ERROR', message)
    
    def debug(self, message: str):
        self._log_with_consolidation('DEBUG', message)
    
    def log_milestone(self, title: str, data: dict):
        """Log milestone simplificado"""
        self.info(f"üéØ {title}")
        for key, value in data.items():
            self.info(f"   üìä {key}: {value}")
    
    def track_operation(self, operation: str, duration: float):
        """Track operation performance"""
        if operation not in self.performance_metrics['operations']:
            self.performance_metrics['operations'][operation] = []
        self.performance_metrics['operations'][operation].append(duration)
    
    def get_performance_summary(self) -> dict:
        """Obter resumo de performance"""
        total_time = time.time() - self.performance_metrics['start_time']
        consolidation_stats = self.consolidator.get_stats()
        
        return {
            'total_execution_time': total_time,
            'operations_tracked': len(self.performance_metrics['operations']),
            'log_file': str(self.log_file_path) if self.log_file_path else None,
            'consolidation': consolidation_stats
        }

# Inst√¢ncia global do logger
enhanced_logger = SimpleEnhancedLogger()

# =============== CONFIGURA√á√ÉO DE ENVIRONMENT ===============

def setup_environment(args):
    """Configurar ambiente baseado nos argumentos"""
    
    # Configurar n√≠vel de logging
    if args.debug:
        os.environ['INSIGHTS_DEBUG'] = 'true'
        enhanced_logger.info("üêõ Modo DEBUG ativado")
    elif args.minimal:
        os.environ['INSIGHTS_LOG_LEVEL'] = 'MINIMAL'
        enhanced_logger.info("üîá Modo MINIMAL ativado")
    elif args.verbose:
        os.environ['INSIGHTS_LOG_LEVEL'] = 'VERBOSE'
        enhanced_logger.info("üì¢ Modo VERBOSE ativado")
    
    # Configurar environment
    if args.production:
        os.environ['ENVIRONMENT'] = 'production'
        enhanced_logger.info("üè≠ Modo PRODU√á√ÉO ativado")
    
    # Configurar cache
    if args.no_cache:
        os.environ['INSIGHTS_DISABLE_CACHE'] = 'true'
        enhanced_logger.info("üö´ Cache DESABILITADO")
    
    # Configura√ß√µes do v2
    if args.no_consolidation:
        enhanced_logger.enable_consolidation = False
        enhanced_logger.info("üìù Consolida√ß√£o de logs DESABILITADA")
    
    if args.tools_report:
        enhanced_logger.info("üîß Relat√≥rio de ferramentas ATIVADO")

# =============== VALIDA√á√ÉO DE DATAS ===============

def validate_dates(data_inicio: str, data_fim: str):
    """Validar formato e l√≥gica das datas"""
    
    try:
        start_date = datetime.strptime(data_inicio, '%Y-%m-%d')
        end_date = datetime.strptime(data_fim, '%Y-%m-%d')
        
        if start_date > end_date:
            raise ValueError("Data in√≠cio n√£o pode ser posterior √† data fim")
        
        if end_date > datetime.now():
            print("‚ö†Ô∏è Data fim est√° no futuro - usando dados dispon√≠veis")
        
        days_diff = (end_date - start_date).days
        if days_diff > 365:
            print(f"‚ö†Ô∏è Per√≠odo muito longo ({days_diff} dias) - pode impactar performance")
        
        return True
        
    except ValueError as e:
        print(f"‚ùå Erro na data: {e}")
        return False

# =============== C√ÅLCULO DE DATAS ===============

def calculate_dates(args):
    """Calcular datas baseado nos argumentos"""
    
    if args.start and args.end:
        # Datas espec√≠ficas
        return args.start, args.end
    
    elif args.days:
        # √öltimos N dias
        end_date = datetime.now()
        start_date = end_date - timedelta(days=args.days)
        return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')
    
    else:
        # Padr√£o: 90 dias (otimizado do v2)
        end_date = datetime.now()
        start_date = end_date - timedelta(days=90)
        return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')

# =============== EXIBI√á√ÉO DE INFORMA√á√ïES ===============

def show_execution_info(data_inicio: str, data_fim: str, args):
    """Exibir informa√ß√µes da execu√ß√£o"""
    
    enhanced_logger.info("="*60)
    enhanced_logger.info("‚ö° INSIGHTS-AI OTIMIZADO (UNIFICADO)")
    enhanced_logger.info("="*60)
    
    # Per√≠odo
    start_date = datetime.strptime(data_inicio, '%Y-%m-%d')
    end_date = datetime.strptime(data_fim, '%Y-%m-%d')
    days = (end_date - start_date).days
    
    # Configura√ß√µes
    config_info = []
    if args.debug:
        config_info.append("DEBUG")
    elif args.minimal:
        config_info.append("MINIMAL") 
    elif args.verbose:
        config_info.append("VERBOSE")
    else:
        config_info.append("NORMAL")
        
    if args.production:
        config_info.append("PRODU√á√ÉO")
    else:
        config_info.append("DESENVOLVIMENTO")
        
    if args.no_cache:
        config_info.append("SEM CACHE")
    else:
        config_info.append("COM CACHE")
    
    # Features do v2
    v2_features = []
    if not args.no_consolidation:
        v2_features.append("CONSOLIDA√á√ÉO")
    if args.tools_report:
        v2_features.append("TOOLS_REPORT")
    if args.validate_files:
        v2_features.append("VALIDA√á√ÉO_ARQUIVOS")
    
    # Log milestone
    enhanced_logger.log_milestone("CONFIGURA√á√ÉO DO SISTEMA", {
        "Per√≠odo": f"{data_inicio} at√© {data_fim} ({days} dias)",
        "In√≠cio": datetime.now().strftime('%H:%M:%S'),
        "Configura√ß√£o": ' | '.join(config_info),
        "Features_v2": ' + '.join(v2_features) if v2_features else "Padr√£o",
        "Log_File": str(enhanced_logger.log_file_path) if enhanced_logger.log_file_path else "N/A"
    })

def show_performance_metrics():
    """Exibir m√©tricas de performance"""
    
    try:
        from insights.crew_optimized import get_performance_metrics
        
        metrics = get_performance_metrics()
        
        enhanced_logger.info("üìä M√âTRICAS DE PERFORMANCE:")
        enhanced_logger.info("-" * 30)
        enhanced_logger.info(f"   ‚Ä¢ N√≠vel de log:    {metrics.get('log_level', 'N/A')}")
        enhanced_logger.info(f"   ‚Ä¢ Cache:           {'‚úÖ' if metrics.get('cache_enabled') else '‚ùå'}")
        enhanced_logger.info(f"   ‚Ä¢ Lazy loading:    {'‚úÖ' if metrics.get('lazy_loading') else '‚ùå'}")
        enhanced_logger.info(f"   ‚Ä¢ Cache entries:   {metrics.get('cache_size', 0)}")
        
        if metrics.get('cache_hits', 0) > 0 or metrics.get('cache_misses', 0) > 0:
            total_requests = metrics.get('cache_hits', 0) + metrics.get('cache_misses', 0)
            hit_rate = (metrics.get('cache_hits', 0) / total_requests) * 100 if total_requests > 0 else 0
            enhanced_logger.info(f"   ‚Ä¢ Cache hit rate:  {hit_rate:.1f}%")
        
        # Adicionar m√©tricas do enhanced logger
        enhanced_metrics = enhanced_logger.get_performance_summary()
        enhanced_logger.info(f"   ‚Ä¢ Ops rastreadas:  {enhanced_metrics['operations_tracked']}")
        enhanced_logger.info(f"   ‚Ä¢ Logs suprimidos: {enhanced_metrics['consolidation']['suppressed_messages']}")
            
    except Exception as e:
        enhanced_logger.warning(f"Erro ao obter m√©tricas: {e}")

# =============== FUNCIONALIDADES DO V2 SIMPLIFICADAS ===============

def run_tools_report():
    """Executar relat√≥rio de ferramentas (simplificado do v2)"""
    try:
        enhanced_logger.info("üîß Gerando relat√≥rio de ferramentas...")
        
        # Relat√≥rio b√°sico das ferramentas do sistema (sem depend√™ncia do enhanced_tool_integration)
        try:
            from insights.config.tools_config_v3 import get_tools_statistics
            stats = get_tools_statistics()
            
            enhanced_logger.log_milestone("RELAT√ìRIO DE FERRAMENTAS", {
                "Total_ferramentas": stats.get('total_tools', 0),
                "Categorias": stats.get('categories', 0),
                "Integra√ß√µes": stats.get('integrations', 0)
            })
        except ImportError:
            enhanced_logger.warning("Sistema de estat√≠sticas de ferramentas n√£o dispon√≠vel")
        
        # Relat√≥rio b√°sico adicional
        enhanced_logger.info("üìä Ferramentas do sistema:")
        enhanced_logger.info("   ‚Ä¢ SQL Query Tool: Extra√ß√£o de dados")
        enhanced_logger.info("   ‚Ä¢ File Read Tool: Leitura de arquivos")
        enhanced_logger.info("   ‚Ä¢ Statistical Analysis Tool: An√°lises estat√≠sticas")
        enhanced_logger.info("   ‚Ä¢ Business Intelligence Tool: Dashboards")
        enhanced_logger.info("   ‚Ä¢ File Generation Tool: Gera√ß√£o de relat√≥rios")
            
    except Exception as e:
        enhanced_logger.error(f"Erro no relat√≥rio de ferramentas: {e}")

def validate_generated_files(data_inicio: str, data_fim: str):
    """Validar arquivos gerados (simplificado do v2)"""
    try:
        enhanced_logger.info("üìÅ Validando arquivos gerados...")
        
        # Arquivos esperados b√°sicos
        expected_files = [
            "data/vendas.csv",
            "output/insights_*.txt"
        ]
        
        found_files = []
        missing_files = []
        
        for file_pattern in expected_files:
            if "*" in file_pattern:
                # Pattern matching b√°sico
                base_dir = Path(file_pattern).parent
                if base_dir.exists():
                    pattern = Path(file_pattern).name.replace("*", "")
                    matching_files = list(base_dir.glob(f"*{pattern}*"))
                    if matching_files:
                        found_files.extend(matching_files)
                    else:
                        missing_files.append(file_pattern)
                else:
                    missing_files.append(file_pattern)
            else:
                file_path = Path(file_pattern)
                if file_path.exists():
                    found_files.append(file_path)
                else:
                    missing_files.append(file_pattern)
        
        # Log resultado
        enhanced_logger.log_milestone("VALIDA√á√ÉO DE ARQUIVOS", {
            "Arquivos_encontrados": len(found_files),
            "Arquivos_esperados": len(expected_files),
            "Taxa_completude": f"{(len(found_files)/len(expected_files)*100):.1f}%" if expected_files else "N/A"
        })
        
        if missing_files:
            enhanced_logger.warning(f"Arquivos n√£o encontrados: {missing_files}")
        
        return len(found_files) >= len(expected_files) * 0.8  # 80% de completude
        
    except Exception as e:
        enhanced_logger.error(f"Erro na valida√ß√£o de arquivos: {e}")
        return False

# =============== EXECU√á√ÉO PRINCIPAL ===============

def run_insights_optimized(data_inicio: str, data_fim: str, args):
    """Executar o Insights-AI otimizado com funcionalidades v2"""
    
    execution_start = time.time()
    
    try:
        enhanced_logger.info("üöÄ Carregando sistema otimizado...")
        
        # Importar fun√ß√£o otimizada
        from insights.crew_optimized import run_optimized_crew
        
        # Executar an√°lise
        enhanced_logger.info("‚ö° Executando an√°lise de insights...")
        result = run_optimized_crew(data_inicio, data_fim)
        
        # Calcular tempo total
        total_time = time.time() - execution_start
        enhanced_logger.track_operation("main_execution", total_time)
        
        # Relat√≥rio de ferramentas se solicitado
        if args.tools_report:
            run_tools_report()
        
        # Valida√ß√£o de arquivos se solicitada
        if args.validate_files:
            validation_ok = validate_generated_files(data_inicio, data_fim)
            if not validation_ok:
                enhanced_logger.warning("‚ö†Ô∏è Valida√ß√£o de arquivos incompleta")
        
        # Log resultado final
        enhanced_logger.log_milestone("EXECU√á√ÉO CONCLU√çDA", {
            "Tempo_total": f"{total_time:.2f}s",
            "Resultado_chars": len(str(result)) if result else 0,
            "Status": "Sucesso"
        })
        
        # M√©tricas de performance
        show_performance_metrics()
        
        # Salvar resultado se necess√°rio
        if hasattr(result, 'raw') and len(str(result.raw)) > 100:
            save_file = f"output/insights_optimized_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            Path("output").mkdir(exist_ok=True)
            
            with open(save_file, 'w', encoding='utf-8') as f:
                f.write(str(result.raw))
            
            enhanced_logger.info(f"üíæ Resultado salvo em: {save_file}")
        
        return result
        
    except KeyboardInterrupt:
        execution_time = time.time() - execution_start
        enhanced_logger.warning(f"Execu√ß√£o interrompida pelo usu√°rio ap√≥s {execution_time:.2f}s")
        return None
        
    except Exception as e:
        execution_time = time.time() - execution_start
        enhanced_logger.error(f"Erro ap√≥s {execution_time:.2f}s: {e}")
        
        # Exibir stack trace em modo debug
        if os.getenv('INSIGHTS_DEBUG', 'false').lower() == 'true':
            import traceback
            traceback.print_exc()
        
        raise

# =============== INTERFACE DE LINHA DE COMANDO ===============

def create_parser():
    """Criar parser de argumentos de linha de comando"""
    
    parser = argparse.ArgumentParser(
        description="Insights-AI Otimizado (Unificado) - An√°lise de dados com performance avan√ßada",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Funcionalidades integradas das vers√µes optimized + enhanced v2:
  ‚Ä¢ Sistema de logging otimizado com consolida√ß√£o b√°sica
  ‚Ä¢ Cache inteligente e lazy loading de ferramentas  
  ‚Ä¢ Log file management autom√°tico
  ‚Ä¢ Relat√≥rios de performance e ferramentas
  ‚Ä¢ Valida√ß√£o de arquivos gerados
  ‚Ä¢ Per√≠odo padr√£o otimizado (90 dias)

Exemplos de uso:
  %(prog)s                           # √öltimos 90 dias
  %(prog)s --days 60                 # √öltimos 60 dias  
  %(prog)s --start 2024-01-01 --end 2024-12-31
  %(prog)s --debug                   # Modo debug
  %(prog)s --minimal                 # Logs m√≠nimos
  %(prog)s --production              # Modo produ√ß√£o
  %(prog)s --tools-report            # Relat√≥rio de ferramentas
  %(prog)s --validate-files          # Validar arquivos gerados
  %(prog)s --no-consolidation        # Desabilitar consolida√ß√£o
        """
    )
    
    # Grupo de datas
    date_group = parser.add_argument_group('Per√≠odo de an√°lise')
    date_group.add_argument(
        '--start', 
        type=str, 
        help='Data in√≠cio (YYYY-MM-DD)'
    )
    date_group.add_argument(
        '--end', 
        type=str, 
        help='Data fim (YYYY-MM-DD)'
    )
    date_group.add_argument(
        '--days', 
        type=int, 
        help='√öltimos N dias (padr√£o: 90)'
    )
    
    # Grupo de logging
    log_group = parser.add_argument_group('Configura√ß√£o de logs')
    log_group.add_argument(
        '--debug', 
        action='store_true', 
        help='Modo debug (logs detalhados)'
    )
    log_group.add_argument(
        '--verbose', 
        action='store_true', 
        help='Modo verbose (mais logs)'
    )
    log_group.add_argument(
        '--minimal', 
        action='store_true', 
        help='Modo minimal (poucos logs)'
    )
    log_group.add_argument(
        '--no-consolidation', 
        action='store_true', 
        help='Desabilitar consolida√ß√£o de logs'
    )
    
    # Grupo de performance
    perf_group = parser.add_argument_group('Configura√ß√£o de performance')
    perf_group.add_argument(
        '--production', 
        action='store_true', 
        help='Modo produ√ß√£o'
    )
    perf_group.add_argument(
        '--no-cache', 
        action='store_true', 
        help='Desabilitar cache'
    )
    
    # Funcionalidades do v2
    v2_group = parser.add_argument_group('Funcionalidades avan√ßadas (do v2)')
    v2_group.add_argument(
        '--tools-report', 
        action='store_true', 
        help='Gerar relat√≥rio detalhado de ferramentas'
    )
    v2_group.add_argument(
        '--validate-files', 
        action='store_true', 
        help='Validar se arquivos obrigat√≥rios foram gerados'
    )
    
    # Op√ß√µes gerais
    parser.add_argument(
        '--version', 
        action='version', 
        version='Insights-AI Otimizado v2.0 (Unificado)'
    )
    
    return parser

# =============== FUN√á√ÉO PRINCIPAL ===============

def main():
    """Fun√ß√£o principal"""
    
    # Parser de argumentos
    parser = create_parser()
    args = parser.parse_args()
    
    # Validar argumentos
    if (args.start and not args.end) or (args.end and not args.start):
        parser.error("--start e --end devem ser usados juntos")
    
    if args.debug and args.minimal:
        parser.error("--debug e --minimal s√£o mutuamente exclusivos")
    
    # Configurar ambiente
    setup_environment(args)
    
    # Calcular datas
    data_inicio, data_fim = calculate_dates(args)
    
    # Validar datas
    if not validate_dates(data_inicio, data_fim):
        sys.exit(1)
    
    # Exibir informa√ß√µes
    show_execution_info(data_inicio, data_fim, args)
    
    try:
        # Executar an√°lise
        result = run_insights_optimized(data_inicio, data_fim, args)
        
        if result:
            enhanced_logger.info("üéâ Insights-AI executado com sucesso!")
            enhanced_logger.info(f"üïí Finalizado: {datetime.now().strftime('%H:%M:%S')}")
            
            # Estat√≠sticas finais
            performance_summary = enhanced_logger.get_performance_summary()
            if performance_summary['consolidation']['suppressed_messages'] > 0:
                enhanced_logger.info(f"üì¶ Logs consolidados: {performance_summary['consolidation']['suppressed_messages']} mensagens suprimidas")
            if performance_summary['log_file']:
                enhanced_logger.info(f"üìÅ Log salvo em: {performance_summary['log_file']}")
        
    except Exception as e:
        enhanced_logger.error(f"Falha na execu√ß√£o: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 