#!/usr/bin/env python
"""
‚ö° BENCHMARK DE PERFORMANCE INSIGHTS-AI
=====================================

Script para comparar performance entre:
- Vers√£o original (crew.py)
- Vers√£o otimizada (crew_optimized.py)

M√©tricas avaliadas:
- Tempo de inicializa√ß√£o
- Uso de mem√≥ria
- Verbosidade de logs
- Tempo total de execu√ß√£o
"""

import sys
import os
import time
import tracemalloc
import psutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, List

# Adicionar path do projeto
sys.path.append(str(Path(__file__).parent.parent))

# =============== CONFIGURA√á√ÉO DO BENCHMARK ===============

class BenchmarkConfig:
    """Configura√ß√£o do benchmark"""
    
    # Per√≠odos de teste
    TEST_PERIODS = [
        ("2024-11-01", "2024-11-30"),  # 1 m√™s
        ("2024-10-01", "2024-11-30"),  # 2 meses
    ]
    
    # M√©tricas a coletar
    METRICS = [
        'initialization_time',
        'memory_usage_mb',
        'log_count',
        'execution_time',
        'total_time'
    ]
    
    # N√∫mero de execu√ß√µes para m√©dia
    ITERATIONS = 3

# =============== COLLECTOR DE M√âTRICAS ===============

class MetricsCollector:
    """Coletor de m√©tricas de performance"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """Reset das m√©tricas"""
        self.start_time = None
        self.init_time = None
        self.memory_start = 0
        self.memory_peak = 0
        self.log_count = 0
        self.process = psutil.Process()
    
    def start_measurement(self):
        """Iniciar medi√ß√£o"""
        self.reset()
        self.start_time = time.time()
        tracemalloc.start()
        self.memory_start = self.process.memory_info().rss / 1024 / 1024
    
    def mark_initialization_complete(self):
        """Marcar conclus√£o da inicializa√ß√£o"""
        if self.start_time:
            self.init_time = time.time() - self.start_time
    
    def stop_measurement(self) -> Dict[str, Any]:
        """Parar medi√ß√£o e retornar m√©tricas"""
        end_time = time.time()
        
        # Mem√≥ria
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        memory_end = self.process.memory_info().rss / 1024 / 1024
        memory_used = memory_end - self.memory_start
        memory_peak_mb = peak / 1024 / 1024
        
        # Tempo total
        total_time = end_time - (self.start_time or end_time)
        
        return {
            'initialization_time': self.init_time or 0,
            'memory_usage_mb': memory_used,
            'memory_peak_mb': memory_peak_mb,
            'execution_time': total_time - (self.init_time or 0),
            'total_time': total_time,
            'log_count': self.log_count
        }

# =============== INTERCEPTADOR DE LOGS ===============

class LogInterceptor:
    """Intercepta logs para contar verbosidade"""
    
    def __init__(self):
        self.log_count = 0
        self.log_levels = {}
        self.logs = []
    
    def intercept_log(self, level: str, message: str):
        """Interceptar um log"""
        self.log_count += 1
        self.log_levels[level] = self.log_levels.get(level, 0) + 1
        
        # Armazenar apenas os primeiros 100 logs para an√°lise
        if len(self.logs) < 100:
            self.logs.append({
                'level': level,
                'message': message[:100],  # Truncar mensagem
                'timestamp': time.time()
            })
    
    def get_stats(self) -> Dict[str, Any]:
        """Obter estat√≠sticas dos logs"""
        return {
            'total_logs': self.log_count,
            'by_level': self.log_levels,
            'sample_logs': self.logs[:10]  # Primeiros 10 logs
        }

# =============== BENCHMARK DA VERS√ÉO ORIGINAL ===============

def benchmark_original_crew(data_inicio: str, data_fim: str) -> Dict[str, Any]:
    """Benchmark da vers√£o original"""
    
    collector = MetricsCollector()
    log_interceptor = LogInterceptor()
    
    try:
        print("üîÑ Testando vers√£o ORIGINAL...")
        collector.start_measurement()
        
        # Importar vers√£o original
        from insights.crew import Insights
        
        collector.mark_initialization_complete()
        
        # Executar crew original
        crew_instance = Insights()
        
        inputs = {
            'data_inicio': data_inicio,
            'data_fim': data_fim
        }
        
        # Simular execu√ß√£o (sem executar realmente para speed)
        # result = crew_instance.crew().kickoff(inputs=inputs)
        
        # Para benchmark, vamos apenas medir a cria√ß√£o do crew
        crew = crew_instance.crew()
        
        metrics = collector.stop_measurement()
        
        # Adicionar informa√ß√µes espec√≠ficas
        metrics.update({
            'version': 'original',
            'agents_count': len(crew.agents) if hasattr(crew, 'agents') else 0,
            'tasks_count': len(crew.tasks) if hasattr(crew, 'tasks') else 0,
            'log_stats': log_interceptor.get_stats()
        })
        
        return metrics
        
    except Exception as e:
        print(f"‚ùå Erro no benchmark original: {e}")
        return {
            'version': 'original',
            'error': str(e),
            'metrics': collector.stop_measurement()
        }

# =============== BENCHMARK DA VERS√ÉO OTIMIZADA ===============

def benchmark_optimized_crew(data_inicio: str, data_fim: str) -> Dict[str, Any]:
    """Benchmark da vers√£o otimizada"""
    
    collector = MetricsCollector()
    log_interceptor = LogInterceptor()
    
    try:
        print("‚ö° Testando vers√£o OTIMIZADA...")
        collector.start_measurement()
        
        # Importar vers√£o otimizada
        from insights.crew_optimized import OptimizedInsights
        
        collector.mark_initialization_complete()
        
        # Executar crew otimizado
        crew_instance = OptimizedInsights()
        
        inputs = {
            'data_inicio': data_inicio,
            'data_fim': data_fim
        }
        
        # Para benchmark, vamos apenas medir a cria√ß√£o do crew
        crew = crew_instance.crew()
        
        metrics = collector.stop_measurement()
        
        # Adicionar informa√ß√µes espec√≠ficas
        metrics.update({
            'version': 'optimized',
            'agents_count': len(crew.agents) if hasattr(crew, 'agents') else 0,
            'tasks_count': len(crew.tasks) if hasattr(crew, 'tasks') else 0,
            'log_stats': log_interceptor.get_stats()
        })
        
        return metrics
        
    except Exception as e:
        print(f"‚ùå Erro no benchmark otimizado: {e}")
        return {
            'version': 'optimized',
            'error': str(e),
            'metrics': collector.stop_measurement()
        }

# =============== AN√ÅLISE DE RESULTADOS ===============

def analyze_results(original_results: List[Dict], optimized_results: List[Dict]):
    """Analisar e comparar resultados"""
    
    print("\n" + "="*80)
    print("üìä AN√ÅLISE DE PERFORMANCE - RESULTADOS DETALHADOS")
    print("="*80)
    
    # Calcular m√©dias
    def calc_average(results: List[Dict], metric: str) -> float:
        values = [r.get(metric, 0) for r in results if 'error' not in r]
        return sum(values) / len(values) if values else 0
    
    # M√©tricas principais
    metrics = [
        ('initialization_time', 'Tempo Inicializa√ß√£o (s)', 's'),
        ('memory_usage_mb', 'Uso de Mem√≥ria (MB)', 'MB'),
        ('memory_peak_mb', 'Pico de Mem√≥ria (MB)', 'MB'),
        ('total_time', 'Tempo Total (s)', 's')
    ]
    
    print(f"{'M√©trica':<25} {'Original':<15} {'Otimizada':<15} {'Melhoria':<15}")
    print("-" * 70)
    
    improvements = {}
    
    for metric, label, unit in metrics:
        original_avg = calc_average(original_results, metric)
        optimized_avg = calc_average(optimized_results, metric)
        
        if original_avg > 0:
            improvement = ((original_avg - optimized_avg) / original_avg) * 100
            improvement_str = f"{improvement:+.1f}%"
        else:
            improvement_str = "N/A"
        
        improvements[metric] = improvement
        
        print(f"{label:<25} {original_avg:<15.3f} {optimized_avg:<15.3f} {improvement_str:<15}")
    
    # An√°lise de logs
    print(f"\nüìù AN√ÅLISE DE VERBOSIDADE:")
    print("-" * 40)
    
    original_log_avg = calc_average(original_results, 'log_count')
    optimized_log_avg = calc_average(optimized_results, 'log_count')
    
    if original_log_avg > 0:
        log_reduction = ((original_log_avg - optimized_log_avg) / original_log_avg) * 100
        print(f"Logs Original:     {original_log_avg:.0f}")
        print(f"Logs Otimizada:    {optimized_log_avg:.0f}")
        print(f"Redu√ß√£o de logs:   {log_reduction:.1f}%")
    
    # Score geral de melhoria
    print(f"\nüéØ SCORE GERAL DE MELHORIA:")
    print("-" * 30)
    
    valid_improvements = [v for v in improvements.values() if v is not None and v > -100]
    if valid_improvements:
        avg_improvement = sum(valid_improvements) / len(valid_improvements)
        print(f"Melhoria m√©dia:    {avg_improvement:.1f}%")
        
        if avg_improvement > 20:
            print("‚úÖ EXCELENTE melhoria de performance!")
        elif avg_improvement > 10:
            print("‚úÖ BOA melhoria de performance!")
        elif avg_improvement > 0:
            print("‚úÖ Melhoria modesta de performance")
        else:
            print("‚ö†Ô∏è Performance similar ou pior")
    
    # Recomenda√ß√µes
    print(f"\nüí° RECOMENDA√á√ïES:")
    print("-" * 20)
    
    if improvements.get('initialization_time', 0) > 10:
        print("‚úÖ Inicializa√ß√£o significativamente mais r√°pida")
    
    if improvements.get('memory_usage_mb', 0) > 5:
        print("‚úÖ Uso de mem√≥ria reduzido")
    
    if log_reduction > 30:
        print("‚úÖ Logs muito menos verbosos")
    
    return improvements

# =============== FUN√á√ÉO PRINCIPAL ===============

def run_performance_benchmark():
    """Executar benchmark completo"""
    
    print("‚ö° BENCHMARK DE PERFORMANCE INSIGHTS-AI")
    print("="*50)
    print(f"üìÖ Testando {len(BenchmarkConfig.TEST_PERIODS)} per√≠odos")
    print(f"üîÑ {BenchmarkConfig.ITERATIONS} itera√ß√µes cada")
    print(f"‚è∞ Iniciando em {datetime.now().strftime('%H:%M:%S')}")
    
    all_original_results = []
    all_optimized_results = []
    
    for period_idx, (data_inicio, data_fim) in enumerate(BenchmarkConfig.TEST_PERIODS):
        print(f"\nüìä PER√çODO {period_idx + 1}: {data_inicio} at√© {data_fim}")
        print("-" * 40)
        
        # Executar m√∫ltiplas itera√ß√µes
        for iteration in range(BenchmarkConfig.ITERATIONS):
            print(f"\nüîÑ Itera√ß√£o {iteration + 1}/{BenchmarkConfig.ITERATIONS}")
            
            # Benchmark vers√£o original
            original_result = benchmark_original_crew(data_inicio, data_fim)
            all_original_results.append(original_result)
            
            # Pequena pausa entre testes
            time.sleep(1)
            
            # Benchmark vers√£o otimizada  
            optimized_result = benchmark_optimized_crew(data_inicio, data_fim)
            all_optimized_results.append(optimized_result)
            
            # Pausa entre itera√ß√µes
            time.sleep(1)
            
            # Log resumo da itera√ß√£o
            if 'error' not in original_result and 'error' not in optimized_result:
                orig_time = original_result.get('total_time', 0)
                opt_time = optimized_result.get('total_time', 0)
                improvement = ((orig_time - opt_time) / orig_time * 100) if orig_time > 0 else 0
                print(f"   Original: {orig_time:.2f}s | Otimizada: {opt_time:.2f}s | Melhoria: {improvement:+.1f}%")
    
    # An√°lise final
    analyze_results(all_original_results, all_optimized_results)
    
    # Salvar resultados
    save_benchmark_results(all_original_results, all_optimized_results)

def save_benchmark_results(original_results: List[Dict], optimized_results: List[Dict]):
    """Salvar resultados do benchmark"""
    
    try:
        import json
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'config': {
                'test_periods': BenchmarkConfig.TEST_PERIODS,
                'iterations': BenchmarkConfig.ITERATIONS
            },
            'original_results': original_results,
            'optimized_results': optimized_results
        }
        
        # Salvar em arquivo
        output_dir = Path("logs/benchmarks")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        filename = f"performance_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        output_file = output_dir / filename
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Resultados salvos em: {output_file}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao salvar resultados: {e}")

if __name__ == "__main__":
    # Verificar se podemos importar os m√≥dulos necess√°rios
    try:
        # Test imports
        import insights.crew
        import insights.crew_optimized
        print("‚úÖ M√≥dulos encontrados, iniciando benchmark...")
    except ImportError as e:
        print(f"‚ùå Erro de importa√ß√£o: {e}")
        print("Certifique-se de que os m√≥dulos est√£o no PYTHONPATH")
        sys.exit(1)
    
    # Executar benchmark
    try:
        run_performance_benchmark()
        print(f"\nüèÅ Benchmark conclu√≠do em {datetime.now().strftime('%H:%M:%S')}")
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Benchmark interrompido pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro no benchmark: {e}")
        import traceback
        traceback.print_exc() 